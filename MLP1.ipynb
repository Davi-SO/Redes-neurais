{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfd3e1f-45b3-4bce-8db0-4683673ee2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ucimlrepo import fetch_ucirepo\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bc9217a-9973-423c-9351-4ee0396c5d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_ucirepo(id=468)\n",
    "\n",
    "X = pd.DataFrame(data.data.features)\n",
    "y = pd.Series(data.data.targets.values.ravel(), name='Revenue')\n",
    "\n",
    "#codificação de variáveis categóricas\n",
    "\n",
    "X_encoded = X.copy()\n",
    "for col in X.select_dtypes(include=['object']).columns:\n",
    "    le = LabelEncoder()\n",
    "    X_encoded[col] = le.fit_transform(X[col])\n",
    "\n",
    "#divisão dos dados - 80/20\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "#normalização dos dados\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dca149b9-1955-4c59-9d88-0a9cbe314439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 64 candidates, totalling 192 fits\n",
      "[CV 1/3] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.880 total time=  15.3s\n",
      "[CV 2/3] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.874 total time=  12.9s\n",
      "[CV 3/3] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.876 total time=  14.5s\n",
      "[CV 1/3] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.898 total time=  20.4s\n",
      "[CV 2/3] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.898 total time=  20.0s\n",
      "[CV 3/3] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.887 total time=  26.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.891 total time=  29.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.898 total time=  30.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.884 total time=  29.3s\n",
      "[CV 1/3] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.887 total time=  18.7s\n",
      "[CV 2/3] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.882 total time=  17.8s\n",
      "[CV 3/3] END activation=relu, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.876 total time=  18.8s\n",
      "[CV 1/3] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.877 total time=  15.3s\n",
      "[CV 2/3] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.880 total time=  13.2s\n",
      "[CV 3/3] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.870 total time=  12.8s\n",
      "[CV 1/3] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.897 total time=  13.4s\n",
      "[CV 2/3] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.895 total time=  11.4s\n",
      "[CV 3/3] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.890 total time=  13.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.899 total time=  17.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.890 total time=  17.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.892 total time=  17.5s\n",
      "[CV 1/3] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.884 total time=  11.7s\n",
      "[CV 2/3] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.885 total time=  12.6s\n",
      "[CV 3/3] END activation=relu, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.874 total time=  12.4s\n",
      "[CV 1/3] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.889 total time=  15.5s\n",
      "[CV 2/3] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.874 total time=  15.7s\n",
      "[CV 3/3] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.874 total time=  14.7s\n",
      "[CV 1/3] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.899 total time=  18.8s\n",
      "[CV 2/3] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.894 total time=  25.7s\n",
      "[CV 3/3] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.891 total time=  26.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.890 total time=  29.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.893 total time=  29.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.888 total time=  29.2s\n",
      "[CV 1/3] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.888 total time=  16.8s\n",
      "[CV 2/3] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.884 total time=  18.1s\n",
      "[CV 3/3] END activation=relu, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.879 total time=  18.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.867 total time=  17.3s\n",
      "[CV 2/3] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.867 total time=  13.5s\n",
      "[CV 3/3] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.873 total time=  14.3s\n",
      "[CV 1/3] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.899 total time=   9.8s\n",
      "[CV 2/3] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.895 total time=  11.8s\n",
      "[CV 3/3] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.894 total time=   8.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.895 total time=  17.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.898 total time=  17.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.889 total time=  17.2s\n",
      "[CV 1/3] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.884 total time=  12.0s\n",
      "[CV 2/3] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.884 total time=  12.8s\n",
      "[CV 3/3] END activation=relu, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.878 total time=  13.2s\n",
      "[CV 1/3] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.878 total time=  23.8s\n",
      "[CV 2/3] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.870 total time=  26.4s\n",
      "[CV 3/3] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.874 total time=  27.9s\n",
      "[CV 1/3] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.896 total time=  17.9s\n",
      "[CV 2/3] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.897 total time=  20.3s\n",
      "[CV 3/3] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.893 total time=  22.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.898 total time=  29.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.896 total time=  29.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.895 total time=  29.6s\n",
      "[CV 1/3] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.886 total time=   9.6s\n",
      "[CV 2/3] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.886 total time=   8.3s\n",
      "[CV 3/3] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.879 total time=   8.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.878 total time=  17.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.871 total time=  17.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.869 total time=  17.8s\n",
      "[CV 1/3] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.898 total time=  11.5s\n",
      "[CV 2/3] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.897 total time=   9.2s\n",
      "[CV 3/3] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.891 total time=  11.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.900 total time=  20.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.901 total time=  18.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.893 total time=  18.0s\n",
      "[CV 1/3] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.884 total time=   7.7s\n",
      "[CV 2/3] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.887 total time=   7.4s\n",
      "[CV 3/3] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.877 total time=   7.4s\n",
      "[CV 1/3] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.880 total time=  23.2s\n",
      "[CV 2/3] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.876 total time=  25.4s\n",
      "[CV 3/3] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.873 total time=  25.1s\n",
      "[CV 1/3] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.901 total time=  21.1s\n",
      "[CV 2/3] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.897 total time=  15.6s\n",
      "[CV 3/3] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.890 total time=  18.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.896 total time=  30.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.897 total time=  30.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.890 total time=  30.4s\n",
      "[CV 1/3] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.886 total time=   9.0s\n",
      "[CV 2/3] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.885 total time=   8.8s\n",
      "[CV 3/3] END activation=tanh, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.882 total time=   9.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.872 total time=  17.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.875 total time=  18.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.868 total time=  18.1s\n",
      "[CV 1/3] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.898 total time=  10.9s\n",
      "[CV 2/3] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.896 total time=  10.1s\n",
      "[CV 3/3] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.890 total time=  10.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.898 total time=  18.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.895 total time=  18.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.889 total time=  18.1s\n",
      "[CV 1/3] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.888 total time=   7.4s\n",
      "[CV 2/3] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.886 total time=   6.4s\n",
      "[CV 3/3] END activation=tanh, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.877 total time=   7.2s\n",
      "[CV 1/3] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.898 total time=  31.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.899 total time= 1.1min\n",
      "[CV 3/3] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.893 total time=  33.7s\n",
      "[CV 1/3] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.845 total time=   2.8s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.890 total time= 1.2min\n",
      "[CV 3/3] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.845 total time=   2.0s\n",
      "[CV 1/3] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.894 total time=  55.4s\n",
      "[CV 2/3] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.896 total time=  56.5s\n",
      "[CV 3/3] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.888 total time=  52.8s\n",
      "[CV 1/3] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.845 total time=   3.8s\n",
      "[CV 2/3] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.845 total time=   3.8s\n",
      "[CV 3/3] END activation=logistic, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.845 total time=   4.0s\n",
      "[CV 1/3] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.898 total time=  33.8s\n",
      "[CV 2/3] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.894 total time=  11.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.891 total time=  39.6s\n",
      "[CV 1/3] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.845 total time=   1.1s\n",
      "[CV 2/3] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.891 total time=  40.4s\n",
      "[CV 3/3] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.845 total time=   1.2s\n",
      "[CV 1/3] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.893 total time=  21.0s\n",
      "[CV 2/3] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.894 total time=  29.5s\n",
      "[CV 3/3] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.887 total time=  16.9s\n",
      "[CV 1/3] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.845 total time=   2.7s\n",
      "[CV 2/3] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.845 total time=   2.6s\n",
      "[CV 3/3] END activation=logistic, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.845 total time=   2.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.891 total time= 1.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.883 total time= 1.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.884 total time= 1.1min\n",
      "[CV 1/3] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.886 total time= 1.1min\n",
      "[CV 2/3] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.845 total time=   3.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.879 total time= 1.2min\n",
      "[CV 1/3] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.894 total time=  48.1s\n",
      "[CV 2/3] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.893 total time= 1.1min\n",
      "[CV 3/3] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.889 total time=  50.1s\n",
      "[CV 1/3] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.845 total time=   4.0s\n",
      "[CV 2/3] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.845 total time=   4.0s\n",
      "[CV 3/3] END activation=logistic, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.845 total time=   3.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/3] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.898 total time=  39.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/3] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.892 total time=  38.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/3] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.894 total time=  40.1s\n",
      "[CV 1/3] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.845 total time=   1.2s\n",
      "[CV 2/3] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.845 total time=   1.2s\n",
      "[CV 3/3] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.845 total time=   1.4s\n",
      "[CV 1/3] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.893 total time=  28.3s\n",
      "[CV 2/3] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.894 total time=  37.0s\n",
      "[CV 3/3] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.888 total time=  22.6s\n",
      "[CV 1/3] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.845 total time=   2.9s\n",
      "[CV 2/3] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.845 total time=   2.8s\n",
      "[CV 3/3] END activation=logistic, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.845 total time=   2.9s\n",
      "[CV 1/3] END activation=identity, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.887 total time=   2.1s\n",
      "[CV 2/3] END activation=identity, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.888 total time=   1.9s\n",
      "[CV 3/3] END activation=identity, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.878 total time=   1.2s\n",
      "[CV 1/3] END activation=identity, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.887 total time=   0.8s\n",
      "[CV 2/3] END activation=identity, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.888 total time=   0.9s\n",
      "[CV 3/3] END activation=identity, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.879 total time=   1.1s\n",
      "[CV 1/3] END activation=identity, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.887 total time=   1.3s\n",
      "[CV 2/3] END activation=identity, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.888 total time=   0.8s\n",
      "[CV 3/3] END activation=identity, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.879 total time=   1.1s\n",
      "[CV 1/3] END activation=identity, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.884 total time=   3.4s\n",
      "[CV 2/3] END activation=identity, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.884 total time=   3.1s\n",
      "[CV 3/3] END activation=identity, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.879 total time=   3.6s\n",
      "[CV 1/3] END activation=identity, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.888 total time=   0.4s\n",
      "[CV 2/3] END activation=identity, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.886 total time=   0.3s\n",
      "[CV 3/3] END activation=identity, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.881 total time=   0.6s\n",
      "[CV 1/3] END activation=identity, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.887 total time=   0.8s\n",
      "[CV 2/3] END activation=identity, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.888 total time=   0.8s\n",
      "[CV 3/3] END activation=identity, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.878 total time=   0.8s\n",
      "[CV 1/3] END activation=identity, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.887 total time=   1.5s\n",
      "[CV 2/3] END activation=identity, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.888 total time=   1.0s\n",
      "[CV 3/3] END activation=identity, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.879 total time=   0.9s\n",
      "[CV 1/3] END activation=identity, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.884 total time=   3.2s\n",
      "[CV 2/3] END activation=identity, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.885 total time=   3.1s\n",
      "[CV 3/3] END activation=identity, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.878 total time=   3.3s\n",
      "[CV 1/3] END activation=identity, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.887 total time=   1.4s\n",
      "[CV 2/3] END activation=identity, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.890 total time=   0.4s\n",
      "[CV 3/3] END activation=identity, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.881 total time=   0.7s\n",
      "[CV 1/3] END activation=identity, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.887 total time=   0.9s\n",
      "[CV 2/3] END activation=identity, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.888 total time=   1.0s\n",
      "[CV 3/3] END activation=identity, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.879 total time=   1.0s\n",
      "[CV 1/3] END activation=identity, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.887 total time=   0.9s\n",
      "[CV 2/3] END activation=identity, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.887 total time=   1.0s\n",
      "[CV 3/3] END activation=identity, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.880 total time=   0.9s\n",
      "[CV 1/3] END activation=identity, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.883 total time=   3.2s\n",
      "[CV 2/3] END activation=identity, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.885 total time=   2.6s\n",
      "[CV 3/3] END activation=identity, alpha=0.001, hidden_layer_sizes=(100, 150), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.877 total time=   3.5s\n",
      "[CV 1/3] END activation=identity, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.887 total time=   0.3s\n",
      "[CV 2/3] END activation=identity, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.885 total time=   0.4s\n",
      "[CV 3/3] END activation=identity, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=adam;, score=0.882 total time=   0.5s\n",
      "[CV 1/3] END activation=identity, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.887 total time=   0.9s\n",
      "[CV 2/3] END activation=identity, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.888 total time=   0.8s\n",
      "[CV 3/3] END activation=identity, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=500, solver=sgd;, score=0.879 total time=   0.8s\n",
      "[CV 1/3] END activation=identity, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.887 total time=   0.8s\n",
      "[CV 2/3] END activation=identity, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.888 total time=   1.0s\n",
      "[CV 3/3] END activation=identity, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=adam;, score=0.880 total time=   1.1s\n",
      "[CV 1/3] END activation=identity, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.884 total time=   3.0s\n",
      "[CV 2/3] END activation=identity, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.881 total time=   3.1s\n",
      "[CV 3/3] END activation=identity, alpha=0.001, hidden_layer_sizes=(50, 100), learning_rate_init=0.0001, max_iter=500, solver=sgd;, score=0.876 total time=   2.9s\n",
      "Best parameters:\n",
      " {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (50, 100), 'learning_rate_init': 0.0001, 'max_iter': 500, 'solver': 'adam'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#definição e configuração do MLP e hiperparametros\n",
    "\n",
    "mlp = MLPClassifier()\n",
    "params = {\n",
    "    'hidden_layer_sizes': [(100, 150),(50,100)],\n",
    "    'activation': ['relu','tanh','logistic', 'identity'],\n",
    "    'learning_rate_init': [0.001,0.0001],\n",
    "    'alpha': [0.01,0.001],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'max_iter': [500]\n",
    "}\n",
    "gs_mlp = GridSearchCV(mlp, params, cv = 3, verbose = 3,scoring = 'accuracy')\n",
    "\n",
    "gs_mlp.fit(X_train, y_train)\n",
    "print(\"Best parameters:\\n\", gs_mlp.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "189f9cc7-a227-4aca-9921-c65004d1c460",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimized_mlp = MLPClassifier(\n",
    "#    hidden_layer_sizes = (50, 100),\n",
    "#    activation = 'tanh',\n",
    "#    alpha = 0.01,\n",
    "#    learning_rate_init = 0.001,\n",
    "#    max_iter = 600,\n",
    "#    solver = 'adam',\n",
    "#     verbose = True\n",
    "#)\n",
    "#optimized_mlp.fit(X_train, y_train)\n",
    "#optimized_mlp_pred = optimized_mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b4f14e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Iteration 1, loss = 0.36506949\n",
      "Iteration 2, loss = 0.29021444\n",
      "Iteration 3, loss = 0.28712535\n",
      "Iteration 4, loss = 0.28587333\n",
      "Iteration 5, loss = 0.28178368\n",
      "Iteration 6, loss = 0.27839599\n",
      "Iteration 7, loss = 0.27299333\n",
      "Iteration 8, loss = 0.26783594\n",
      "Iteration 9, loss = 0.26307945\n",
      "Iteration 10, loss = 0.25835146\n",
      "Iteration 11, loss = 0.25786153\n",
      "Iteration 12, loss = 0.25286422\n",
      "Iteration 13, loss = 0.25141075\n",
      "Iteration 14, loss = 0.24954044\n",
      "Iteration 15, loss = 0.24845093\n",
      "Iteration 16, loss = 0.24636004\n",
      "Iteration 17, loss = 0.24661645\n",
      "Iteration 18, loss = 0.24607652\n",
      "Iteration 19, loss = 0.24443316\n",
      "Iteration 20, loss = 0.24446231\n",
      "Iteration 21, loss = 0.24286046\n",
      "Iteration 22, loss = 0.24121333\n",
      "Iteration 23, loss = 0.24104397\n",
      "Iteration 24, loss = 0.24067111\n",
      "Iteration 25, loss = 0.23806004\n",
      "Iteration 26, loss = 0.23853407\n",
      "Iteration 27, loss = 0.23689237\n",
      "Iteration 28, loss = 0.23925055\n",
      "Iteration 29, loss = 0.23646674\n",
      "Iteration 30, loss = 0.23514896\n",
      "Iteration 31, loss = 0.23430953\n",
      "Iteration 32, loss = 0.23342986\n",
      "Iteration 33, loss = 0.23286358\n",
      "Iteration 34, loss = 0.23123775\n",
      "Iteration 35, loss = 0.23066759\n",
      "Iteration 36, loss = 0.23002746\n",
      "Iteration 37, loss = 0.22971918\n",
      "Iteration 38, loss = 0.22862093\n",
      "Iteration 39, loss = 0.22881897\n",
      "Iteration 40, loss = 0.22728399\n",
      "Iteration 41, loss = 0.22597137\n",
      "Iteration 42, loss = 0.22482396\n",
      "Iteration 43, loss = 0.22422411\n",
      "Iteration 44, loss = 0.22233346\n",
      "Iteration 45, loss = 0.22157865\n",
      "Iteration 46, loss = 0.22025868\n",
      "Iteration 47, loss = 0.22003871\n",
      "Iteration 48, loss = 0.22003609\n",
      "Iteration 49, loss = 0.22211551\n",
      "Iteration 50, loss = 0.21780183\n",
      "Iteration 51, loss = 0.21944834\n",
      "Iteration 52, loss = 0.21821099\n",
      "Iteration 53, loss = 0.21661037\n",
      "Iteration 54, loss = 0.21627444\n",
      "Iteration 55, loss = 0.21563973\n",
      "Iteration 56, loss = 0.21479855\n",
      "Iteration 57, loss = 0.21289972\n",
      "Iteration 58, loss = 0.21243916\n",
      "Iteration 59, loss = 0.20940838\n",
      "Iteration 60, loss = 0.21037719\n",
      "Iteration 61, loss = 0.20831815\n",
      "Iteration 62, loss = 0.20799600\n",
      "Iteration 63, loss = 0.20837594\n",
      "Iteration 64, loss = 0.20843229\n",
      "Iteration 65, loss = 0.20502091\n",
      "Iteration 66, loss = 0.20414596\n",
      "Iteration 67, loss = 0.20490600\n",
      "Iteration 68, loss = 0.20369738\n",
      "Iteration 69, loss = 0.20409342\n",
      "Iteration 70, loss = 0.20190018\n",
      "Iteration 71, loss = 0.20305517\n",
      "Iteration 72, loss = 0.20187961\n",
      "Iteration 73, loss = 0.19766884\n",
      "Iteration 74, loss = 0.19642041\n",
      "Iteration 75, loss = 0.19711761\n",
      "Iteration 76, loss = 0.19718906\n",
      "Iteration 77, loss = 0.19700921\n",
      "Iteration 78, loss = 0.19639395\n",
      "Iteration 79, loss = 0.19310778\n",
      "Iteration 80, loss = 0.19295086\n",
      "Iteration 81, loss = 0.19215447\n",
      "Iteration 82, loss = 0.19177755\n",
      "Iteration 83, loss = 0.18918143\n",
      "Iteration 84, loss = 0.18891001\n",
      "Iteration 85, loss = 0.19028103\n",
      "Iteration 86, loss = 0.18943675\n",
      "Iteration 87, loss = 0.18513143\n",
      "Iteration 88, loss = 0.18580575\n",
      "Iteration 89, loss = 0.18635503\n",
      "Iteration 90, loss = 0.18551784\n",
      "Iteration 91, loss = 0.18316964\n",
      "Iteration 92, loss = 0.18214391\n",
      "Iteration 93, loss = 0.18184581\n",
      "Iteration 94, loss = 0.17984731\n",
      "Iteration 95, loss = 0.17988201\n",
      "Iteration 96, loss = 0.17955024\n",
      "Iteration 97, loss = 0.17869174\n",
      "Iteration 98, loss = 0.18004947\n",
      "Iteration 99, loss = 0.17621577\n",
      "Iteration 100, loss = 0.17510517\n",
      "Iteration 101, loss = 0.17520101\n",
      "Iteration 102, loss = 0.17339345\n",
      "Iteration 103, loss = 0.17237132\n",
      "Iteration 104, loss = 0.17092878\n",
      "Iteration 105, loss = 0.17192672\n",
      "Iteration 106, loss = 0.17013419\n",
      "Iteration 107, loss = 0.17042221\n",
      "Iteration 108, loss = 0.16757374\n",
      "Iteration 109, loss = 0.16829361\n",
      "Iteration 110, loss = 0.16758754\n",
      "Iteration 111, loss = 0.16807525\n",
      "Iteration 112, loss = 0.16588946\n",
      "Iteration 113, loss = 0.16514475\n",
      "Iteration 114, loss = 0.16491457\n",
      "Iteration 115, loss = 0.16401977\n",
      "Iteration 116, loss = 0.16056367\n",
      "Iteration 117, loss = 0.16135119\n",
      "Iteration 118, loss = 0.16072572\n",
      "Iteration 119, loss = 0.16111851\n",
      "Iteration 120, loss = 0.15864467\n",
      "Iteration 121, loss = 0.15740508\n",
      "Iteration 122, loss = 0.15615592\n",
      "Iteration 123, loss = 0.15589076\n",
      "Iteration 124, loss = 0.15480738\n",
      "Iteration 125, loss = 0.15486341\n",
      "Iteration 126, loss = 0.15637519\n",
      "Iteration 127, loss = 0.15329097\n",
      "Iteration 128, loss = 0.15128634\n",
      "Iteration 129, loss = 0.15139304\n",
      "Iteration 130, loss = 0.14967917\n",
      "Iteration 131, loss = 0.15110265\n",
      "Iteration 132, loss = 0.14664427\n",
      "Iteration 133, loss = 0.14671123\n",
      "Iteration 134, loss = 0.14764596\n",
      "Iteration 135, loss = 0.14524227\n",
      "Iteration 136, loss = 0.14587792\n",
      "Iteration 137, loss = 0.14434831\n",
      "Iteration 138, loss = 0.14143155\n",
      "Iteration 139, loss = 0.14243171\n",
      "Iteration 140, loss = 0.14117833\n",
      "Iteration 141, loss = 0.14016052\n",
      "Iteration 142, loss = 0.13873196\n",
      "Iteration 143, loss = 0.13774941\n",
      "Iteration 144, loss = 0.13902884\n",
      "Iteration 145, loss = 0.13819652\n",
      "Iteration 146, loss = 0.13501955\n",
      "Iteration 147, loss = 0.13598171\n",
      "Iteration 148, loss = 0.13422387\n",
      "Iteration 149, loss = 0.13310709\n",
      "Iteration 150, loss = 0.13251172\n",
      "Iteration 151, loss = 0.13252072\n",
      "Iteration 152, loss = 0.13144047\n",
      "Iteration 153, loss = 0.13095835\n",
      "Iteration 154, loss = 0.12945760\n",
      "Iteration 155, loss = 0.12865806\n",
      "Iteration 156, loss = 0.12814837\n",
      "Iteration 157, loss = 0.12748394\n",
      "Iteration 158, loss = 0.12559897\n",
      "Iteration 159, loss = 0.12504112\n",
      "Iteration 160, loss = 0.12608833\n",
      "Iteration 161, loss = 0.12375609\n",
      "Iteration 162, loss = 0.12234194\n",
      "Iteration 163, loss = 0.12161740\n",
      "Iteration 164, loss = 0.12030710\n",
      "Iteration 165, loss = 0.11883005\n",
      "Iteration 166, loss = 0.11902687\n",
      "Iteration 167, loss = 0.11966120\n",
      "Iteration 168, loss = 0.11778528\n",
      "Iteration 169, loss = 0.11854025\n",
      "Iteration 170, loss = 0.11671797\n",
      "Iteration 171, loss = 0.11648375\n",
      "Iteration 172, loss = 0.11506776\n",
      "Iteration 173, loss = 0.11640969\n",
      "Iteration 174, loss = 0.11194635\n",
      "Iteration 175, loss = 0.11182853\n",
      "Iteration 176, loss = 0.11104732\n",
      "Iteration 177, loss = 0.11209709\n",
      "Iteration 178, loss = 0.10919355\n",
      "Iteration 179, loss = 0.10963712\n",
      "Iteration 180, loss = 0.10925604\n",
      "Iteration 181, loss = 0.10764919\n",
      "Iteration 182, loss = 0.10644434\n",
      "Iteration 183, loss = 0.10673623\n",
      "Iteration 184, loss = 0.10590233\n",
      "Iteration 185, loss = 0.10423017\n",
      "Iteration 186, loss = 0.10560272\n",
      "Iteration 187, loss = 0.10418210\n",
      "Iteration 188, loss = 0.10169765\n",
      "Iteration 189, loss = 0.10171380\n",
      "Iteration 190, loss = 0.10010583\n",
      "Iteration 191, loss = 0.10003122\n",
      "Iteration 192, loss = 0.09965646\n",
      "Iteration 193, loss = 0.09813386\n",
      "Iteration 194, loss = 0.09829455\n",
      "Iteration 195, loss = 0.09739416\n",
      "Iteration 196, loss = 0.09835165\n",
      "Iteration 197, loss = 0.09719650\n",
      "Iteration 198, loss = 0.09735984\n",
      "Iteration 199, loss = 0.09578785\n",
      "Iteration 200, loss = 0.09365632\n",
      "Iteration 201, loss = 0.09392455\n",
      "Iteration 202, loss = 0.09435633\n",
      "Iteration 203, loss = 0.09438095\n",
      "Iteration 204, loss = 0.09122093\n",
      "Iteration 205, loss = 0.09327444\n",
      "Iteration 206, loss = 0.09000005\n",
      "Iteration 207, loss = 0.09075843\n",
      "Iteration 208, loss = 0.09085132\n",
      "Iteration 209, loss = 0.08941711\n",
      "Iteration 210, loss = 0.08991747\n",
      "Iteration 211, loss = 0.08768408\n",
      "Iteration 212, loss = 0.08693092\n",
      "Iteration 213, loss = 0.08634767\n",
      "Iteration 214, loss = 0.08627372\n",
      "Iteration 215, loss = 0.08542827\n",
      "Iteration 216, loss = 0.08500800\n",
      "Iteration 217, loss = 0.08477683\n",
      "Iteration 218, loss = 0.08524567\n",
      "Iteration 219, loss = 0.08285665\n",
      "Iteration 220, loss = 0.08240548\n",
      "Iteration 221, loss = 0.08187645\n",
      "Iteration 222, loss = 0.08005391\n",
      "Iteration 223, loss = 0.08087599\n",
      "Iteration 224, loss = 0.08004892\n",
      "Iteration 225, loss = 0.07926660\n",
      "Iteration 226, loss = 0.07859465\n",
      "Iteration 227, loss = 0.07867262\n",
      "Iteration 228, loss = 0.07869047\n",
      "Iteration 229, loss = 0.07800412\n",
      "Iteration 230, loss = 0.07663735\n",
      "Iteration 231, loss = 0.07733805\n",
      "Iteration 232, loss = 0.07665082\n",
      "Iteration 233, loss = 0.07554302\n",
      "Iteration 234, loss = 0.07421285\n",
      "Iteration 235, loss = 0.07487833\n",
      "Iteration 236, loss = 0.07489660\n",
      "Iteration 237, loss = 0.07354421\n",
      "Iteration 238, loss = 0.07299108\n",
      "Iteration 239, loss = 0.07365064\n",
      "Iteration 240, loss = 0.07151546\n",
      "Iteration 241, loss = 0.07090828\n",
      "Iteration 242, loss = 0.07097708\n",
      "Iteration 243, loss = 0.07122064\n",
      "Iteration 244, loss = 0.07016104\n",
      "Iteration 245, loss = 0.07056216\n",
      "Iteration 246, loss = 0.07081592\n",
      "Iteration 247, loss = 0.06927161\n",
      "Iteration 248, loss = 0.06872006\n",
      "Iteration 249, loss = 0.06805247\n",
      "Iteration 250, loss = 0.06963269\n",
      "Iteration 251, loss = 0.06775081\n",
      "Iteration 252, loss = 0.06713520\n",
      "Iteration 253, loss = 0.06635996\n",
      "Iteration 254, loss = 0.06569943\n",
      "Iteration 255, loss = 0.06586325\n",
      "Iteration 256, loss = 0.06649183\n",
      "Iteration 257, loss = 0.06579174\n",
      "Iteration 258, loss = 0.06332134\n",
      "Iteration 259, loss = 0.06656090\n",
      "Iteration 260, loss = 0.06405656\n",
      "Iteration 261, loss = 0.06299329\n",
      "Iteration 262, loss = 0.06298824\n",
      "Iteration 263, loss = 0.06167132\n",
      "Iteration 264, loss = 0.06187135\n",
      "Iteration 265, loss = 0.06083906\n",
      "Iteration 266, loss = 0.06192297\n",
      "Iteration 267, loss = 0.06035085\n",
      "Iteration 268, loss = 0.05989100\n",
      "Iteration 269, loss = 0.06024437\n",
      "Iteration 270, loss = 0.05967644\n",
      "Iteration 271, loss = 0.05995723\n",
      "Iteration 272, loss = 0.05987903\n",
      "Iteration 273, loss = 0.05758927\n",
      "Iteration 274, loss = 0.05858805\n",
      "Iteration 275, loss = 0.05789517\n",
      "Iteration 276, loss = 0.05687321\n",
      "Iteration 277, loss = 0.05831930\n",
      "Iteration 278, loss = 0.05699926\n",
      "Iteration 279, loss = 0.05670319\n",
      "Iteration 280, loss = 0.05665845\n",
      "Iteration 281, loss = 0.05643798\n",
      "Iteration 282, loss = 0.05617254\n",
      "Iteration 283, loss = 0.05566298\n",
      "Iteration 284, loss = 0.05649204\n",
      "Iteration 285, loss = 0.05587742\n",
      "Iteration 286, loss = 0.05519500\n",
      "Iteration 287, loss = 0.05473893\n",
      "Iteration 288, loss = 0.05398503\n",
      "Iteration 289, loss = 0.05400799\n",
      "Iteration 290, loss = 0.05448065\n",
      "Iteration 291, loss = 0.05402387\n",
      "Iteration 292, loss = 0.05364214\n",
      "Iteration 293, loss = 0.05363546\n",
      "Iteration 294, loss = 0.05296621\n",
      "Iteration 295, loss = 0.05351134\n",
      "Iteration 296, loss = 0.05249402\n",
      "Iteration 297, loss = 0.05125436\n",
      "Iteration 298, loss = 0.05144217\n",
      "Iteration 299, loss = 0.05080674\n",
      "Iteration 300, loss = 0.05162025\n",
      "Iteration 301, loss = 0.05309675\n",
      "Iteration 302, loss = 0.05080315\n",
      "Iteration 303, loss = 0.05026229\n",
      "Iteration 304, loss = 0.04960211\n",
      "Iteration 305, loss = 0.04961449\n",
      "Iteration 306, loss = 0.04955049\n",
      "Iteration 307, loss = 0.04897446\n",
      "Iteration 308, loss = 0.04890128\n",
      "Iteration 309, loss = 0.04902038\n",
      "Iteration 310, loss = 0.04826410\n",
      "Iteration 311, loss = 0.04896579\n",
      "Iteration 312, loss = 0.04816724\n",
      "Iteration 313, loss = 0.04793166\n",
      "Iteration 314, loss = 0.04816663\n",
      "Iteration 315, loss = 0.04809664\n",
      "Iteration 316, loss = 0.04775150\n",
      "Iteration 317, loss = 0.04723412\n",
      "Iteration 318, loss = 0.04752589\n",
      "Iteration 319, loss = 0.04697501\n",
      "Iteration 320, loss = 0.04633407\n",
      "Iteration 321, loss = 0.04637926\n",
      "Iteration 322, loss = 0.04627853\n",
      "Iteration 323, loss = 0.04619004\n",
      "Iteration 324, loss = 0.04566247\n",
      "Iteration 325, loss = 0.04537802\n",
      "Iteration 326, loss = 0.04569353\n",
      "Iteration 327, loss = 0.04605022\n",
      "Iteration 328, loss = 0.04447756\n",
      "Iteration 329, loss = 0.04561938\n",
      "Iteration 330, loss = 0.04489538\n",
      "Iteration 331, loss = 0.04586528\n",
      "Iteration 332, loss = 0.04516546\n",
      "Iteration 333, loss = 0.04388024\n",
      "Iteration 334, loss = 0.04473921\n",
      "Iteration 335, loss = 0.04398410\n",
      "Iteration 336, loss = 0.04379523\n",
      "Iteration 337, loss = 0.04341106\n",
      "Iteration 338, loss = 0.04333153\n",
      "Iteration 339, loss = 0.04288705\n",
      "Iteration 340, loss = 0.04306326\n",
      "Iteration 341, loss = 0.04426629\n",
      "Iteration 342, loss = 0.04300608\n",
      "Iteration 343, loss = 0.04222929\n",
      "Iteration 344, loss = 0.04248833\n",
      "Iteration 345, loss = 0.04285632\n",
      "Iteration 346, loss = 0.04258170\n",
      "Iteration 347, loss = 0.04237854\n",
      "Iteration 348, loss = 0.04166709\n",
      "Iteration 349, loss = 0.04198831\n",
      "Iteration 350, loss = 0.04117485\n",
      "Iteration 351, loss = 0.04114788\n",
      "Iteration 352, loss = 0.04111599\n",
      "Iteration 353, loss = 0.04096277\n",
      "Iteration 354, loss = 0.04189135\n",
      "Iteration 355, loss = 0.04086595\n",
      "Iteration 356, loss = 0.04016107\n",
      "Iteration 357, loss = 0.04062393\n",
      "Iteration 358, loss = 0.04011555\n",
      "Iteration 359, loss = 0.04079002\n",
      "Iteration 360, loss = 0.04052233\n",
      "Iteration 361, loss = 0.04037496\n",
      "Iteration 362, loss = 0.04007219\n",
      "Iteration 363, loss = 0.03939320\n",
      "Iteration 364, loss = 0.04002258\n",
      "Iteration 365, loss = 0.03983692\n",
      "Iteration 366, loss = 0.03973749\n",
      "Iteration 367, loss = 0.04001886\n",
      "Iteration 368, loss = 0.04001078\n",
      "Iteration 369, loss = 0.03989990\n",
      "Iteration 370, loss = 0.03943827\n",
      "Iteration 371, loss = 0.03975381\n",
      "Iteration 372, loss = 0.03914315\n",
      "Iteration 373, loss = 0.03854704\n",
      "Iteration 374, loss = 0.03964758\n",
      "Iteration 375, loss = 0.03882665\n",
      "Iteration 376, loss = 0.03885369\n",
      "Iteration 377, loss = 0.03831120\n",
      "Iteration 378, loss = 0.03885098\n",
      "Iteration 379, loss = 0.03809874\n",
      "Iteration 380, loss = 0.03858807\n",
      "Iteration 381, loss = 0.03882395\n",
      "Iteration 382, loss = 0.03824014\n",
      "Iteration 383, loss = 0.03772497\n",
      "Iteration 384, loss = 0.03742127\n",
      "Iteration 385, loss = 0.03779883\n",
      "Iteration 386, loss = 0.03723140\n",
      "Iteration 387, loss = 0.03747060\n",
      "Iteration 388, loss = 0.03709327\n",
      "Iteration 389, loss = 0.03771724\n",
      "Iteration 390, loss = 0.03755405\n",
      "Iteration 391, loss = 0.03723518\n",
      "Iteration 392, loss = 0.03669330\n",
      "Iteration 393, loss = 0.03693366\n",
      "Iteration 394, loss = 0.03702044\n",
      "Iteration 395, loss = 0.03674700\n",
      "Iteration 396, loss = 0.03678952\n",
      "Iteration 397, loss = 0.03667657\n",
      "Iteration 398, loss = 0.03606789\n",
      "Iteration 399, loss = 0.03617991\n",
      "Iteration 400, loss = 0.03620802\n",
      "Iteration 401, loss = 0.03638295\n",
      "Iteration 402, loss = 0.03581903\n",
      "Iteration 403, loss = 0.03588737\n",
      "Iteration 404, loss = 0.03574920\n",
      "Iteration 405, loss = 0.03542186\n",
      "Iteration 406, loss = 0.03666778\n",
      "Iteration 407, loss = 0.03543773\n",
      "Iteration 408, loss = 0.03655860\n",
      "Iteration 409, loss = 0.03613583\n",
      "Iteration 410, loss = 0.03623234\n",
      "Iteration 411, loss = 0.03652744\n",
      "Iteration 412, loss = 0.03599865\n",
      "Iteration 413, loss = 0.03604837\n",
      "Iteration 414, loss = 0.03558987\n",
      "Iteration 415, loss = 0.03572877\n",
      "Iteration 416, loss = 0.03514215\n",
      "Iteration 417, loss = 0.03522323\n",
      "Iteration 418, loss = 0.03481844\n",
      "Iteration 419, loss = 0.03459101\n",
      "Iteration 420, loss = 0.03434614\n",
      "Iteration 421, loss = 0.03485891\n",
      "Iteration 422, loss = 0.03453274\n",
      "Iteration 423, loss = 0.03398941\n",
      "Iteration 424, loss = 0.03469481\n",
      "Iteration 425, loss = 0.03465518\n",
      "Iteration 426, loss = 0.03422636\n",
      "Iteration 427, loss = 0.03444957\n",
      "Iteration 428, loss = 0.03439887\n",
      "Iteration 429, loss = 0.03421574\n",
      "Iteration 430, loss = 0.03423166\n",
      "Iteration 431, loss = 0.03369235\n",
      "Iteration 432, loss = 0.03411518\n",
      "Iteration 433, loss = 0.03412111\n",
      "Iteration 434, loss = 0.03386113\n",
      "Iteration 435, loss = 0.03490648\n",
      "Iteration 436, loss = 0.03439594\n",
      "Iteration 437, loss = 0.03625676\n",
      "Iteration 438, loss = 0.03496513\n",
      "Iteration 439, loss = 0.03345327\n",
      "Iteration 440, loss = 0.03351227\n",
      "Iteration 441, loss = 0.03334446\n",
      "Iteration 442, loss = 0.03358400\n",
      "Iteration 443, loss = 0.03367485\n",
      "Iteration 444, loss = 0.03491126\n",
      "Iteration 445, loss = 0.03323865\n",
      "Iteration 446, loss = 0.03364489\n",
      "Iteration 447, loss = 0.03291687\n",
      "Iteration 448, loss = 0.03369865\n",
      "Iteration 449, loss = 0.03462966\n",
      "Iteration 450, loss = 0.03347960\n",
      "Iteration 451, loss = 0.03299531\n",
      "Iteration 452, loss = 0.03281853\n",
      "Iteration 453, loss = 0.03272292\n",
      "Iteration 454, loss = 0.03317639\n",
      "Iteration 455, loss = 0.03317943\n",
      "Iteration 456, loss = 0.03314986\n",
      "Iteration 457, loss = 0.03262707\n",
      "Iteration 458, loss = 0.03261192\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV 1/3] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=600, solver=adam, verbose=True;, score=0.876 total time=  29.0s\n",
      "Iteration 1, loss = 0.37784337\n",
      "Iteration 2, loss = 0.29831399\n",
      "Iteration 3, loss = 0.29401777\n",
      "Iteration 4, loss = 0.29118390\n",
      "Iteration 5, loss = 0.28670668\n",
      "Iteration 6, loss = 0.28263293\n",
      "Iteration 7, loss = 0.27651085\n",
      "Iteration 8, loss = 0.27224483\n",
      "Iteration 9, loss = 0.26519672\n",
      "Iteration 10, loss = 0.26124941\n",
      "Iteration 11, loss = 0.25894638\n",
      "Iteration 12, loss = 0.25470199\n",
      "Iteration 13, loss = 0.25195370\n",
      "Iteration 14, loss = 0.25208757\n",
      "Iteration 15, loss = 0.24764927\n",
      "Iteration 16, loss = 0.24736868\n",
      "Iteration 17, loss = 0.24510914\n",
      "Iteration 18, loss = 0.24448450\n",
      "Iteration 19, loss = 0.24400107\n",
      "Iteration 20, loss = 0.24286803\n",
      "Iteration 21, loss = 0.24089536\n",
      "Iteration 22, loss = 0.24027416\n",
      "Iteration 23, loss = 0.23957638\n",
      "Iteration 24, loss = 0.23728845\n",
      "Iteration 25, loss = 0.23549749\n",
      "Iteration 26, loss = 0.23791097\n",
      "Iteration 27, loss = 0.23721107\n",
      "Iteration 28, loss = 0.23299761\n",
      "Iteration 29, loss = 0.23344332\n",
      "Iteration 30, loss = 0.23026338\n",
      "Iteration 31, loss = 0.23233974\n",
      "Iteration 32, loss = 0.22979199\n",
      "Iteration 33, loss = 0.22945244\n",
      "Iteration 34, loss = 0.22865282\n",
      "Iteration 35, loss = 0.22794818\n",
      "Iteration 36, loss = 0.22651091\n",
      "Iteration 37, loss = 0.22597243\n",
      "Iteration 38, loss = 0.22525605\n",
      "Iteration 39, loss = 0.22499038\n",
      "Iteration 40, loss = 0.22280332\n",
      "Iteration 41, loss = 0.22191467\n",
      "Iteration 42, loss = 0.22069191\n",
      "Iteration 43, loss = 0.22032275\n",
      "Iteration 44, loss = 0.21955142\n",
      "Iteration 45, loss = 0.21864879\n",
      "Iteration 46, loss = 0.21877716\n",
      "Iteration 47, loss = 0.21686534\n",
      "Iteration 48, loss = 0.21674673\n",
      "Iteration 49, loss = 0.21642432\n",
      "Iteration 50, loss = 0.21583277\n",
      "Iteration 51, loss = 0.21286335\n",
      "Iteration 52, loss = 0.21386876\n",
      "Iteration 53, loss = 0.21300640\n",
      "Iteration 54, loss = 0.21234866\n",
      "Iteration 55, loss = 0.21041859\n",
      "Iteration 56, loss = 0.21106050\n",
      "Iteration 57, loss = 0.21019150\n",
      "Iteration 58, loss = 0.21122279\n",
      "Iteration 59, loss = 0.20986953\n",
      "Iteration 60, loss = 0.20608839\n",
      "Iteration 61, loss = 0.20637792\n",
      "Iteration 62, loss = 0.20655845\n",
      "Iteration 63, loss = 0.20433894\n",
      "Iteration 64, loss = 0.20277345\n",
      "Iteration 65, loss = 0.20439049\n",
      "Iteration 66, loss = 0.20338826\n",
      "Iteration 67, loss = 0.20237803\n",
      "Iteration 68, loss = 0.20157585\n",
      "Iteration 69, loss = 0.20042694\n",
      "Iteration 70, loss = 0.19886701\n",
      "Iteration 71, loss = 0.19781877\n",
      "Iteration 72, loss = 0.19917663\n",
      "Iteration 73, loss = 0.19772957\n",
      "Iteration 74, loss = 0.19666273\n",
      "Iteration 75, loss = 0.19505521\n",
      "Iteration 76, loss = 0.19408712\n",
      "Iteration 77, loss = 0.19333144\n",
      "Iteration 78, loss = 0.19277009\n",
      "Iteration 79, loss = 0.19224288\n",
      "Iteration 80, loss = 0.19185401\n",
      "Iteration 81, loss = 0.19059396\n",
      "Iteration 82, loss = 0.19197774\n",
      "Iteration 83, loss = 0.19017673\n",
      "Iteration 84, loss = 0.18935405\n",
      "Iteration 85, loss = 0.18833880\n",
      "Iteration 86, loss = 0.18584906\n",
      "Iteration 87, loss = 0.18533608\n",
      "Iteration 88, loss = 0.18514414\n",
      "Iteration 89, loss = 0.18345023\n",
      "Iteration 90, loss = 0.18265071\n",
      "Iteration 91, loss = 0.18151116\n",
      "Iteration 92, loss = 0.18275845\n",
      "Iteration 93, loss = 0.18192743\n",
      "Iteration 94, loss = 0.18106399\n",
      "Iteration 95, loss = 0.18167143\n",
      "Iteration 96, loss = 0.17965484\n",
      "Iteration 97, loss = 0.17832308\n",
      "Iteration 98, loss = 0.17820670\n",
      "Iteration 99, loss = 0.17430951\n",
      "Iteration 100, loss = 0.17339149\n",
      "Iteration 101, loss = 0.17305891\n",
      "Iteration 102, loss = 0.17311630\n",
      "Iteration 103, loss = 0.17317878\n",
      "Iteration 104, loss = 0.17100660\n",
      "Iteration 105, loss = 0.16821019\n",
      "Iteration 106, loss = 0.17049479\n",
      "Iteration 107, loss = 0.16755342\n",
      "Iteration 108, loss = 0.16732702\n",
      "Iteration 109, loss = 0.16609144\n",
      "Iteration 110, loss = 0.16612046\n",
      "Iteration 111, loss = 0.16451911\n",
      "Iteration 112, loss = 0.16411036\n",
      "Iteration 113, loss = 0.16475070\n",
      "Iteration 114, loss = 0.16339505\n",
      "Iteration 115, loss = 0.16319522\n",
      "Iteration 116, loss = 0.15955833\n",
      "Iteration 117, loss = 0.15885535\n",
      "Iteration 118, loss = 0.15721200\n",
      "Iteration 119, loss = 0.15803258\n",
      "Iteration 120, loss = 0.15524854\n",
      "Iteration 121, loss = 0.15357791\n",
      "Iteration 122, loss = 0.15387680\n",
      "Iteration 123, loss = 0.15396358\n",
      "Iteration 124, loss = 0.15137290\n",
      "Iteration 125, loss = 0.14999689\n",
      "Iteration 126, loss = 0.15175843\n",
      "Iteration 127, loss = 0.14956738\n",
      "Iteration 128, loss = 0.15017245\n",
      "Iteration 129, loss = 0.15006742\n",
      "Iteration 130, loss = 0.14640350\n",
      "Iteration 131, loss = 0.14764958\n",
      "Iteration 132, loss = 0.14467375\n",
      "Iteration 133, loss = 0.14482886\n",
      "Iteration 134, loss = 0.14295531\n",
      "Iteration 135, loss = 0.14251341\n",
      "Iteration 136, loss = 0.14080377\n",
      "Iteration 137, loss = 0.13903303\n",
      "Iteration 138, loss = 0.13887501\n",
      "Iteration 139, loss = 0.13788789\n",
      "Iteration 140, loss = 0.13596070\n",
      "Iteration 141, loss = 0.13641482\n",
      "Iteration 142, loss = 0.13584893\n",
      "Iteration 143, loss = 0.13338759\n",
      "Iteration 144, loss = 0.13488585\n",
      "Iteration 145, loss = 0.13138124\n",
      "Iteration 146, loss = 0.13196057\n",
      "Iteration 147, loss = 0.13413715\n",
      "Iteration 148, loss = 0.13125618\n",
      "Iteration 149, loss = 0.12887238\n",
      "Iteration 150, loss = 0.12854553\n",
      "Iteration 151, loss = 0.12664793\n",
      "Iteration 152, loss = 0.12587317\n",
      "Iteration 153, loss = 0.12846979\n",
      "Iteration 154, loss = 0.12582750\n",
      "Iteration 155, loss = 0.12359723\n",
      "Iteration 156, loss = 0.12266241\n",
      "Iteration 157, loss = 0.12120692\n",
      "Iteration 158, loss = 0.12144212\n",
      "Iteration 159, loss = 0.11959576\n",
      "Iteration 160, loss = 0.11851659\n",
      "Iteration 161, loss = 0.11740710\n",
      "Iteration 162, loss = 0.11737513\n",
      "Iteration 163, loss = 0.11604289\n",
      "Iteration 164, loss = 0.11440082\n",
      "Iteration 165, loss = 0.11369101\n",
      "Iteration 166, loss = 0.11325130\n",
      "Iteration 167, loss = 0.11380639\n",
      "Iteration 168, loss = 0.11302736\n",
      "Iteration 169, loss = 0.11171957\n",
      "Iteration 170, loss = 0.10919805\n",
      "Iteration 171, loss = 0.11005644\n",
      "Iteration 172, loss = 0.10992314\n",
      "Iteration 173, loss = 0.10944650\n",
      "Iteration 174, loss = 0.10741824\n",
      "Iteration 175, loss = 0.10721735\n",
      "Iteration 176, loss = 0.10631717\n",
      "Iteration 177, loss = 0.10462826\n",
      "Iteration 178, loss = 0.10269684\n",
      "Iteration 179, loss = 0.10289521\n",
      "Iteration 180, loss = 0.10152907\n",
      "Iteration 181, loss = 0.10274221\n",
      "Iteration 182, loss = 0.10105243\n",
      "Iteration 183, loss = 0.09958276\n",
      "Iteration 184, loss = 0.09862715\n",
      "Iteration 185, loss = 0.10060097\n",
      "Iteration 186, loss = 0.10090981\n",
      "Iteration 187, loss = 0.09733326\n",
      "Iteration 188, loss = 0.09628833\n",
      "Iteration 189, loss = 0.09549401\n",
      "Iteration 190, loss = 0.09472160\n",
      "Iteration 191, loss = 0.09419997\n",
      "Iteration 192, loss = 0.09322324\n",
      "Iteration 193, loss = 0.09244773\n",
      "Iteration 194, loss = 0.09172316\n",
      "Iteration 195, loss = 0.09176832\n",
      "Iteration 196, loss = 0.09059246\n",
      "Iteration 197, loss = 0.09007411\n",
      "Iteration 198, loss = 0.08768251\n",
      "Iteration 199, loss = 0.08798718\n",
      "Iteration 200, loss = 0.08772656\n",
      "Iteration 201, loss = 0.08657625\n",
      "Iteration 202, loss = 0.08670135\n",
      "Iteration 203, loss = 0.08504646\n",
      "Iteration 204, loss = 0.08468681\n",
      "Iteration 205, loss = 0.08223801\n",
      "Iteration 206, loss = 0.08468850\n",
      "Iteration 207, loss = 0.08301041\n",
      "Iteration 208, loss = 0.08181614\n",
      "Iteration 209, loss = 0.07990981\n",
      "Iteration 210, loss = 0.07943871\n",
      "Iteration 211, loss = 0.07939010\n",
      "Iteration 212, loss = 0.08014596\n",
      "Iteration 213, loss = 0.07835292\n",
      "Iteration 214, loss = 0.07865197\n",
      "Iteration 215, loss = 0.07797352\n",
      "Iteration 216, loss = 0.07670675\n",
      "Iteration 217, loss = 0.07598783\n",
      "Iteration 218, loss = 0.07442627\n",
      "Iteration 219, loss = 0.07373280\n",
      "Iteration 220, loss = 0.07406789\n",
      "Iteration 221, loss = 0.07265892\n",
      "Iteration 222, loss = 0.07351466\n",
      "Iteration 223, loss = 0.07196629\n",
      "Iteration 224, loss = 0.07247521\n",
      "Iteration 225, loss = 0.07187979\n",
      "Iteration 226, loss = 0.07015196\n",
      "Iteration 227, loss = 0.07051280\n",
      "Iteration 228, loss = 0.07021337\n",
      "Iteration 229, loss = 0.06972971\n",
      "Iteration 230, loss = 0.06745001\n",
      "Iteration 231, loss = 0.06673571\n",
      "Iteration 232, loss = 0.06638899\n",
      "Iteration 233, loss = 0.06622423\n",
      "Iteration 234, loss = 0.06599009\n",
      "Iteration 235, loss = 0.06536436\n",
      "Iteration 236, loss = 0.06593837\n",
      "Iteration 237, loss = 0.06492789\n",
      "Iteration 238, loss = 0.06368743\n",
      "Iteration 239, loss = 0.06301193\n",
      "Iteration 240, loss = 0.06212536\n",
      "Iteration 241, loss = 0.06205779\n",
      "Iteration 242, loss = 0.06216849\n",
      "Iteration 243, loss = 0.06149625\n",
      "Iteration 244, loss = 0.06038825\n",
      "Iteration 245, loss = 0.06133739\n",
      "Iteration 246, loss = 0.05951824\n",
      "Iteration 247, loss = 0.05932973\n",
      "Iteration 248, loss = 0.05866986\n",
      "Iteration 249, loss = 0.06105647\n",
      "Iteration 250, loss = 0.05952368\n",
      "Iteration 251, loss = 0.05918661\n",
      "Iteration 252, loss = 0.05737586\n",
      "Iteration 253, loss = 0.05811690\n",
      "Iteration 254, loss = 0.05718678\n",
      "Iteration 255, loss = 0.05543671\n",
      "Iteration 256, loss = 0.05558691\n",
      "Iteration 257, loss = 0.05553713\n",
      "Iteration 258, loss = 0.05605130\n",
      "Iteration 259, loss = 0.05541215\n",
      "Iteration 260, loss = 0.05594107\n",
      "Iteration 261, loss = 0.05420848\n",
      "Iteration 262, loss = 0.05244513\n",
      "Iteration 263, loss = 0.05306142\n",
      "Iteration 264, loss = 0.05276881\n",
      "Iteration 265, loss = 0.05155688\n",
      "Iteration 266, loss = 0.05130330\n",
      "Iteration 267, loss = 0.05099119\n",
      "Iteration 268, loss = 0.05090431\n",
      "Iteration 269, loss = 0.05200561\n",
      "Iteration 270, loss = 0.05284967\n",
      "Iteration 271, loss = 0.05065747\n",
      "Iteration 272, loss = 0.04964668\n",
      "Iteration 273, loss = 0.04997123\n",
      "Iteration 274, loss = 0.04942088\n",
      "Iteration 275, loss = 0.04865825\n",
      "Iteration 276, loss = 0.04846941\n",
      "Iteration 277, loss = 0.04912593\n",
      "Iteration 278, loss = 0.04842074\n",
      "Iteration 279, loss = 0.04817390\n",
      "Iteration 280, loss = 0.04706013\n",
      "Iteration 281, loss = 0.04736504\n",
      "Iteration 282, loss = 0.04762203\n",
      "Iteration 283, loss = 0.04639534\n",
      "Iteration 284, loss = 0.04589826\n",
      "Iteration 285, loss = 0.04538180\n",
      "Iteration 286, loss = 0.04660993\n",
      "Iteration 287, loss = 0.04578821\n",
      "Iteration 288, loss = 0.04546677\n",
      "Iteration 289, loss = 0.04523872\n",
      "Iteration 290, loss = 0.04472046\n",
      "Iteration 291, loss = 0.04432925\n",
      "Iteration 292, loss = 0.04516040\n",
      "Iteration 293, loss = 0.04419269\n",
      "Iteration 294, loss = 0.04379554\n",
      "Iteration 295, loss = 0.04355474\n",
      "Iteration 296, loss = 0.04323223\n",
      "Iteration 297, loss = 0.04311702\n",
      "Iteration 298, loss = 0.04295150\n",
      "Iteration 299, loss = 0.04215439\n",
      "Iteration 300, loss = 0.04233819\n",
      "Iteration 301, loss = 0.04218202\n",
      "Iteration 302, loss = 0.04306395\n",
      "Iteration 303, loss = 0.04207837\n",
      "Iteration 304, loss = 0.04157808\n",
      "Iteration 305, loss = 0.04110150\n",
      "Iteration 306, loss = 0.04077433\n",
      "Iteration 307, loss = 0.04110855\n",
      "Iteration 308, loss = 0.04088904\n",
      "Iteration 309, loss = 0.04183292\n",
      "Iteration 310, loss = 0.04074174\n",
      "Iteration 311, loss = 0.03983176\n",
      "Iteration 312, loss = 0.04016271\n",
      "Iteration 313, loss = 0.04001789\n",
      "Iteration 314, loss = 0.03943931\n",
      "Iteration 315, loss = 0.04024286\n",
      "Iteration 316, loss = 0.03890898\n",
      "Iteration 317, loss = 0.03925467\n",
      "Iteration 318, loss = 0.03913971\n",
      "Iteration 319, loss = 0.03853300\n",
      "Iteration 320, loss = 0.03849975\n",
      "Iteration 321, loss = 0.03977686\n",
      "Iteration 322, loss = 0.03952645\n",
      "Iteration 323, loss = 0.03811457\n",
      "Iteration 324, loss = 0.03841965\n",
      "Iteration 325, loss = 0.03816447\n",
      "Iteration 326, loss = 0.03823644\n",
      "Iteration 327, loss = 0.03736264\n",
      "Iteration 328, loss = 0.03698810\n",
      "Iteration 329, loss = 0.03728415\n",
      "Iteration 330, loss = 0.03768783\n",
      "Iteration 331, loss = 0.03688786\n",
      "Iteration 332, loss = 0.03723503\n",
      "Iteration 333, loss = 0.03711545\n",
      "Iteration 334, loss = 0.03662702\n",
      "Iteration 335, loss = 0.03705460\n",
      "Iteration 336, loss = 0.03596400\n",
      "Iteration 337, loss = 0.03718227\n",
      "Iteration 338, loss = 0.03609812\n",
      "Iteration 339, loss = 0.03573810\n",
      "Iteration 340, loss = 0.03619273\n",
      "Iteration 341, loss = 0.03586369\n",
      "Iteration 342, loss = 0.03572918\n",
      "Iteration 343, loss = 0.03501722\n",
      "Iteration 344, loss = 0.03521181\n",
      "Iteration 345, loss = 0.03549036\n",
      "Iteration 346, loss = 0.03571276\n",
      "Iteration 347, loss = 0.03520876\n",
      "Iteration 348, loss = 0.03558687\n",
      "Iteration 349, loss = 0.03555492\n",
      "Iteration 350, loss = 0.03521740\n",
      "Iteration 351, loss = 0.03552045\n",
      "Iteration 352, loss = 0.03525651\n",
      "Iteration 353, loss = 0.03468277\n",
      "Iteration 354, loss = 0.03438560\n",
      "Iteration 355, loss = 0.03422661\n",
      "Iteration 356, loss = 0.03417503\n",
      "Iteration 357, loss = 0.03483403\n",
      "Iteration 358, loss = 0.03490929\n",
      "Iteration 359, loss = 0.03443643\n",
      "Iteration 360, loss = 0.03416531\n",
      "Iteration 361, loss = 0.03347679\n",
      "Iteration 362, loss = 0.03371847\n",
      "Iteration 363, loss = 0.03375014\n",
      "Iteration 364, loss = 0.03360819\n",
      "Iteration 365, loss = 0.03366031\n",
      "Iteration 366, loss = 0.03310203\n",
      "Iteration 367, loss = 0.03316449\n",
      "Iteration 368, loss = 0.03276918\n",
      "Iteration 369, loss = 0.03309891\n",
      "Iteration 370, loss = 0.03387403\n",
      "Iteration 371, loss = 0.03357435\n",
      "Iteration 372, loss = 0.03356412\n",
      "Iteration 373, loss = 0.03342081\n",
      "Iteration 374, loss = 0.03313650\n",
      "Iteration 375, loss = 0.03288326\n",
      "Iteration 376, loss = 0.03259814\n",
      "Iteration 377, loss = 0.03254465\n",
      "Iteration 378, loss = 0.03225897\n",
      "Iteration 379, loss = 0.03192831\n",
      "Iteration 380, loss = 0.03205850\n",
      "Iteration 381, loss = 0.03293766\n",
      "Iteration 382, loss = 0.03309469\n",
      "Iteration 383, loss = 0.03284224\n",
      "Iteration 384, loss = 0.03210879\n",
      "Iteration 385, loss = 0.03209226\n",
      "Iteration 386, loss = 0.03270506\n",
      "Iteration 387, loss = 0.03180565\n",
      "Iteration 388, loss = 0.03180758\n",
      "Iteration 389, loss = 0.03209825\n",
      "Iteration 390, loss = 0.03204833\n",
      "Iteration 391, loss = 0.03148279\n",
      "Iteration 392, loss = 0.03156382\n",
      "Iteration 393, loss = 0.03150944\n",
      "Iteration 394, loss = 0.03122044\n",
      "Iteration 395, loss = 0.03141085\n",
      "Iteration 396, loss = 0.03109367\n",
      "Iteration 397, loss = 0.03170021\n",
      "Iteration 398, loss = 0.03094694\n",
      "Iteration 399, loss = 0.03087398\n",
      "Iteration 400, loss = 0.03112168\n",
      "Iteration 401, loss = 0.03090765\n",
      "Iteration 402, loss = 0.03122975\n",
      "Iteration 403, loss = 0.03108218\n",
      "Iteration 404, loss = 0.03036331\n",
      "Iteration 405, loss = 0.03040848\n",
      "Iteration 406, loss = 0.03047538\n",
      "Iteration 407, loss = 0.03096142\n",
      "Iteration 408, loss = 0.03096080\n",
      "Iteration 409, loss = 0.03054621\n",
      "Iteration 410, loss = 0.03067285\n",
      "Iteration 411, loss = 0.03140949\n",
      "Iteration 412, loss = 0.03120208\n",
      "Iteration 413, loss = 0.03044209\n",
      "Iteration 414, loss = 0.03012562\n",
      "Iteration 415, loss = 0.03023251\n",
      "Iteration 416, loss = 0.03048350\n",
      "Iteration 417, loss = 0.03017183\n",
      "Iteration 418, loss = 0.02993685\n",
      "Iteration 419, loss = 0.02974996\n",
      "Iteration 420, loss = 0.02976818\n",
      "Iteration 421, loss = 0.02976019\n",
      "Iteration 422, loss = 0.02995846\n",
      "Iteration 423, loss = 0.03011912\n",
      "Iteration 424, loss = 0.03014115\n",
      "Iteration 425, loss = 0.02924069\n",
      "Iteration 426, loss = 0.02961760\n",
      "Iteration 427, loss = 0.02962471\n",
      "Iteration 428, loss = 0.02936387\n",
      "Iteration 429, loss = 0.03014505\n",
      "Iteration 430, loss = 0.02917980\n",
      "Iteration 431, loss = 0.02949207\n",
      "Iteration 432, loss = 0.02932854\n",
      "Iteration 433, loss = 0.02959795\n",
      "Iteration 434, loss = 0.02944681\n",
      "Iteration 435, loss = 0.02926873\n",
      "Iteration 436, loss = 0.02962171\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV 2/3] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=600, solver=adam, verbose=True;, score=0.873 total time=  26.8s\n",
      "Iteration 1, loss = 0.38868133\n",
      "Iteration 2, loss = 0.28734395\n",
      "Iteration 3, loss = 0.28279575\n",
      "Iteration 4, loss = 0.28021315\n",
      "Iteration 5, loss = 0.27833045\n",
      "Iteration 6, loss = 0.27449906\n",
      "Iteration 7, loss = 0.27044483\n",
      "Iteration 8, loss = 0.26720472\n",
      "Iteration 9, loss = 0.26144808\n",
      "Iteration 10, loss = 0.25809050\n",
      "Iteration 11, loss = 0.25457111\n",
      "Iteration 12, loss = 0.25095192\n",
      "Iteration 13, loss = 0.24996934\n",
      "Iteration 14, loss = 0.24812288\n",
      "Iteration 15, loss = 0.24716415\n",
      "Iteration 16, loss = 0.24688894\n",
      "Iteration 17, loss = 0.24382002\n",
      "Iteration 18, loss = 0.24347166\n",
      "Iteration 19, loss = 0.24359247\n",
      "Iteration 20, loss = 0.23925459\n",
      "Iteration 21, loss = 0.23850502\n",
      "Iteration 22, loss = 0.23738215\n",
      "Iteration 23, loss = 0.23673231\n",
      "Iteration 24, loss = 0.23490003\n",
      "Iteration 25, loss = 0.23468245\n",
      "Iteration 26, loss = 0.23436661\n",
      "Iteration 27, loss = 0.23320162\n",
      "Iteration 28, loss = 0.23134235\n",
      "Iteration 29, loss = 0.23133440\n",
      "Iteration 30, loss = 0.22894136\n",
      "Iteration 31, loss = 0.22743500\n",
      "Iteration 32, loss = 0.22721976\n",
      "Iteration 33, loss = 0.22875633\n",
      "Iteration 34, loss = 0.22649639\n",
      "Iteration 35, loss = 0.22650113\n",
      "Iteration 36, loss = 0.22497810\n",
      "Iteration 37, loss = 0.22413098\n",
      "Iteration 38, loss = 0.22119482\n",
      "Iteration 39, loss = 0.22420558\n",
      "Iteration 40, loss = 0.21941349\n",
      "Iteration 41, loss = 0.22164920\n",
      "Iteration 42, loss = 0.21944020\n",
      "Iteration 43, loss = 0.21857018\n",
      "Iteration 44, loss = 0.21648814\n",
      "Iteration 45, loss = 0.21658533\n",
      "Iteration 46, loss = 0.21650737\n",
      "Iteration 47, loss = 0.21342762\n",
      "Iteration 48, loss = 0.21305425\n",
      "Iteration 49, loss = 0.21387548\n",
      "Iteration 50, loss = 0.21109874\n",
      "Iteration 51, loss = 0.21207756\n",
      "Iteration 52, loss = 0.21435408\n",
      "Iteration 53, loss = 0.20922316\n",
      "Iteration 54, loss = 0.21022483\n",
      "Iteration 55, loss = 0.20958205\n",
      "Iteration 56, loss = 0.20548526\n",
      "Iteration 57, loss = 0.20578422\n",
      "Iteration 58, loss = 0.20464425\n",
      "Iteration 59, loss = 0.20317008\n",
      "Iteration 60, loss = 0.20469889\n",
      "Iteration 61, loss = 0.20193521\n",
      "Iteration 62, loss = 0.20169109\n",
      "Iteration 63, loss = 0.20322991\n",
      "Iteration 64, loss = 0.20033739\n",
      "Iteration 65, loss = 0.20216819\n",
      "Iteration 66, loss = 0.19894812\n",
      "Iteration 67, loss = 0.20034101\n",
      "Iteration 68, loss = 0.19649041\n",
      "Iteration 69, loss = 0.19869091\n",
      "Iteration 70, loss = 0.19697380\n",
      "Iteration 71, loss = 0.19506978\n",
      "Iteration 72, loss = 0.19650795\n",
      "Iteration 73, loss = 0.19332628\n",
      "Iteration 74, loss = 0.19265986\n",
      "Iteration 75, loss = 0.19345666\n",
      "Iteration 76, loss = 0.19334496\n",
      "Iteration 77, loss = 0.19126232\n",
      "Iteration 78, loss = 0.19167531\n",
      "Iteration 79, loss = 0.19137878\n",
      "Iteration 80, loss = 0.19069241\n",
      "Iteration 81, loss = 0.18676060\n",
      "Iteration 82, loss = 0.18657778\n",
      "Iteration 83, loss = 0.18764007\n",
      "Iteration 84, loss = 0.18539349\n",
      "Iteration 85, loss = 0.18549275\n",
      "Iteration 86, loss = 0.18352308\n",
      "Iteration 87, loss = 0.18279407\n",
      "Iteration 88, loss = 0.18085004\n",
      "Iteration 89, loss = 0.18202629\n",
      "Iteration 90, loss = 0.18187419\n",
      "Iteration 91, loss = 0.17863948\n",
      "Iteration 92, loss = 0.17818951\n",
      "Iteration 93, loss = 0.17978942\n",
      "Iteration 94, loss = 0.17546671\n",
      "Iteration 95, loss = 0.17692803\n",
      "Iteration 96, loss = 0.17413113\n",
      "Iteration 97, loss = 0.17288007\n",
      "Iteration 98, loss = 0.17444679\n",
      "Iteration 99, loss = 0.17171899\n",
      "Iteration 100, loss = 0.17010158\n",
      "Iteration 101, loss = 0.16944192\n",
      "Iteration 102, loss = 0.17027771\n",
      "Iteration 103, loss = 0.16735950\n",
      "Iteration 104, loss = 0.16845682\n",
      "Iteration 105, loss = 0.16674555\n",
      "Iteration 106, loss = 0.16746482\n",
      "Iteration 107, loss = 0.16644653\n",
      "Iteration 108, loss = 0.16479363\n",
      "Iteration 109, loss = 0.16286714\n",
      "Iteration 110, loss = 0.16312921\n",
      "Iteration 111, loss = 0.16063088\n",
      "Iteration 112, loss = 0.16017213\n",
      "Iteration 113, loss = 0.16117696\n",
      "Iteration 114, loss = 0.15845749\n",
      "Iteration 115, loss = 0.15877768\n",
      "Iteration 116, loss = 0.15989280\n",
      "Iteration 117, loss = 0.15552182\n",
      "Iteration 118, loss = 0.15453790\n",
      "Iteration 119, loss = 0.15608265\n",
      "Iteration 120, loss = 0.15347799\n",
      "Iteration 121, loss = 0.15146071\n",
      "Iteration 122, loss = 0.15131876\n",
      "Iteration 123, loss = 0.15060413\n",
      "Iteration 124, loss = 0.14977017\n",
      "Iteration 125, loss = 0.15036056\n",
      "Iteration 126, loss = 0.14710268\n",
      "Iteration 127, loss = 0.14725338\n",
      "Iteration 128, loss = 0.14574370\n",
      "Iteration 129, loss = 0.14452750\n",
      "Iteration 130, loss = 0.14294226\n",
      "Iteration 131, loss = 0.14482219\n",
      "Iteration 132, loss = 0.14519839\n",
      "Iteration 133, loss = 0.14138643\n",
      "Iteration 134, loss = 0.13870234\n",
      "Iteration 135, loss = 0.13961576\n",
      "Iteration 136, loss = 0.13659566\n",
      "Iteration 137, loss = 0.13755497\n",
      "Iteration 138, loss = 0.13726577\n",
      "Iteration 139, loss = 0.13788649\n",
      "Iteration 140, loss = 0.13853094\n",
      "Iteration 141, loss = 0.13409347\n",
      "Iteration 142, loss = 0.13516560\n",
      "Iteration 143, loss = 0.13167170\n",
      "Iteration 144, loss = 0.13047222\n",
      "Iteration 145, loss = 0.13212176\n",
      "Iteration 146, loss = 0.12947402\n",
      "Iteration 147, loss = 0.12898270\n",
      "Iteration 148, loss = 0.12632788\n",
      "Iteration 149, loss = 0.12701318\n",
      "Iteration 150, loss = 0.12516601\n",
      "Iteration 151, loss = 0.12525311\n",
      "Iteration 152, loss = 0.12439003\n",
      "Iteration 153, loss = 0.12307165\n",
      "Iteration 154, loss = 0.12196135\n",
      "Iteration 155, loss = 0.12166801\n",
      "Iteration 156, loss = 0.11986721\n",
      "Iteration 157, loss = 0.12101533\n",
      "Iteration 158, loss = 0.11915585\n",
      "Iteration 159, loss = 0.12042105\n",
      "Iteration 160, loss = 0.11664506\n",
      "Iteration 161, loss = 0.11502401\n",
      "Iteration 162, loss = 0.11518325\n",
      "Iteration 163, loss = 0.11488113\n",
      "Iteration 164, loss = 0.11512848\n",
      "Iteration 165, loss = 0.11462695\n",
      "Iteration 166, loss = 0.11381961\n",
      "Iteration 167, loss = 0.11275820\n",
      "Iteration 168, loss = 0.10952841\n",
      "Iteration 169, loss = 0.11111250\n",
      "Iteration 170, loss = 0.11038922\n",
      "Iteration 171, loss = 0.10827270\n",
      "Iteration 172, loss = 0.10929632\n",
      "Iteration 173, loss = 0.10701368\n",
      "Iteration 174, loss = 0.10817390\n",
      "Iteration 175, loss = 0.10466575\n",
      "Iteration 176, loss = 0.10469830\n",
      "Iteration 177, loss = 0.10207554\n",
      "Iteration 178, loss = 0.10336415\n",
      "Iteration 179, loss = 0.10135602\n",
      "Iteration 180, loss = 0.10152389\n",
      "Iteration 181, loss = 0.10090310\n",
      "Iteration 182, loss = 0.10076488\n",
      "Iteration 183, loss = 0.09849833\n",
      "Iteration 184, loss = 0.09854715\n",
      "Iteration 185, loss = 0.09920703\n",
      "Iteration 186, loss = 0.09649400\n",
      "Iteration 187, loss = 0.09562198\n",
      "Iteration 188, loss = 0.09608490\n",
      "Iteration 189, loss = 0.09475639\n",
      "Iteration 190, loss = 0.09320854\n",
      "Iteration 191, loss = 0.09345090\n",
      "Iteration 192, loss = 0.09239813\n",
      "Iteration 193, loss = 0.09084989\n",
      "Iteration 194, loss = 0.09111665\n",
      "Iteration 195, loss = 0.09166802\n",
      "Iteration 196, loss = 0.08856288\n",
      "Iteration 197, loss = 0.09188201\n",
      "Iteration 198, loss = 0.09205454\n",
      "Iteration 199, loss = 0.08765048\n",
      "Iteration 200, loss = 0.08657502\n",
      "Iteration 201, loss = 0.08698487\n",
      "Iteration 202, loss = 0.08613626\n",
      "Iteration 203, loss = 0.08554260\n",
      "Iteration 204, loss = 0.08385549\n",
      "Iteration 205, loss = 0.08313167\n",
      "Iteration 206, loss = 0.08299313\n",
      "Iteration 207, loss = 0.08257771\n",
      "Iteration 208, loss = 0.08305399\n",
      "Iteration 209, loss = 0.08213168\n",
      "Iteration 210, loss = 0.08008930\n",
      "Iteration 211, loss = 0.08041695\n",
      "Iteration 212, loss = 0.07996713\n",
      "Iteration 213, loss = 0.07877261\n",
      "Iteration 214, loss = 0.07953575\n",
      "Iteration 215, loss = 0.07921579\n",
      "Iteration 216, loss = 0.07805430\n",
      "Iteration 217, loss = 0.07601099\n",
      "Iteration 218, loss = 0.07675654\n",
      "Iteration 219, loss = 0.07469755\n",
      "Iteration 220, loss = 0.07487527\n",
      "Iteration 221, loss = 0.07440231\n",
      "Iteration 222, loss = 0.07459298\n",
      "Iteration 223, loss = 0.07265987\n",
      "Iteration 224, loss = 0.07199523\n",
      "Iteration 225, loss = 0.07240727\n",
      "Iteration 226, loss = 0.07115750\n",
      "Iteration 227, loss = 0.07154014\n",
      "Iteration 228, loss = 0.07044772\n",
      "Iteration 229, loss = 0.06900554\n",
      "Iteration 230, loss = 0.07015952\n",
      "Iteration 231, loss = 0.06799931\n",
      "Iteration 232, loss = 0.06906466\n",
      "Iteration 233, loss = 0.06808254\n",
      "Iteration 234, loss = 0.06775203\n",
      "Iteration 235, loss = 0.06806682\n",
      "Iteration 236, loss = 0.06723576\n",
      "Iteration 237, loss = 0.06661042\n",
      "Iteration 238, loss = 0.06603816\n",
      "Iteration 239, loss = 0.06562967\n",
      "Iteration 240, loss = 0.06495128\n",
      "Iteration 241, loss = 0.06420889\n",
      "Iteration 242, loss = 0.06419785\n",
      "Iteration 243, loss = 0.06321501\n",
      "Iteration 244, loss = 0.06192255\n",
      "Iteration 245, loss = 0.06193474\n",
      "Iteration 246, loss = 0.06329414\n",
      "Iteration 247, loss = 0.06134170\n",
      "Iteration 248, loss = 0.06237748\n",
      "Iteration 249, loss = 0.06142805\n",
      "Iteration 250, loss = 0.06089661\n",
      "Iteration 251, loss = 0.05974103\n",
      "Iteration 252, loss = 0.05956440\n",
      "Iteration 253, loss = 0.06026390\n",
      "Iteration 254, loss = 0.05973030\n",
      "Iteration 255, loss = 0.05839779\n",
      "Iteration 256, loss = 0.06089907\n",
      "Iteration 257, loss = 0.05806579\n",
      "Iteration 258, loss = 0.05767826\n",
      "Iteration 259, loss = 0.05798219\n",
      "Iteration 260, loss = 0.05766029\n",
      "Iteration 261, loss = 0.05691734\n",
      "Iteration 262, loss = 0.05641778\n",
      "Iteration 263, loss = 0.05622119\n",
      "Iteration 264, loss = 0.05490686\n",
      "Iteration 265, loss = 0.05584149\n",
      "Iteration 266, loss = 0.05441362\n",
      "Iteration 267, loss = 0.05428300\n",
      "Iteration 268, loss = 0.05417750\n",
      "Iteration 269, loss = 0.05446106\n",
      "Iteration 270, loss = 0.05335134\n",
      "Iteration 271, loss = 0.05451310\n",
      "Iteration 272, loss = 0.05356685\n",
      "Iteration 273, loss = 0.05342991\n",
      "Iteration 274, loss = 0.05353395\n",
      "Iteration 275, loss = 0.05285215\n",
      "Iteration 276, loss = 0.05215809\n",
      "Iteration 277, loss = 0.05151384\n",
      "Iteration 278, loss = 0.04968691\n",
      "Iteration 279, loss = 0.05017115\n",
      "Iteration 280, loss = 0.05004169\n",
      "Iteration 281, loss = 0.05044039\n",
      "Iteration 282, loss = 0.04962367\n",
      "Iteration 283, loss = 0.04925728\n",
      "Iteration 284, loss = 0.04966074\n",
      "Iteration 285, loss = 0.05022903\n",
      "Iteration 286, loss = 0.04824757\n",
      "Iteration 287, loss = 0.04821664\n",
      "Iteration 288, loss = 0.04867770\n",
      "Iteration 289, loss = 0.04779478\n",
      "Iteration 290, loss = 0.04839391\n",
      "Iteration 291, loss = 0.04724639\n",
      "Iteration 292, loss = 0.04774411\n",
      "Iteration 293, loss = 0.04750433\n",
      "Iteration 294, loss = 0.04641338\n",
      "Iteration 295, loss = 0.04621557\n",
      "Iteration 296, loss = 0.04636175\n",
      "Iteration 297, loss = 0.04598840\n",
      "Iteration 298, loss = 0.04724152\n",
      "Iteration 299, loss = 0.04581635\n",
      "Iteration 300, loss = 0.04698906\n",
      "Iteration 301, loss = 0.04664629\n",
      "Iteration 302, loss = 0.04435636\n",
      "Iteration 303, loss = 0.04446683\n",
      "Iteration 304, loss = 0.04438650\n",
      "Iteration 305, loss = 0.04558596\n",
      "Iteration 306, loss = 0.04556435\n",
      "Iteration 307, loss = 0.04419931\n",
      "Iteration 308, loss = 0.04453900\n",
      "Iteration 309, loss = 0.04299494\n",
      "Iteration 310, loss = 0.04332841\n",
      "Iteration 311, loss = 0.04318759\n",
      "Iteration 312, loss = 0.04281068\n",
      "Iteration 313, loss = 0.04356087\n",
      "Iteration 314, loss = 0.04287675\n",
      "Iteration 315, loss = 0.04422808\n",
      "Iteration 316, loss = 0.04309755\n",
      "Iteration 317, loss = 0.04352099\n",
      "Iteration 318, loss = 0.04180039\n",
      "Iteration 319, loss = 0.04170651\n",
      "Iteration 320, loss = 0.04256366\n",
      "Iteration 321, loss = 0.04160997\n",
      "Iteration 322, loss = 0.04177089\n",
      "Iteration 323, loss = 0.04184096\n",
      "Iteration 324, loss = 0.04121437\n",
      "Iteration 325, loss = 0.04092628\n",
      "Iteration 326, loss = 0.04080520\n",
      "Iteration 327, loss = 0.04001855\n",
      "Iteration 328, loss = 0.04006524\n",
      "Iteration 329, loss = 0.03976693\n",
      "Iteration 330, loss = 0.04087631\n",
      "Iteration 331, loss = 0.04072488\n",
      "Iteration 332, loss = 0.03950059\n",
      "Iteration 333, loss = 0.03977571\n",
      "Iteration 334, loss = 0.04012428\n",
      "Iteration 335, loss = 0.04006341\n",
      "Iteration 336, loss = 0.04268600\n",
      "Iteration 337, loss = 0.03957300\n",
      "Iteration 338, loss = 0.03941044\n",
      "Iteration 339, loss = 0.03864614\n",
      "Iteration 340, loss = 0.03905261\n",
      "Iteration 341, loss = 0.03809957\n",
      "Iteration 342, loss = 0.03952033\n",
      "Iteration 343, loss = 0.03947286\n",
      "Iteration 344, loss = 0.03778972\n",
      "Iteration 345, loss = 0.03814272\n",
      "Iteration 346, loss = 0.03830443\n",
      "Iteration 347, loss = 0.03754505\n",
      "Iteration 348, loss = 0.03904958\n",
      "Iteration 349, loss = 0.03749403\n",
      "Iteration 350, loss = 0.03800806\n",
      "Iteration 351, loss = 0.03739022\n",
      "Iteration 352, loss = 0.03721039\n",
      "Iteration 353, loss = 0.03761270\n",
      "Iteration 354, loss = 0.03707484\n",
      "Iteration 355, loss = 0.03689488\n",
      "Iteration 356, loss = 0.03706002\n",
      "Iteration 357, loss = 0.03658940\n",
      "Iteration 358, loss = 0.03773976\n",
      "Iteration 359, loss = 0.03639356\n",
      "Iteration 360, loss = 0.03673528\n",
      "Iteration 361, loss = 0.03616308\n",
      "Iteration 362, loss = 0.03659853\n",
      "Iteration 363, loss = 0.03694943\n",
      "Iteration 364, loss = 0.03561719\n",
      "Iteration 365, loss = 0.03519875\n",
      "Iteration 366, loss = 0.03646085\n",
      "Iteration 367, loss = 0.03655641\n",
      "Iteration 368, loss = 0.03556956\n",
      "Iteration 369, loss = 0.03563483\n",
      "Iteration 370, loss = 0.03574601\n",
      "Iteration 371, loss = 0.03540386\n",
      "Iteration 372, loss = 0.03673608\n",
      "Iteration 373, loss = 0.03510790\n",
      "Iteration 374, loss = 0.03521310\n",
      "Iteration 375, loss = 0.03577899\n",
      "Iteration 376, loss = 0.03487154\n",
      "Iteration 377, loss = 0.03448137\n",
      "Iteration 378, loss = 0.03469654\n",
      "Iteration 379, loss = 0.03452710\n",
      "Iteration 380, loss = 0.03461489\n",
      "Iteration 381, loss = 0.03528226\n",
      "Iteration 382, loss = 0.03504658\n",
      "Iteration 383, loss = 0.03483847\n",
      "Iteration 384, loss = 0.03517243\n",
      "Iteration 385, loss = 0.03590490\n",
      "Iteration 386, loss = 0.03563571\n",
      "Iteration 387, loss = 0.03397591\n",
      "Iteration 388, loss = 0.03374490\n",
      "Iteration 389, loss = 0.03426518\n",
      "Iteration 390, loss = 0.03428286\n",
      "Iteration 391, loss = 0.03553450\n",
      "Iteration 392, loss = 0.03330490\n",
      "Iteration 393, loss = 0.03301390\n",
      "Iteration 394, loss = 0.03340320\n",
      "Iteration 395, loss = 0.03443634\n",
      "Iteration 396, loss = 0.03360103\n",
      "Iteration 397, loss = 0.03326371\n",
      "Iteration 398, loss = 0.03324427\n",
      "Iteration 399, loss = 0.03387232\n",
      "Iteration 400, loss = 0.03264735\n",
      "Iteration 401, loss = 0.03467251\n",
      "Iteration 402, loss = 0.03393804\n",
      "Iteration 403, loss = 0.03356051\n",
      "Iteration 404, loss = 0.03391944\n",
      "Iteration 405, loss = 0.03247906\n",
      "Iteration 406, loss = 0.03384499\n",
      "Iteration 407, loss = 0.03312112\n",
      "Iteration 408, loss = 0.03275527\n",
      "Iteration 409, loss = 0.03309046\n",
      "Iteration 410, loss = 0.03315056\n",
      "Iteration 411, loss = 0.03390158\n",
      "Iteration 412, loss = 0.03299627\n",
      "Iteration 413, loss = 0.03287404\n",
      "Iteration 414, loss = 0.03215988\n",
      "Iteration 415, loss = 0.03399996\n",
      "Iteration 416, loss = 0.03303147\n",
      "Iteration 417, loss = 0.03195367\n",
      "Iteration 418, loss = 0.03256117\n",
      "Iteration 419, loss = 0.03242633\n",
      "Iteration 420, loss = 0.03215972\n",
      "Iteration 421, loss = 0.03174451\n",
      "Iteration 422, loss = 0.03215871\n",
      "Iteration 423, loss = 0.03182194\n",
      "Iteration 424, loss = 0.03188458\n",
      "Iteration 425, loss = 0.03196786\n",
      "Iteration 426, loss = 0.03186166\n",
      "Iteration 427, loss = 0.03255565\n",
      "Iteration 428, loss = 0.03148454\n",
      "Iteration 429, loss = 0.03147560\n",
      "Iteration 430, loss = 0.03153450\n",
      "Iteration 431, loss = 0.03129122\n",
      "Iteration 432, loss = 0.03102550\n",
      "Iteration 433, loss = 0.03253103\n",
      "Iteration 434, loss = 0.03129614\n",
      "Iteration 435, loss = 0.03137888\n",
      "Iteration 436, loss = 0.03094315\n",
      "Iteration 437, loss = 0.03102803\n",
      "Iteration 438, loss = 0.03087799\n",
      "Iteration 439, loss = 0.03153956\n",
      "Iteration 440, loss = 0.03141391\n",
      "Iteration 441, loss = 0.03052274\n",
      "Iteration 442, loss = 0.03123865\n",
      "Iteration 443, loss = 0.03087903\n",
      "Iteration 444, loss = 0.03124182\n",
      "Iteration 445, loss = 0.03066054\n",
      "Iteration 446, loss = 0.03100460\n",
      "Iteration 447, loss = 0.03081483\n",
      "Iteration 448, loss = 0.03089618\n",
      "Iteration 449, loss = 0.03060934\n",
      "Iteration 450, loss = 0.03085328\n",
      "Iteration 451, loss = 0.03179753\n",
      "Iteration 452, loss = 0.03152993\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV 3/3] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 150), learning_rate_init=0.001, max_iter=600, solver=adam, verbose=True;, score=0.877 total time=  27.8s\n",
      "Iteration 1, loss = 0.39524337\n",
      "Iteration 2, loss = 0.29325176\n",
      "Iteration 3, loss = 0.28574146\n",
      "Iteration 4, loss = 0.28243810\n",
      "Iteration 5, loss = 0.27968703\n",
      "Iteration 6, loss = 0.27660912\n",
      "Iteration 7, loss = 0.27493094\n",
      "Iteration 8, loss = 0.27072087\n",
      "Iteration 9, loss = 0.26726069\n",
      "Iteration 10, loss = 0.26439612\n",
      "Iteration 11, loss = 0.26182428\n",
      "Iteration 12, loss = 0.25922836\n",
      "Iteration 13, loss = 0.25546281\n",
      "Iteration 14, loss = 0.25383897\n",
      "Iteration 15, loss = 0.25193521\n",
      "Iteration 16, loss = 0.25032827\n",
      "Iteration 17, loss = 0.24849826\n",
      "Iteration 18, loss = 0.24798807\n",
      "Iteration 19, loss = 0.24612352\n",
      "Iteration 20, loss = 0.24563087\n",
      "Iteration 21, loss = 0.24415441\n",
      "Iteration 22, loss = 0.24317598\n",
      "Iteration 23, loss = 0.24297739\n",
      "Iteration 24, loss = 0.24146763\n",
      "Iteration 25, loss = 0.24055548\n",
      "Iteration 26, loss = 0.24026734\n",
      "Iteration 27, loss = 0.23928250\n",
      "Iteration 28, loss = 0.24007304\n",
      "Iteration 29, loss = 0.23863307\n",
      "Iteration 30, loss = 0.23736031\n",
      "Iteration 31, loss = 0.23767502\n",
      "Iteration 32, loss = 0.23572271\n",
      "Iteration 33, loss = 0.23576057\n",
      "Iteration 34, loss = 0.23529965\n",
      "Iteration 35, loss = 0.23497571\n",
      "Iteration 36, loss = 0.23466946\n",
      "Iteration 37, loss = 0.23222401\n",
      "Iteration 38, loss = 0.23177350\n",
      "Iteration 39, loss = 0.23095032\n",
      "Iteration 40, loss = 0.23185526\n",
      "Iteration 41, loss = 0.23122639\n",
      "Iteration 42, loss = 0.22859318\n",
      "Iteration 43, loss = 0.22873434\n",
      "Iteration 44, loss = 0.22849643\n",
      "Iteration 45, loss = 0.22694992\n",
      "Iteration 46, loss = 0.22752786\n",
      "Iteration 47, loss = 0.22699070\n",
      "Iteration 48, loss = 0.22578050\n",
      "Iteration 49, loss = 0.22683997\n",
      "Iteration 50, loss = 0.22554996\n",
      "Iteration 51, loss = 0.22396092\n",
      "Iteration 52, loss = 0.22316369\n",
      "Iteration 53, loss = 0.22408880\n",
      "Iteration 54, loss = 0.22208955\n",
      "Iteration 55, loss = 0.22073873\n",
      "Iteration 56, loss = 0.22134948\n",
      "Iteration 57, loss = 0.22004237\n",
      "Iteration 58, loss = 0.22030592\n",
      "Iteration 59, loss = 0.21948775\n",
      "Iteration 60, loss = 0.21774295\n",
      "Iteration 61, loss = 0.21810846\n",
      "Iteration 62, loss = 0.21727398\n",
      "Iteration 63, loss = 0.21565593\n",
      "Iteration 64, loss = 0.21605990\n",
      "Iteration 65, loss = 0.21587495\n",
      "Iteration 66, loss = 0.21625166\n",
      "Iteration 67, loss = 0.21518856\n",
      "Iteration 68, loss = 0.21377625\n",
      "Iteration 69, loss = 0.21341105\n",
      "Iteration 70, loss = 0.21279148\n",
      "Iteration 71, loss = 0.21201744\n",
      "Iteration 72, loss = 0.21325893\n",
      "Iteration 73, loss = 0.21139728\n",
      "Iteration 74, loss = 0.21134926\n",
      "Iteration 75, loss = 0.20924073\n",
      "Iteration 76, loss = 0.20978910\n",
      "Iteration 77, loss = 0.21016973\n",
      "Iteration 78, loss = 0.20822005\n",
      "Iteration 79, loss = 0.20804019\n",
      "Iteration 80, loss = 0.20777635\n",
      "Iteration 81, loss = 0.20781459\n",
      "Iteration 82, loss = 0.20755842\n",
      "Iteration 83, loss = 0.20550280\n",
      "Iteration 84, loss = 0.20595300\n",
      "Iteration 85, loss = 0.20447406\n",
      "Iteration 86, loss = 0.20433177\n",
      "Iteration 87, loss = 0.20404449\n",
      "Iteration 88, loss = 0.20473809\n",
      "Iteration 89, loss = 0.20380075\n",
      "Iteration 90, loss = 0.20207944\n",
      "Iteration 91, loss = 0.20148605\n",
      "Iteration 92, loss = 0.20072581\n",
      "Iteration 93, loss = 0.20179830\n",
      "Iteration 94, loss = 0.19929400\n",
      "Iteration 95, loss = 0.20179450\n",
      "Iteration 96, loss = 0.20091043\n",
      "Iteration 97, loss = 0.19844740\n",
      "Iteration 98, loss = 0.19950619\n",
      "Iteration 99, loss = 0.19877052\n",
      "Iteration 100, loss = 0.19723398\n",
      "Iteration 101, loss = 0.19817457\n",
      "Iteration 102, loss = 0.19669159\n",
      "Iteration 103, loss = 0.19639821\n",
      "Iteration 104, loss = 0.19548732\n",
      "Iteration 105, loss = 0.19627899\n",
      "Iteration 106, loss = 0.19558337\n",
      "Iteration 107, loss = 0.19273971\n",
      "Iteration 108, loss = 0.19320553\n",
      "Iteration 109, loss = 0.19124717\n",
      "Iteration 110, loss = 0.19426775\n",
      "Iteration 111, loss = 0.19245199\n",
      "Iteration 112, loss = 0.19297138\n",
      "Iteration 113, loss = 0.19251976\n",
      "Iteration 114, loss = 0.19100345\n",
      "Iteration 115, loss = 0.19065661\n",
      "Iteration 116, loss = 0.19001596\n",
      "Iteration 117, loss = 0.19038705\n",
      "Iteration 118, loss = 0.18894641\n",
      "Iteration 119, loss = 0.18780708\n",
      "Iteration 120, loss = 0.18715676\n",
      "Iteration 121, loss = 0.18949669\n",
      "Iteration 122, loss = 0.18719547\n",
      "Iteration 123, loss = 0.18510162\n",
      "Iteration 124, loss = 0.18474482\n",
      "Iteration 125, loss = 0.18568972\n",
      "Iteration 126, loss = 0.18469531\n",
      "Iteration 127, loss = 0.18420124\n",
      "Iteration 128, loss = 0.18494455\n",
      "Iteration 129, loss = 0.18367440\n",
      "Iteration 130, loss = 0.18235490\n",
      "Iteration 131, loss = 0.18304314\n",
      "Iteration 132, loss = 0.18116697\n",
      "Iteration 133, loss = 0.18124567\n",
      "Iteration 134, loss = 0.17979721\n",
      "Iteration 135, loss = 0.17936672\n",
      "Iteration 136, loss = 0.18066745\n",
      "Iteration 137, loss = 0.18029987\n",
      "Iteration 138, loss = 0.18104862\n",
      "Iteration 139, loss = 0.17951001\n",
      "Iteration 140, loss = 0.17969889\n",
      "Iteration 141, loss = 0.17718668\n",
      "Iteration 142, loss = 0.17805532\n",
      "Iteration 143, loss = 0.17681222\n",
      "Iteration 144, loss = 0.17604857\n",
      "Iteration 145, loss = 0.17771335\n",
      "Iteration 146, loss = 0.17692359\n",
      "Iteration 147, loss = 0.17448048\n",
      "Iteration 148, loss = 0.17475825\n",
      "Iteration 149, loss = 0.17612939\n",
      "Iteration 150, loss = 0.17480430\n",
      "Iteration 151, loss = 0.17367114\n",
      "Iteration 152, loss = 0.17199406\n",
      "Iteration 153, loss = 0.17267843\n",
      "Iteration 154, loss = 0.17184926\n",
      "Iteration 155, loss = 0.17106749\n",
      "Iteration 156, loss = 0.17018687\n",
      "Iteration 157, loss = 0.17310366\n",
      "Iteration 158, loss = 0.17005332\n",
      "Iteration 159, loss = 0.16825554\n",
      "Iteration 160, loss = 0.16900462\n",
      "Iteration 161, loss = 0.16892990\n",
      "Iteration 162, loss = 0.16894407\n",
      "Iteration 163, loss = 0.16704233\n",
      "Iteration 164, loss = 0.16715874\n",
      "Iteration 165, loss = 0.16709719\n",
      "Iteration 166, loss = 0.16524748\n",
      "Iteration 167, loss = 0.16661249\n",
      "Iteration 168, loss = 0.16409759\n",
      "Iteration 169, loss = 0.16395636\n",
      "Iteration 170, loss = 0.16322774\n",
      "Iteration 171, loss = 0.16442244\n",
      "Iteration 172, loss = 0.16167158\n",
      "Iteration 173, loss = 0.16309167\n",
      "Iteration 174, loss = 0.16270076\n",
      "Iteration 175, loss = 0.16090814\n",
      "Iteration 176, loss = 0.15920013\n",
      "Iteration 177, loss = 0.16011898\n",
      "Iteration 178, loss = 0.16151369\n",
      "Iteration 179, loss = 0.16036703\n",
      "Iteration 180, loss = 0.15899854\n",
      "Iteration 181, loss = 0.15839967\n",
      "Iteration 182, loss = 0.15783511\n",
      "Iteration 183, loss = 0.15728293\n",
      "Iteration 184, loss = 0.15566844\n",
      "Iteration 185, loss = 0.15809855\n",
      "Iteration 186, loss = 0.15654959\n",
      "Iteration 187, loss = 0.15590957\n",
      "Iteration 188, loss = 0.15541969\n",
      "Iteration 189, loss = 0.15443635\n",
      "Iteration 190, loss = 0.15349329\n",
      "Iteration 191, loss = 0.15360186\n",
      "Iteration 192, loss = 0.15509102\n",
      "Iteration 193, loss = 0.15188708\n",
      "Iteration 194, loss = 0.15362506\n",
      "Iteration 195, loss = 0.15231354\n",
      "Iteration 196, loss = 0.15111242\n",
      "Iteration 197, loss = 0.15126682\n",
      "Iteration 198, loss = 0.15164865\n",
      "Iteration 199, loss = 0.14904739\n",
      "Iteration 200, loss = 0.15113098\n",
      "Iteration 201, loss = 0.14828133\n",
      "Iteration 202, loss = 0.14728756\n",
      "Iteration 203, loss = 0.14776846\n",
      "Iteration 204, loss = 0.14681119\n",
      "Iteration 205, loss = 0.14648128\n",
      "Iteration 206, loss = 0.14604978\n",
      "Iteration 207, loss = 0.14470187\n",
      "Iteration 208, loss = 0.14400415\n",
      "Iteration 209, loss = 0.14486951\n",
      "Iteration 210, loss = 0.14409091\n",
      "Iteration 211, loss = 0.14428750\n",
      "Iteration 212, loss = 0.14437912\n",
      "Iteration 213, loss = 0.14284239\n",
      "Iteration 214, loss = 0.14389916\n",
      "Iteration 215, loss = 0.14104493\n",
      "Iteration 216, loss = 0.14078184\n",
      "Iteration 217, loss = 0.14067822\n",
      "Iteration 218, loss = 0.14065845\n",
      "Iteration 219, loss = 0.14085540\n",
      "Iteration 220, loss = 0.13854930\n",
      "Iteration 221, loss = 0.14025274\n",
      "Iteration 222, loss = 0.13900547\n",
      "Iteration 223, loss = 0.13783583\n",
      "Iteration 224, loss = 0.13809309\n",
      "Iteration 225, loss = 0.13714270\n",
      "Iteration 226, loss = 0.13968296\n",
      "Iteration 227, loss = 0.13755953\n",
      "Iteration 228, loss = 0.13483178\n",
      "Iteration 229, loss = 0.13527354\n",
      "Iteration 230, loss = 0.13498225\n",
      "Iteration 231, loss = 0.13299550\n",
      "Iteration 232, loss = 0.13292126\n",
      "Iteration 233, loss = 0.13220180\n",
      "Iteration 234, loss = 0.13316206\n",
      "Iteration 235, loss = 0.13130518\n",
      "Iteration 236, loss = 0.13225432\n",
      "Iteration 237, loss = 0.13049529\n",
      "Iteration 238, loss = 0.13263651\n",
      "Iteration 239, loss = 0.13028667\n",
      "Iteration 240, loss = 0.12951192\n",
      "Iteration 241, loss = 0.12911110\n",
      "Iteration 242, loss = 0.12994383\n",
      "Iteration 243, loss = 0.12848038\n",
      "Iteration 244, loss = 0.12697410\n",
      "Iteration 245, loss = 0.12679817\n",
      "Iteration 246, loss = 0.12582855\n",
      "Iteration 247, loss = 0.12522721\n",
      "Iteration 248, loss = 0.12613976\n",
      "Iteration 249, loss = 0.12617831\n",
      "Iteration 250, loss = 0.12870681\n",
      "Iteration 251, loss = 0.12590435\n",
      "Iteration 252, loss = 0.12546318\n",
      "Iteration 253, loss = 0.12335929\n",
      "Iteration 254, loss = 0.12454039\n",
      "Iteration 255, loss = 0.12311001\n",
      "Iteration 256, loss = 0.12190840\n",
      "Iteration 257, loss = 0.12200249\n",
      "Iteration 258, loss = 0.12194166\n",
      "Iteration 259, loss = 0.11950214\n",
      "Iteration 260, loss = 0.12007040\n",
      "Iteration 261, loss = 0.11907902\n",
      "Iteration 262, loss = 0.11964101\n",
      "Iteration 263, loss = 0.11962874\n",
      "Iteration 264, loss = 0.11898829\n",
      "Iteration 265, loss = 0.11807356\n",
      "Iteration 266, loss = 0.11834114\n",
      "Iteration 267, loss = 0.11645003\n",
      "Iteration 268, loss = 0.11827446\n",
      "Iteration 269, loss = 0.11670910\n",
      "Iteration 270, loss = 0.11554424\n",
      "Iteration 271, loss = 0.11442287\n",
      "Iteration 272, loss = 0.11461296\n",
      "Iteration 273, loss = 0.11311060\n",
      "Iteration 274, loss = 0.11506697\n",
      "Iteration 275, loss = 0.11308462\n",
      "Iteration 276, loss = 0.11239564\n",
      "Iteration 277, loss = 0.11234344\n",
      "Iteration 278, loss = 0.11229487\n",
      "Iteration 279, loss = 0.11084687\n",
      "Iteration 280, loss = 0.11164311\n",
      "Iteration 281, loss = 0.10995958\n",
      "Iteration 282, loss = 0.11030453\n",
      "Iteration 283, loss = 0.11079109\n",
      "Iteration 284, loss = 0.10941614\n",
      "Iteration 285, loss = 0.10881294\n",
      "Iteration 286, loss = 0.10835407\n",
      "Iteration 287, loss = 0.10825172\n",
      "Iteration 288, loss = 0.10648808\n",
      "Iteration 289, loss = 0.10779558\n",
      "Iteration 290, loss = 0.10658955\n",
      "Iteration 291, loss = 0.10647283\n",
      "Iteration 292, loss = 0.10594221\n",
      "Iteration 293, loss = 0.10553602\n",
      "Iteration 294, loss = 0.10427222\n",
      "Iteration 295, loss = 0.10366622\n",
      "Iteration 296, loss = 0.10438214\n",
      "Iteration 297, loss = 0.10448017\n",
      "Iteration 298, loss = 0.10296603\n",
      "Iteration 299, loss = 0.10389654\n",
      "Iteration 300, loss = 0.10226596\n",
      "Iteration 301, loss = 0.10238602\n",
      "Iteration 302, loss = 0.10139267\n",
      "Iteration 303, loss = 0.10044037\n",
      "Iteration 304, loss = 0.10029167\n",
      "Iteration 305, loss = 0.09947786\n",
      "Iteration 306, loss = 0.09896804\n",
      "Iteration 307, loss = 0.09893090\n",
      "Iteration 308, loss = 0.09963285\n",
      "Iteration 309, loss = 0.09896049\n",
      "Iteration 310, loss = 0.09700751\n",
      "Iteration 311, loss = 0.09627234\n",
      "Iteration 312, loss = 0.09709166\n",
      "Iteration 313, loss = 0.09751585\n",
      "Iteration 314, loss = 0.09625365\n",
      "Iteration 315, loss = 0.09552413\n",
      "Iteration 316, loss = 0.09437388\n",
      "Iteration 317, loss = 0.09494113\n",
      "Iteration 318, loss = 0.09560593\n",
      "Iteration 319, loss = 0.09404734\n",
      "Iteration 320, loss = 0.09541849\n",
      "Iteration 321, loss = 0.09340057\n",
      "Iteration 322, loss = 0.09232231\n",
      "Iteration 323, loss = 0.09260925\n",
      "Iteration 324, loss = 0.09243422\n",
      "Iteration 325, loss = 0.09192397\n",
      "Iteration 326, loss = 0.09104330\n",
      "Iteration 327, loss = 0.09136366\n",
      "Iteration 328, loss = 0.09027645\n",
      "Iteration 329, loss = 0.09048007\n",
      "Iteration 330, loss = 0.09111484\n",
      "Iteration 331, loss = 0.08933803\n",
      "Iteration 332, loss = 0.08850845\n",
      "Iteration 333, loss = 0.08840517\n",
      "Iteration 334, loss = 0.08785137\n",
      "Iteration 335, loss = 0.08795408\n",
      "Iteration 336, loss = 0.08725419\n",
      "Iteration 337, loss = 0.08722877\n",
      "Iteration 338, loss = 0.08578227\n",
      "Iteration 339, loss = 0.08678891\n",
      "Iteration 340, loss = 0.08514590\n",
      "Iteration 341, loss = 0.08665149\n",
      "Iteration 342, loss = 0.08564533\n",
      "Iteration 343, loss = 0.08552191\n",
      "Iteration 344, loss = 0.08412198\n",
      "Iteration 345, loss = 0.08385195\n",
      "Iteration 346, loss = 0.08431165\n",
      "Iteration 347, loss = 0.08322733\n",
      "Iteration 348, loss = 0.08230467\n",
      "Iteration 349, loss = 0.08319196\n",
      "Iteration 350, loss = 0.08179111\n",
      "Iteration 351, loss = 0.08211654\n",
      "Iteration 352, loss = 0.08185063\n",
      "Iteration 353, loss = 0.08145186\n",
      "Iteration 354, loss = 0.07972810\n",
      "Iteration 355, loss = 0.08036748\n",
      "Iteration 356, loss = 0.08018636\n",
      "Iteration 357, loss = 0.07938918\n",
      "Iteration 358, loss = 0.07873769\n",
      "Iteration 359, loss = 0.07838833\n",
      "Iteration 360, loss = 0.07833508\n",
      "Iteration 361, loss = 0.07895418\n",
      "Iteration 362, loss = 0.07818662\n",
      "Iteration 363, loss = 0.07759963\n",
      "Iteration 364, loss = 0.07773788\n",
      "Iteration 365, loss = 0.07722786\n",
      "Iteration 366, loss = 0.07730906\n",
      "Iteration 367, loss = 0.07759118\n",
      "Iteration 368, loss = 0.07585072\n",
      "Iteration 369, loss = 0.07529649\n",
      "Iteration 370, loss = 0.07562605\n",
      "Iteration 371, loss = 0.07531972\n",
      "Iteration 372, loss = 0.07489069\n",
      "Iteration 373, loss = 0.07494494\n",
      "Iteration 374, loss = 0.07311070\n",
      "Iteration 375, loss = 0.07455781\n",
      "Iteration 376, loss = 0.07412420\n",
      "Iteration 377, loss = 0.07296200\n",
      "Iteration 378, loss = 0.07406684\n",
      "Iteration 379, loss = 0.07347122\n",
      "Iteration 380, loss = 0.07219828\n",
      "Iteration 381, loss = 0.07241561\n",
      "Iteration 382, loss = 0.07294325\n",
      "Iteration 383, loss = 0.07143650\n",
      "Iteration 384, loss = 0.07185732\n",
      "Iteration 385, loss = 0.06995415\n",
      "Iteration 386, loss = 0.07169892\n",
      "Iteration 387, loss = 0.07026816\n",
      "Iteration 388, loss = 0.06992127\n",
      "Iteration 389, loss = 0.06979847\n",
      "Iteration 390, loss = 0.06932343\n",
      "Iteration 391, loss = 0.07082085\n",
      "Iteration 392, loss = 0.06885309\n",
      "Iteration 393, loss = 0.06859777\n",
      "Iteration 394, loss = 0.06878206\n",
      "Iteration 395, loss = 0.06871339\n",
      "Iteration 396, loss = 0.06684156\n",
      "Iteration 397, loss = 0.06696994\n",
      "Iteration 398, loss = 0.06794440\n",
      "Iteration 399, loss = 0.06751287\n",
      "Iteration 400, loss = 0.06830785\n",
      "Iteration 401, loss = 0.06632289\n",
      "Iteration 402, loss = 0.06638785\n",
      "Iteration 403, loss = 0.06585478\n",
      "Iteration 404, loss = 0.06606983\n",
      "Iteration 405, loss = 0.06616329\n",
      "Iteration 406, loss = 0.06518693\n",
      "Iteration 407, loss = 0.06514902\n",
      "Iteration 408, loss = 0.06445894\n",
      "Iteration 409, loss = 0.06416017\n",
      "Iteration 410, loss = 0.06534147\n",
      "Iteration 411, loss = 0.06327265\n",
      "Iteration 412, loss = 0.06420174\n",
      "Iteration 413, loss = 0.06352758\n",
      "Iteration 414, loss = 0.06228098\n",
      "Iteration 415, loss = 0.06414165\n",
      "Iteration 416, loss = 0.06178437\n",
      "Iteration 417, loss = 0.06255405\n",
      "Iteration 418, loss = 0.06201000\n",
      "Iteration 419, loss = 0.06152193\n",
      "Iteration 420, loss = 0.06190702\n",
      "Iteration 421, loss = 0.06081644\n",
      "Iteration 422, loss = 0.06126118\n",
      "Iteration 423, loss = 0.06153639\n",
      "Iteration 424, loss = 0.06100388\n",
      "Iteration 425, loss = 0.06143693\n",
      "Iteration 426, loss = 0.06044402\n",
      "Iteration 427, loss = 0.05979130\n",
      "Iteration 428, loss = 0.05972023\n",
      "Iteration 429, loss = 0.05956236\n",
      "Iteration 430, loss = 0.05917917\n",
      "Iteration 431, loss = 0.05989371\n",
      "Iteration 432, loss = 0.05992970\n",
      "Iteration 433, loss = 0.05914147\n",
      "Iteration 434, loss = 0.05899711\n",
      "Iteration 435, loss = 0.05897640\n",
      "Iteration 436, loss = 0.05815354\n",
      "Iteration 437, loss = 0.05828093\n",
      "Iteration 438, loss = 0.05739857\n",
      "Iteration 439, loss = 0.05706541\n",
      "Iteration 440, loss = 0.05757862\n",
      "Iteration 441, loss = 0.05753268\n",
      "Iteration 442, loss = 0.05705916\n",
      "Iteration 443, loss = 0.05779630\n",
      "Iteration 444, loss = 0.05647773\n",
      "Iteration 445, loss = 0.05673390\n",
      "Iteration 446, loss = 0.05606546\n",
      "Iteration 447, loss = 0.05528792\n",
      "Iteration 448, loss = 0.05548750\n",
      "Iteration 449, loss = 0.05554394\n",
      "Iteration 450, loss = 0.05530066\n",
      "Iteration 451, loss = 0.05560647\n",
      "Iteration 452, loss = 0.05559046\n",
      "Iteration 453, loss = 0.05482519\n",
      "Iteration 454, loss = 0.05409187\n",
      "Iteration 455, loss = 0.05426415\n",
      "Iteration 456, loss = 0.05532097\n",
      "Iteration 457, loss = 0.05353481\n",
      "Iteration 458, loss = 0.05358508\n",
      "Iteration 459, loss = 0.05354757\n",
      "Iteration 460, loss = 0.05298642\n",
      "Iteration 461, loss = 0.05283365\n",
      "Iteration 462, loss = 0.05398947\n",
      "Iteration 463, loss = 0.05277734\n",
      "Iteration 464, loss = 0.05278804\n",
      "Iteration 465, loss = 0.05284187\n",
      "Iteration 466, loss = 0.05229549\n",
      "Iteration 467, loss = 0.05240459\n",
      "Iteration 468, loss = 0.05295114\n",
      "Iteration 469, loss = 0.05207978\n",
      "Iteration 470, loss = 0.05223314\n",
      "Iteration 471, loss = 0.05089906\n",
      "Iteration 472, loss = 0.05104442\n",
      "Iteration 473, loss = 0.05122356\n",
      "Iteration 474, loss = 0.05129108\n",
      "Iteration 475, loss = 0.05064879\n",
      "Iteration 476, loss = 0.05041873\n",
      "Iteration 477, loss = 0.05134301\n",
      "Iteration 478, loss = 0.05005031\n",
      "Iteration 479, loss = 0.04981116\n",
      "Iteration 480, loss = 0.05048498\n",
      "Iteration 481, loss = 0.04993764\n",
      "Iteration 482, loss = 0.04954586\n",
      "Iteration 483, loss = 0.04947457\n",
      "Iteration 484, loss = 0.04916707\n",
      "Iteration 485, loss = 0.04975156\n",
      "Iteration 486, loss = 0.04968790\n",
      "Iteration 487, loss = 0.04917380\n",
      "Iteration 488, loss = 0.04862326\n",
      "Iteration 489, loss = 0.04908694\n",
      "Iteration 490, loss = 0.04850768\n",
      "Iteration 491, loss = 0.05020480\n",
      "Iteration 492, loss = 0.04970189\n",
      "Iteration 493, loss = 0.04862475\n",
      "Iteration 494, loss = 0.04814238\n",
      "Iteration 495, loss = 0.04819765\n",
      "Iteration 496, loss = 0.04946609\n",
      "Iteration 497, loss = 0.04795578\n",
      "Iteration 498, loss = 0.04770889\n",
      "Iteration 499, loss = 0.04701778\n",
      "Iteration 500, loss = 0.04811337\n",
      "Iteration 501, loss = 0.04815310\n",
      "Iteration 502, loss = 0.04709210\n",
      "Iteration 503, loss = 0.04663144\n",
      "Iteration 504, loss = 0.04654532\n",
      "Iteration 505, loss = 0.04610023\n",
      "Iteration 506, loss = 0.04790539\n",
      "Iteration 507, loss = 0.04657341\n",
      "Iteration 508, loss = 0.04586914\n",
      "Iteration 509, loss = 0.04596760\n",
      "Iteration 510, loss = 0.04642966\n",
      "Iteration 511, loss = 0.04641588\n",
      "Iteration 512, loss = 0.04592032\n",
      "Iteration 513, loss = 0.04592849\n",
      "Iteration 514, loss = 0.04594133\n",
      "Iteration 515, loss = 0.04515264\n",
      "Iteration 516, loss = 0.04506895\n",
      "Iteration 517, loss = 0.04561311\n",
      "Iteration 518, loss = 0.04534141\n",
      "Iteration 519, loss = 0.04435440\n",
      "Iteration 520, loss = 0.04491397\n",
      "Iteration 521, loss = 0.04438866\n",
      "Iteration 522, loss = 0.04426274\n",
      "Iteration 523, loss = 0.04461754\n",
      "Iteration 524, loss = 0.04384604\n",
      "Iteration 525, loss = 0.04440790\n",
      "Iteration 526, loss = 0.04396209\n",
      "Iteration 527, loss = 0.04541419\n",
      "Iteration 528, loss = 0.04398628\n",
      "Iteration 529, loss = 0.04461567\n",
      "Iteration 530, loss = 0.04449393\n",
      "Iteration 531, loss = 0.04320688\n",
      "Iteration 532, loss = 0.04321374\n",
      "Iteration 533, loss = 0.04301706\n",
      "Iteration 534, loss = 0.04423946\n",
      "Iteration 535, loss = 0.04368818\n",
      "Iteration 536, loss = 0.04257715\n",
      "Iteration 537, loss = 0.04214277\n",
      "Iteration 538, loss = 0.04244024\n",
      "Iteration 539, loss = 0.04352530\n",
      "Iteration 540, loss = 0.04306690\n",
      "Iteration 541, loss = 0.04266516\n",
      "Iteration 542, loss = 0.04218397\n",
      "Iteration 543, loss = 0.04330633\n",
      "Iteration 544, loss = 0.04373646\n",
      "Iteration 545, loss = 0.04220353\n",
      "Iteration 546, loss = 0.04325520\n",
      "Iteration 547, loss = 0.04206967\n",
      "Iteration 548, loss = 0.04268638\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV 1/3] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=600, solver=adam, verbose=True;, score=0.873 total time=  20.0s\n",
      "Iteration 1, loss = 0.45306012\n",
      "Iteration 2, loss = 0.30300395\n",
      "Iteration 3, loss = 0.29272540\n",
      "Iteration 4, loss = 0.28997096\n",
      "Iteration 5, loss = 0.28725077\n",
      "Iteration 6, loss = 0.28516340\n",
      "Iteration 7, loss = 0.28239528\n",
      "Iteration 8, loss = 0.27973496\n",
      "Iteration 9, loss = 0.27602019\n",
      "Iteration 10, loss = 0.27244144\n",
      "Iteration 11, loss = 0.27081010\n",
      "Iteration 12, loss = 0.26592922\n",
      "Iteration 13, loss = 0.26386471\n",
      "Iteration 14, loss = 0.26091922\n",
      "Iteration 15, loss = 0.25841764\n",
      "Iteration 16, loss = 0.25658050\n",
      "Iteration 17, loss = 0.25397974\n",
      "Iteration 18, loss = 0.25214144\n",
      "Iteration 19, loss = 0.25033368\n",
      "Iteration 20, loss = 0.24898210\n",
      "Iteration 21, loss = 0.24713640\n",
      "Iteration 22, loss = 0.24569167\n",
      "Iteration 23, loss = 0.24393223\n",
      "Iteration 24, loss = 0.24317017\n",
      "Iteration 25, loss = 0.24305545\n",
      "Iteration 26, loss = 0.24200744\n",
      "Iteration 27, loss = 0.24051611\n",
      "Iteration 28, loss = 0.23949001\n",
      "Iteration 29, loss = 0.23907292\n",
      "Iteration 30, loss = 0.23835750\n",
      "Iteration 31, loss = 0.23680203\n",
      "Iteration 32, loss = 0.23534920\n",
      "Iteration 33, loss = 0.23486508\n",
      "Iteration 34, loss = 0.23362145\n",
      "Iteration 35, loss = 0.23340622\n",
      "Iteration 36, loss = 0.23235256\n",
      "Iteration 37, loss = 0.23230653\n",
      "Iteration 38, loss = 0.23066458\n",
      "Iteration 39, loss = 0.23061911\n",
      "Iteration 40, loss = 0.23022625\n",
      "Iteration 41, loss = 0.22951275\n",
      "Iteration 42, loss = 0.22920236\n",
      "Iteration 43, loss = 0.22765349\n",
      "Iteration 44, loss = 0.22685358\n",
      "Iteration 45, loss = 0.22609442\n",
      "Iteration 46, loss = 0.22603171\n",
      "Iteration 47, loss = 0.22514175\n",
      "Iteration 48, loss = 0.22443585\n",
      "Iteration 49, loss = 0.22321449\n",
      "Iteration 50, loss = 0.22324681\n",
      "Iteration 51, loss = 0.22336471\n",
      "Iteration 52, loss = 0.22120429\n",
      "Iteration 53, loss = 0.22057907\n",
      "Iteration 54, loss = 0.22181633\n",
      "Iteration 55, loss = 0.21995873\n",
      "Iteration 56, loss = 0.21988365\n",
      "Iteration 57, loss = 0.21915382\n",
      "Iteration 58, loss = 0.21817582\n",
      "Iteration 59, loss = 0.21890999\n",
      "Iteration 60, loss = 0.21714520\n",
      "Iteration 61, loss = 0.21645843\n",
      "Iteration 62, loss = 0.21646657\n",
      "Iteration 63, loss = 0.21570795\n",
      "Iteration 64, loss = 0.21412251\n",
      "Iteration 65, loss = 0.21450839\n",
      "Iteration 66, loss = 0.21424820\n",
      "Iteration 67, loss = 0.21261583\n",
      "Iteration 68, loss = 0.21242083\n",
      "Iteration 69, loss = 0.21131979\n",
      "Iteration 70, loss = 0.21107000\n",
      "Iteration 71, loss = 0.21105500\n",
      "Iteration 72, loss = 0.21035822\n",
      "Iteration 73, loss = 0.20915082\n",
      "Iteration 74, loss = 0.20842419\n",
      "Iteration 75, loss = 0.20750121\n",
      "Iteration 76, loss = 0.20710072\n",
      "Iteration 77, loss = 0.20859285\n",
      "Iteration 78, loss = 0.20760683\n",
      "Iteration 79, loss = 0.20694855\n",
      "Iteration 80, loss = 0.20603284\n",
      "Iteration 81, loss = 0.20616634\n",
      "Iteration 82, loss = 0.20579939\n",
      "Iteration 83, loss = 0.20530600\n",
      "Iteration 84, loss = 0.20376047\n",
      "Iteration 85, loss = 0.20302839\n",
      "Iteration 86, loss = 0.20235450\n",
      "Iteration 87, loss = 0.20346619\n",
      "Iteration 88, loss = 0.20267798\n",
      "Iteration 89, loss = 0.20251433\n",
      "Iteration 90, loss = 0.20038379\n",
      "Iteration 91, loss = 0.20080266\n",
      "Iteration 92, loss = 0.20147573\n",
      "Iteration 93, loss = 0.20042646\n",
      "Iteration 94, loss = 0.19798746\n",
      "Iteration 95, loss = 0.19892293\n",
      "Iteration 96, loss = 0.20023621\n",
      "Iteration 97, loss = 0.19706004\n",
      "Iteration 98, loss = 0.19763779\n",
      "Iteration 99, loss = 0.19792095\n",
      "Iteration 100, loss = 0.19728331\n",
      "Iteration 101, loss = 0.19603455\n",
      "Iteration 102, loss = 0.19544675\n",
      "Iteration 103, loss = 0.19711656\n",
      "Iteration 104, loss = 0.19464627\n",
      "Iteration 105, loss = 0.19450000\n",
      "Iteration 106, loss = 0.19310701\n",
      "Iteration 107, loss = 0.19276755\n",
      "Iteration 108, loss = 0.19314193\n",
      "Iteration 109, loss = 0.19486968\n",
      "Iteration 110, loss = 0.19243288\n",
      "Iteration 111, loss = 0.19319133\n",
      "Iteration 112, loss = 0.19107071\n",
      "Iteration 113, loss = 0.18997931\n",
      "Iteration 114, loss = 0.18936381\n",
      "Iteration 115, loss = 0.18914178\n",
      "Iteration 116, loss = 0.19101288\n",
      "Iteration 117, loss = 0.18876447\n",
      "Iteration 118, loss = 0.18956183\n",
      "Iteration 119, loss = 0.18708495\n",
      "Iteration 120, loss = 0.18569371\n",
      "Iteration 121, loss = 0.18689730\n",
      "Iteration 122, loss = 0.18634858\n",
      "Iteration 123, loss = 0.18594105\n",
      "Iteration 124, loss = 0.18422213\n",
      "Iteration 125, loss = 0.18452736\n",
      "Iteration 126, loss = 0.18339949\n",
      "Iteration 127, loss = 0.18339676\n",
      "Iteration 128, loss = 0.18375145\n",
      "Iteration 129, loss = 0.18153072\n",
      "Iteration 130, loss = 0.18189223\n",
      "Iteration 131, loss = 0.18201934\n",
      "Iteration 132, loss = 0.18098028\n",
      "Iteration 133, loss = 0.18366886\n",
      "Iteration 134, loss = 0.18198883\n",
      "Iteration 135, loss = 0.18062544\n",
      "Iteration 136, loss = 0.17859666\n",
      "Iteration 137, loss = 0.17874361\n",
      "Iteration 138, loss = 0.17856393\n",
      "Iteration 139, loss = 0.17987260\n",
      "Iteration 140, loss = 0.17713764\n",
      "Iteration 141, loss = 0.17666947\n",
      "Iteration 142, loss = 0.17712832\n",
      "Iteration 143, loss = 0.17696931\n",
      "Iteration 144, loss = 0.17555898\n",
      "Iteration 145, loss = 0.17531664\n",
      "Iteration 146, loss = 0.17416572\n",
      "Iteration 147, loss = 0.17539312\n",
      "Iteration 148, loss = 0.17458103\n",
      "Iteration 149, loss = 0.17355745\n",
      "Iteration 150, loss = 0.17292502\n",
      "Iteration 151, loss = 0.17339502\n",
      "Iteration 152, loss = 0.17195240\n",
      "Iteration 153, loss = 0.17394697\n",
      "Iteration 154, loss = 0.17255055\n",
      "Iteration 155, loss = 0.17225694\n",
      "Iteration 156, loss = 0.17044025\n",
      "Iteration 157, loss = 0.16865014\n",
      "Iteration 158, loss = 0.16830446\n",
      "Iteration 159, loss = 0.16897909\n",
      "Iteration 160, loss = 0.16729125\n",
      "Iteration 161, loss = 0.16767329\n",
      "Iteration 162, loss = 0.16741287\n",
      "Iteration 163, loss = 0.16759517\n",
      "Iteration 164, loss = 0.16651951\n",
      "Iteration 165, loss = 0.16782573\n",
      "Iteration 166, loss = 0.16509437\n",
      "Iteration 167, loss = 0.16584889\n",
      "Iteration 168, loss = 0.16424617\n",
      "Iteration 169, loss = 0.16422385\n",
      "Iteration 170, loss = 0.16327333\n",
      "Iteration 171, loss = 0.16270427\n",
      "Iteration 172, loss = 0.16320769\n",
      "Iteration 173, loss = 0.16205277\n",
      "Iteration 174, loss = 0.16248302\n",
      "Iteration 175, loss = 0.16253777\n",
      "Iteration 176, loss = 0.16054922\n",
      "Iteration 177, loss = 0.16072007\n",
      "Iteration 178, loss = 0.16093614\n",
      "Iteration 179, loss = 0.15862975\n",
      "Iteration 180, loss = 0.15897312\n",
      "Iteration 181, loss = 0.15751151\n",
      "Iteration 182, loss = 0.15859299\n",
      "Iteration 183, loss = 0.15648133\n",
      "Iteration 184, loss = 0.15679629\n",
      "Iteration 185, loss = 0.15816153\n",
      "Iteration 186, loss = 0.15570954\n",
      "Iteration 187, loss = 0.15629270\n",
      "Iteration 188, loss = 0.15374874\n",
      "Iteration 189, loss = 0.15403494\n",
      "Iteration 190, loss = 0.15349019\n",
      "Iteration 191, loss = 0.15442303\n",
      "Iteration 192, loss = 0.15369340\n",
      "Iteration 193, loss = 0.15290948\n",
      "Iteration 194, loss = 0.15236668\n",
      "Iteration 195, loss = 0.15193331\n",
      "Iteration 196, loss = 0.15123909\n",
      "Iteration 197, loss = 0.15135635\n",
      "Iteration 198, loss = 0.14934863\n",
      "Iteration 199, loss = 0.14864065\n",
      "Iteration 200, loss = 0.14951369\n",
      "Iteration 201, loss = 0.14923463\n",
      "Iteration 202, loss = 0.14778064\n",
      "Iteration 203, loss = 0.14646296\n",
      "Iteration 204, loss = 0.14732223\n",
      "Iteration 205, loss = 0.14722619\n",
      "Iteration 206, loss = 0.14654784\n",
      "Iteration 207, loss = 0.14607995\n",
      "Iteration 208, loss = 0.14549304\n",
      "Iteration 209, loss = 0.14530357\n",
      "Iteration 210, loss = 0.14413894\n",
      "Iteration 211, loss = 0.14434058\n",
      "Iteration 212, loss = 0.14312472\n",
      "Iteration 213, loss = 0.14383633\n",
      "Iteration 214, loss = 0.14167089\n",
      "Iteration 215, loss = 0.14231909\n",
      "Iteration 216, loss = 0.14378726\n",
      "Iteration 217, loss = 0.14125269\n",
      "Iteration 218, loss = 0.14018630\n",
      "Iteration 219, loss = 0.13877058\n",
      "Iteration 220, loss = 0.13916691\n",
      "Iteration 221, loss = 0.13916718\n",
      "Iteration 222, loss = 0.13982561\n",
      "Iteration 223, loss = 0.13859452\n",
      "Iteration 224, loss = 0.13891695\n",
      "Iteration 225, loss = 0.13681654\n",
      "Iteration 226, loss = 0.13585333\n",
      "Iteration 227, loss = 0.13755846\n",
      "Iteration 228, loss = 0.13545432\n",
      "Iteration 229, loss = 0.13633359\n",
      "Iteration 230, loss = 0.13408673\n",
      "Iteration 231, loss = 0.13438199\n",
      "Iteration 232, loss = 0.13811566\n",
      "Iteration 233, loss = 0.13300737\n",
      "Iteration 234, loss = 0.13369852\n",
      "Iteration 235, loss = 0.13345706\n",
      "Iteration 236, loss = 0.13195305\n",
      "Iteration 237, loss = 0.13101874\n",
      "Iteration 238, loss = 0.13079445\n",
      "Iteration 239, loss = 0.13154432\n",
      "Iteration 240, loss = 0.13064667\n",
      "Iteration 241, loss = 0.13058779\n",
      "Iteration 242, loss = 0.12857382\n",
      "Iteration 243, loss = 0.13058433\n",
      "Iteration 244, loss = 0.12839019\n",
      "Iteration 245, loss = 0.12863593\n",
      "Iteration 246, loss = 0.12614030\n",
      "Iteration 247, loss = 0.12667129\n",
      "Iteration 248, loss = 0.12658641\n",
      "Iteration 249, loss = 0.12641834\n",
      "Iteration 250, loss = 0.12643399\n",
      "Iteration 251, loss = 0.12524135\n",
      "Iteration 252, loss = 0.12537096\n",
      "Iteration 253, loss = 0.12359378\n",
      "Iteration 254, loss = 0.12185689\n",
      "Iteration 255, loss = 0.12198084\n",
      "Iteration 256, loss = 0.12229967\n",
      "Iteration 257, loss = 0.12168150\n",
      "Iteration 258, loss = 0.12088015\n",
      "Iteration 259, loss = 0.12071133\n",
      "Iteration 260, loss = 0.11939616\n",
      "Iteration 261, loss = 0.12200410\n",
      "Iteration 262, loss = 0.12007082\n",
      "Iteration 263, loss = 0.12014453\n",
      "Iteration 264, loss = 0.11808249\n",
      "Iteration 265, loss = 0.11699557\n",
      "Iteration 266, loss = 0.11728044\n",
      "Iteration 267, loss = 0.11762209\n",
      "Iteration 268, loss = 0.11632722\n",
      "Iteration 269, loss = 0.11616723\n",
      "Iteration 270, loss = 0.11569049\n",
      "Iteration 271, loss = 0.11524801\n",
      "Iteration 272, loss = 0.11460995\n",
      "Iteration 273, loss = 0.11400433\n",
      "Iteration 274, loss = 0.11421872\n",
      "Iteration 275, loss = 0.11361383\n",
      "Iteration 276, loss = 0.11461245\n",
      "Iteration 277, loss = 0.11206879\n",
      "Iteration 278, loss = 0.11110934\n",
      "Iteration 279, loss = 0.11086228\n",
      "Iteration 280, loss = 0.11070460\n",
      "Iteration 281, loss = 0.11167604\n",
      "Iteration 282, loss = 0.10858077\n",
      "Iteration 283, loss = 0.10964099\n",
      "Iteration 284, loss = 0.10919990\n",
      "Iteration 285, loss = 0.10859959\n",
      "Iteration 286, loss = 0.10754593\n",
      "Iteration 287, loss = 0.10704186\n",
      "Iteration 288, loss = 0.10848799\n",
      "Iteration 289, loss = 0.10626228\n",
      "Iteration 290, loss = 0.10566211\n",
      "Iteration 291, loss = 0.10475471\n",
      "Iteration 292, loss = 0.10588860\n",
      "Iteration 293, loss = 0.10488202\n",
      "Iteration 294, loss = 0.10402812\n",
      "Iteration 295, loss = 0.10307856\n",
      "Iteration 296, loss = 0.10305334\n",
      "Iteration 297, loss = 0.10325988\n",
      "Iteration 298, loss = 0.10246688\n",
      "Iteration 299, loss = 0.10240555\n",
      "Iteration 300, loss = 0.10230031\n",
      "Iteration 301, loss = 0.10065879\n",
      "Iteration 302, loss = 0.10151148\n",
      "Iteration 303, loss = 0.10033492\n",
      "Iteration 304, loss = 0.10154181\n",
      "Iteration 305, loss = 0.09782479\n",
      "Iteration 306, loss = 0.09821386\n",
      "Iteration 307, loss = 0.09954475\n",
      "Iteration 308, loss = 0.09750014\n",
      "Iteration 309, loss = 0.09810888\n",
      "Iteration 310, loss = 0.09754043\n",
      "Iteration 311, loss = 0.09781248\n",
      "Iteration 312, loss = 0.09665249\n",
      "Iteration 313, loss = 0.09670576\n",
      "Iteration 314, loss = 0.09537514\n",
      "Iteration 315, loss = 0.09574135\n",
      "Iteration 316, loss = 0.09423314\n",
      "Iteration 317, loss = 0.09489058\n",
      "Iteration 318, loss = 0.09359158\n",
      "Iteration 319, loss = 0.09319487\n",
      "Iteration 320, loss = 0.09237024\n",
      "Iteration 321, loss = 0.09195478\n",
      "Iteration 322, loss = 0.09024654\n",
      "Iteration 323, loss = 0.09089383\n",
      "Iteration 324, loss = 0.09192149\n",
      "Iteration 325, loss = 0.09040985\n",
      "Iteration 326, loss = 0.09152580\n",
      "Iteration 327, loss = 0.09190851\n",
      "Iteration 328, loss = 0.09173522\n",
      "Iteration 329, loss = 0.08932742\n",
      "Iteration 330, loss = 0.08832525\n",
      "Iteration 331, loss = 0.08912773\n",
      "Iteration 332, loss = 0.08770655\n",
      "Iteration 333, loss = 0.08752968\n",
      "Iteration 334, loss = 0.08739480\n",
      "Iteration 335, loss = 0.08624182\n",
      "Iteration 336, loss = 0.08636627\n",
      "Iteration 337, loss = 0.08710408\n",
      "Iteration 338, loss = 0.08588087\n",
      "Iteration 339, loss = 0.08522751\n",
      "Iteration 340, loss = 0.08453132\n",
      "Iteration 341, loss = 0.08375708\n",
      "Iteration 342, loss = 0.08608943\n",
      "Iteration 343, loss = 0.08293041\n",
      "Iteration 344, loss = 0.08329759\n",
      "Iteration 345, loss = 0.08299740\n",
      "Iteration 346, loss = 0.08309446\n",
      "Iteration 347, loss = 0.08224488\n",
      "Iteration 348, loss = 0.08202546\n",
      "Iteration 349, loss = 0.08109691\n",
      "Iteration 350, loss = 0.08023802\n",
      "Iteration 351, loss = 0.08146094\n",
      "Iteration 352, loss = 0.08070399\n",
      "Iteration 353, loss = 0.07955033\n",
      "Iteration 354, loss = 0.07962934\n",
      "Iteration 355, loss = 0.07947887\n",
      "Iteration 356, loss = 0.07955803\n",
      "Iteration 357, loss = 0.07837019\n",
      "Iteration 358, loss = 0.07767410\n",
      "Iteration 359, loss = 0.07776866\n",
      "Iteration 360, loss = 0.07751400\n",
      "Iteration 361, loss = 0.07759698\n",
      "Iteration 362, loss = 0.07770223\n",
      "Iteration 363, loss = 0.07685773\n",
      "Iteration 364, loss = 0.07644835\n",
      "Iteration 365, loss = 0.07545134\n",
      "Iteration 366, loss = 0.07507645\n",
      "Iteration 367, loss = 0.07609084\n",
      "Iteration 368, loss = 0.07439475\n",
      "Iteration 369, loss = 0.07403234\n",
      "Iteration 370, loss = 0.07384127\n",
      "Iteration 371, loss = 0.07335652\n",
      "Iteration 372, loss = 0.07365868\n",
      "Iteration 373, loss = 0.07285951\n",
      "Iteration 374, loss = 0.07218405\n",
      "Iteration 375, loss = 0.07161927\n",
      "Iteration 376, loss = 0.07158189\n",
      "Iteration 377, loss = 0.07204519\n",
      "Iteration 378, loss = 0.07098192\n",
      "Iteration 379, loss = 0.07149362\n",
      "Iteration 380, loss = 0.07064283\n",
      "Iteration 381, loss = 0.06991174\n",
      "Iteration 382, loss = 0.06905223\n",
      "Iteration 383, loss = 0.07056149\n",
      "Iteration 384, loss = 0.06933461\n",
      "Iteration 385, loss = 0.06830985\n",
      "Iteration 386, loss = 0.06911522\n",
      "Iteration 387, loss = 0.06780374\n",
      "Iteration 388, loss = 0.06707653\n",
      "Iteration 389, loss = 0.06882581\n",
      "Iteration 390, loss = 0.06667518\n",
      "Iteration 391, loss = 0.06652996\n",
      "Iteration 392, loss = 0.06650138\n",
      "Iteration 393, loss = 0.06636823\n",
      "Iteration 394, loss = 0.06577566\n",
      "Iteration 395, loss = 0.06593710\n",
      "Iteration 396, loss = 0.06537931\n",
      "Iteration 397, loss = 0.06469083\n",
      "Iteration 398, loss = 0.06634890\n",
      "Iteration 399, loss = 0.06468948\n",
      "Iteration 400, loss = 0.06422580\n",
      "Iteration 401, loss = 0.06362492\n",
      "Iteration 402, loss = 0.06346838\n",
      "Iteration 403, loss = 0.06423074\n",
      "Iteration 404, loss = 0.06253920\n",
      "Iteration 405, loss = 0.06323780\n",
      "Iteration 406, loss = 0.06285527\n",
      "Iteration 407, loss = 0.06253783\n",
      "Iteration 408, loss = 0.06301801\n",
      "Iteration 409, loss = 0.06166980\n",
      "Iteration 410, loss = 0.06226242\n",
      "Iteration 411, loss = 0.06156083\n",
      "Iteration 412, loss = 0.06094548\n",
      "Iteration 413, loss = 0.06116889\n",
      "Iteration 414, loss = 0.06061791\n",
      "Iteration 415, loss = 0.05962033\n",
      "Iteration 416, loss = 0.05942164\n",
      "Iteration 417, loss = 0.05941768\n",
      "Iteration 418, loss = 0.05887741\n",
      "Iteration 419, loss = 0.05922668\n",
      "Iteration 420, loss = 0.06064010\n",
      "Iteration 421, loss = 0.05841490\n",
      "Iteration 422, loss = 0.05960934\n",
      "Iteration 423, loss = 0.05762976\n",
      "Iteration 424, loss = 0.05742700\n",
      "Iteration 425, loss = 0.05756371\n",
      "Iteration 426, loss = 0.05717617\n",
      "Iteration 427, loss = 0.05770685\n",
      "Iteration 428, loss = 0.05780405\n",
      "Iteration 429, loss = 0.05598872\n",
      "Iteration 430, loss = 0.05670299\n",
      "Iteration 431, loss = 0.05612516\n",
      "Iteration 432, loss = 0.05627950\n",
      "Iteration 433, loss = 0.05639339\n",
      "Iteration 434, loss = 0.05498629\n",
      "Iteration 435, loss = 0.05439174\n",
      "Iteration 436, loss = 0.05500487\n",
      "Iteration 437, loss = 0.05459914\n",
      "Iteration 438, loss = 0.05423453\n",
      "Iteration 439, loss = 0.05336899\n",
      "Iteration 440, loss = 0.05383022\n",
      "Iteration 441, loss = 0.05464148\n",
      "Iteration 442, loss = 0.05298047\n",
      "Iteration 443, loss = 0.05400166\n",
      "Iteration 444, loss = 0.05411271\n",
      "Iteration 445, loss = 0.05320646\n",
      "Iteration 446, loss = 0.05343024\n",
      "Iteration 447, loss = 0.05291270\n",
      "Iteration 448, loss = 0.05213178\n",
      "Iteration 449, loss = 0.05193706\n",
      "Iteration 450, loss = 0.05119459\n",
      "Iteration 451, loss = 0.05137227\n",
      "Iteration 452, loss = 0.05171670\n",
      "Iteration 453, loss = 0.05069729\n",
      "Iteration 454, loss = 0.05030163\n",
      "Iteration 455, loss = 0.05045779\n",
      "Iteration 456, loss = 0.05071001\n",
      "Iteration 457, loss = 0.05007500\n",
      "Iteration 458, loss = 0.04965072\n",
      "Iteration 459, loss = 0.04978728\n",
      "Iteration 460, loss = 0.05023379\n",
      "Iteration 461, loss = 0.05031228\n",
      "Iteration 462, loss = 0.05041398\n",
      "Iteration 463, loss = 0.04899095\n",
      "Iteration 464, loss = 0.04896421\n",
      "Iteration 465, loss = 0.04938437\n",
      "Iteration 466, loss = 0.04868691\n",
      "Iteration 467, loss = 0.04811590\n",
      "Iteration 468, loss = 0.04801925\n",
      "Iteration 469, loss = 0.04887084\n",
      "Iteration 470, loss = 0.04788704\n",
      "Iteration 471, loss = 0.04723973\n",
      "Iteration 472, loss = 0.04765688\n",
      "Iteration 473, loss = 0.04818033\n",
      "Iteration 474, loss = 0.04843595\n",
      "Iteration 475, loss = 0.04696950\n",
      "Iteration 476, loss = 0.04756685\n",
      "Iteration 477, loss = 0.04765006\n",
      "Iteration 478, loss = 0.04643653\n",
      "Iteration 479, loss = 0.04693078\n",
      "Iteration 480, loss = 0.04631143\n",
      "Iteration 481, loss = 0.04591406\n",
      "Iteration 482, loss = 0.04512286\n",
      "Iteration 483, loss = 0.04581705\n",
      "Iteration 484, loss = 0.04556005\n",
      "Iteration 485, loss = 0.04552403\n",
      "Iteration 486, loss = 0.04589882\n",
      "Iteration 487, loss = 0.04517639\n",
      "Iteration 488, loss = 0.04473839\n",
      "Iteration 489, loss = 0.04465473\n",
      "Iteration 490, loss = 0.04560653\n",
      "Iteration 491, loss = 0.04443436\n",
      "Iteration 492, loss = 0.04430969\n",
      "Iteration 493, loss = 0.04387666\n",
      "Iteration 494, loss = 0.04367549\n",
      "Iteration 495, loss = 0.04324086\n",
      "Iteration 496, loss = 0.04342263\n",
      "Iteration 497, loss = 0.04347092\n",
      "Iteration 498, loss = 0.04508436\n",
      "Iteration 499, loss = 0.04311086\n",
      "Iteration 500, loss = 0.04331174\n",
      "Iteration 501, loss = 0.04284588\n",
      "Iteration 502, loss = 0.04253489\n",
      "Iteration 503, loss = 0.04283627\n",
      "Iteration 504, loss = 0.04242976\n",
      "Iteration 505, loss = 0.04224402\n",
      "Iteration 506, loss = 0.04265065\n",
      "Iteration 507, loss = 0.04218200\n",
      "Iteration 508, loss = 0.04223740\n",
      "Iteration 509, loss = 0.04206785\n",
      "Iteration 510, loss = 0.04148140\n",
      "Iteration 511, loss = 0.04168773\n",
      "Iteration 512, loss = 0.04155134\n",
      "Iteration 513, loss = 0.04091412\n",
      "Iteration 514, loss = 0.04079898\n",
      "Iteration 515, loss = 0.04128651\n",
      "Iteration 516, loss = 0.04118600\n",
      "Iteration 517, loss = 0.04192912\n",
      "Iteration 518, loss = 0.04145509\n",
      "Iteration 519, loss = 0.04055084\n",
      "Iteration 520, loss = 0.04016889\n",
      "Iteration 521, loss = 0.04013930\n",
      "Iteration 522, loss = 0.04002007\n",
      "Iteration 523, loss = 0.04022840\n",
      "Iteration 524, loss = 0.03996527\n",
      "Iteration 525, loss = 0.03987634\n",
      "Iteration 526, loss = 0.03985324\n",
      "Iteration 527, loss = 0.03967659\n",
      "Iteration 528, loss = 0.03995544\n",
      "Iteration 529, loss = 0.03990838\n",
      "Iteration 530, loss = 0.03967836\n",
      "Iteration 531, loss = 0.03914911\n",
      "Iteration 532, loss = 0.03924408\n",
      "Iteration 533, loss = 0.03940101\n",
      "Iteration 534, loss = 0.03946807\n",
      "Iteration 535, loss = 0.03871194\n",
      "Iteration 536, loss = 0.03926269\n",
      "Iteration 537, loss = 0.03873770\n",
      "Iteration 538, loss = 0.03940441\n",
      "Iteration 539, loss = 0.03876171\n",
      "Iteration 540, loss = 0.03868707\n",
      "Iteration 541, loss = 0.03754896\n",
      "Iteration 542, loss = 0.03808486\n",
      "Iteration 543, loss = 0.03803267\n",
      "Iteration 544, loss = 0.03819307\n",
      "Iteration 545, loss = 0.03853574\n",
      "Iteration 546, loss = 0.03801451\n",
      "Iteration 547, loss = 0.03799134\n",
      "Iteration 548, loss = 0.03755083\n",
      "Iteration 549, loss = 0.03769362\n",
      "Iteration 550, loss = 0.03749168\n",
      "Iteration 551, loss = 0.03690309\n",
      "Iteration 552, loss = 0.03749527\n",
      "Iteration 553, loss = 0.03724377\n",
      "Iteration 554, loss = 0.03734150\n",
      "Iteration 555, loss = 0.03804976\n",
      "Iteration 556, loss = 0.03710011\n",
      "Iteration 557, loss = 0.03652451\n",
      "Iteration 558, loss = 0.03639479\n",
      "Iteration 559, loss = 0.03676943\n",
      "Iteration 560, loss = 0.03657826\n",
      "Iteration 561, loss = 0.03642404\n",
      "Iteration 562, loss = 0.03636038\n",
      "Iteration 563, loss = 0.03654187\n",
      "Iteration 564, loss = 0.03635136\n",
      "Iteration 565, loss = 0.03591586\n",
      "Iteration 566, loss = 0.03599894\n",
      "Iteration 567, loss = 0.03637119\n",
      "Iteration 568, loss = 0.03583097\n",
      "Iteration 569, loss = 0.03608330\n",
      "Iteration 570, loss = 0.03581674\n",
      "Iteration 571, loss = 0.03523183\n",
      "Iteration 572, loss = 0.03564105\n",
      "Iteration 573, loss = 0.03531682\n",
      "Iteration 574, loss = 0.03539512\n",
      "Iteration 575, loss = 0.03531814\n",
      "Iteration 576, loss = 0.03528975\n",
      "Iteration 577, loss = 0.03548470\n",
      "Iteration 578, loss = 0.03543348\n",
      "Iteration 579, loss = 0.03485938\n",
      "Iteration 580, loss = 0.03499335\n",
      "Iteration 581, loss = 0.03505943\n",
      "Iteration 582, loss = 0.03547392\n",
      "Iteration 583, loss = 0.03477645\n",
      "Iteration 584, loss = 0.03515490\n",
      "Iteration 585, loss = 0.03524559\n",
      "Iteration 586, loss = 0.03483850\n",
      "Iteration 587, loss = 0.03485881\n",
      "Iteration 588, loss = 0.03457764\n",
      "Iteration 589, loss = 0.03478151\n",
      "Iteration 590, loss = 0.03494525\n",
      "Iteration 591, loss = 0.03498796\n",
      "Iteration 592, loss = 0.03575308\n",
      "Iteration 593, loss = 0.03457318\n",
      "Iteration 594, loss = 0.03439208\n",
      "Iteration 595, loss = 0.03405817\n",
      "Iteration 596, loss = 0.03466950\n",
      "Iteration 597, loss = 0.03430744\n",
      "Iteration 598, loss = 0.03396385\n",
      "Iteration 599, loss = 0.03378838\n",
      "Iteration 600, loss = 0.03388553\n",
      "[CV 2/3] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=600, solver=adam, verbose=True;, score=0.861 total time=  22.0s\n",
      "Iteration 1, loss = 0.45175479\n",
      "Iteration 2, loss = 0.29067031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (600) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 3, loss = 0.28157906\n",
      "Iteration 4, loss = 0.27918568\n",
      "Iteration 5, loss = 0.27720542\n",
      "Iteration 6, loss = 0.27550850\n",
      "Iteration 7, loss = 0.27238488\n",
      "Iteration 8, loss = 0.26989063\n",
      "Iteration 9, loss = 0.26720617\n",
      "Iteration 10, loss = 0.26427639\n",
      "Iteration 11, loss = 0.26179648\n",
      "Iteration 12, loss = 0.25856259\n",
      "Iteration 13, loss = 0.25593941\n",
      "Iteration 14, loss = 0.25504900\n",
      "Iteration 15, loss = 0.25165531\n",
      "Iteration 16, loss = 0.25177544\n",
      "Iteration 17, loss = 0.24939877\n",
      "Iteration 18, loss = 0.24636075\n",
      "Iteration 19, loss = 0.24551750\n",
      "Iteration 20, loss = 0.24506309\n",
      "Iteration 21, loss = 0.24314072\n",
      "Iteration 22, loss = 0.24235102\n",
      "Iteration 23, loss = 0.24150431\n",
      "Iteration 24, loss = 0.23965473\n",
      "Iteration 25, loss = 0.23956874\n",
      "Iteration 26, loss = 0.23879366\n",
      "Iteration 27, loss = 0.23749896\n",
      "Iteration 28, loss = 0.23694441\n",
      "Iteration 29, loss = 0.23620361\n",
      "Iteration 30, loss = 0.23566602\n",
      "Iteration 31, loss = 0.23361704\n",
      "Iteration 32, loss = 0.23354478\n",
      "Iteration 33, loss = 0.23291239\n",
      "Iteration 34, loss = 0.23250469\n",
      "Iteration 35, loss = 0.23104557\n",
      "Iteration 36, loss = 0.23126113\n",
      "Iteration 37, loss = 0.22916822\n",
      "Iteration 38, loss = 0.22942435\n",
      "Iteration 39, loss = 0.22899409\n",
      "Iteration 40, loss = 0.22741494\n",
      "Iteration 41, loss = 0.22638912\n",
      "Iteration 42, loss = 0.22484107\n",
      "Iteration 43, loss = 0.22433477\n",
      "Iteration 44, loss = 0.22516020\n",
      "Iteration 45, loss = 0.22423481\n",
      "Iteration 46, loss = 0.22295960\n",
      "Iteration 47, loss = 0.22240807\n",
      "Iteration 48, loss = 0.22147308\n",
      "Iteration 49, loss = 0.22051877\n",
      "Iteration 50, loss = 0.21985843\n",
      "Iteration 51, loss = 0.22008063\n",
      "Iteration 52, loss = 0.21844441\n",
      "Iteration 53, loss = 0.21899138\n",
      "Iteration 54, loss = 0.21866142\n",
      "Iteration 55, loss = 0.21688909\n",
      "Iteration 56, loss = 0.21629126\n",
      "Iteration 57, loss = 0.21567611\n",
      "Iteration 58, loss = 0.21561642\n",
      "Iteration 59, loss = 0.21512357\n",
      "Iteration 60, loss = 0.21394882\n",
      "Iteration 61, loss = 0.21400251\n",
      "Iteration 62, loss = 0.21460891\n",
      "Iteration 63, loss = 0.21178238\n",
      "Iteration 64, loss = 0.21272103\n",
      "Iteration 65, loss = 0.21084764\n",
      "Iteration 66, loss = 0.21061822\n",
      "Iteration 67, loss = 0.21074398\n",
      "Iteration 68, loss = 0.20839972\n",
      "Iteration 69, loss = 0.20898126\n",
      "Iteration 70, loss = 0.20806022\n",
      "Iteration 71, loss = 0.20746908\n",
      "Iteration 72, loss = 0.20876549\n",
      "Iteration 73, loss = 0.20727650\n",
      "Iteration 74, loss = 0.20525551\n",
      "Iteration 75, loss = 0.20808908\n",
      "Iteration 76, loss = 0.20700944\n",
      "Iteration 77, loss = 0.20479670\n",
      "Iteration 78, loss = 0.20404484\n",
      "Iteration 79, loss = 0.20338118\n",
      "Iteration 80, loss = 0.20298296\n",
      "Iteration 81, loss = 0.20126627\n",
      "Iteration 82, loss = 0.20111416\n",
      "Iteration 83, loss = 0.20189099\n",
      "Iteration 84, loss = 0.19999864\n",
      "Iteration 85, loss = 0.19929862\n",
      "Iteration 86, loss = 0.19815247\n",
      "Iteration 87, loss = 0.19806147\n",
      "Iteration 88, loss = 0.19730493\n",
      "Iteration 89, loss = 0.19616688\n",
      "Iteration 90, loss = 0.19637651\n",
      "Iteration 91, loss = 0.19717532\n",
      "Iteration 92, loss = 0.19537705\n",
      "Iteration 93, loss = 0.19505807\n",
      "Iteration 94, loss = 0.19358212\n",
      "Iteration 95, loss = 0.19561988\n",
      "Iteration 96, loss = 0.19427068\n",
      "Iteration 97, loss = 0.19352424\n",
      "Iteration 98, loss = 0.19528778\n",
      "Iteration 99, loss = 0.19267007\n",
      "Iteration 100, loss = 0.19047039\n",
      "Iteration 101, loss = 0.18981840\n",
      "Iteration 102, loss = 0.18951310\n",
      "Iteration 103, loss = 0.18924294\n",
      "Iteration 104, loss = 0.19007841\n",
      "Iteration 105, loss = 0.18877856\n",
      "Iteration 106, loss = 0.18881213\n",
      "Iteration 107, loss = 0.19023640\n",
      "Iteration 108, loss = 0.18709806\n",
      "Iteration 109, loss = 0.18497850\n",
      "Iteration 110, loss = 0.18474415\n",
      "Iteration 111, loss = 0.18316107\n",
      "Iteration 112, loss = 0.18450228\n",
      "Iteration 113, loss = 0.18213724\n",
      "Iteration 114, loss = 0.18309751\n",
      "Iteration 115, loss = 0.18237321\n",
      "Iteration 116, loss = 0.18410206\n",
      "Iteration 117, loss = 0.18138486\n",
      "Iteration 118, loss = 0.18077819\n",
      "Iteration 119, loss = 0.18125227\n",
      "Iteration 120, loss = 0.18043409\n",
      "Iteration 121, loss = 0.17807099\n",
      "Iteration 122, loss = 0.17843744\n",
      "Iteration 123, loss = 0.17835510\n",
      "Iteration 124, loss = 0.17816174\n",
      "Iteration 125, loss = 0.17598972\n",
      "Iteration 126, loss = 0.17673489\n",
      "Iteration 127, loss = 0.17542899\n",
      "Iteration 128, loss = 0.17440873\n",
      "Iteration 129, loss = 0.17372677\n",
      "Iteration 130, loss = 0.17446146\n",
      "Iteration 131, loss = 0.17413445\n",
      "Iteration 132, loss = 0.17249604\n",
      "Iteration 133, loss = 0.17196271\n",
      "Iteration 134, loss = 0.17205440\n",
      "Iteration 135, loss = 0.17230907\n",
      "Iteration 136, loss = 0.16894236\n",
      "Iteration 137, loss = 0.16898504\n",
      "Iteration 138, loss = 0.16905817\n",
      "Iteration 139, loss = 0.16880984\n",
      "Iteration 140, loss = 0.16742360\n",
      "Iteration 141, loss = 0.16676336\n",
      "Iteration 142, loss = 0.16658059\n",
      "Iteration 143, loss = 0.16573407\n",
      "Iteration 144, loss = 0.16553661\n",
      "Iteration 145, loss = 0.16496393\n",
      "Iteration 146, loss = 0.16547195\n",
      "Iteration 147, loss = 0.16415027\n",
      "Iteration 148, loss = 0.16527369\n",
      "Iteration 149, loss = 0.16385234\n",
      "Iteration 150, loss = 0.16199754\n",
      "Iteration 151, loss = 0.16371609\n",
      "Iteration 152, loss = 0.16288521\n",
      "Iteration 153, loss = 0.16075282\n",
      "Iteration 154, loss = 0.16113217\n",
      "Iteration 155, loss = 0.16109502\n",
      "Iteration 156, loss = 0.16223025\n",
      "Iteration 157, loss = 0.15990316\n",
      "Iteration 158, loss = 0.15884264\n",
      "Iteration 159, loss = 0.15761645\n",
      "Iteration 160, loss = 0.15682679\n",
      "Iteration 161, loss = 0.15836017\n",
      "Iteration 162, loss = 0.15625496\n",
      "Iteration 163, loss = 0.15666391\n",
      "Iteration 164, loss = 0.15436784\n",
      "Iteration 165, loss = 0.15430419\n",
      "Iteration 166, loss = 0.15328725\n",
      "Iteration 167, loss = 0.15353150\n",
      "Iteration 168, loss = 0.15380932\n",
      "Iteration 169, loss = 0.15416465\n",
      "Iteration 170, loss = 0.15143406\n",
      "Iteration 171, loss = 0.15148178\n",
      "Iteration 172, loss = 0.15162649\n",
      "Iteration 173, loss = 0.15126342\n",
      "Iteration 174, loss = 0.14986077\n",
      "Iteration 175, loss = 0.14921550\n",
      "Iteration 176, loss = 0.14911289\n",
      "Iteration 177, loss = 0.14819396\n",
      "Iteration 178, loss = 0.14827852\n",
      "Iteration 179, loss = 0.14706262\n",
      "Iteration 180, loss = 0.14552701\n",
      "Iteration 181, loss = 0.14538187\n",
      "Iteration 182, loss = 0.14628407\n",
      "Iteration 183, loss = 0.14420139\n",
      "Iteration 184, loss = 0.14546825\n",
      "Iteration 185, loss = 0.14408273\n",
      "Iteration 186, loss = 0.14395437\n",
      "Iteration 187, loss = 0.14298224\n",
      "Iteration 188, loss = 0.14286162\n",
      "Iteration 189, loss = 0.14247424\n",
      "Iteration 190, loss = 0.14144746\n",
      "Iteration 191, loss = 0.14215850\n",
      "Iteration 192, loss = 0.14174811\n",
      "Iteration 193, loss = 0.14106246\n",
      "Iteration 194, loss = 0.13887428\n",
      "Iteration 195, loss = 0.13831626\n",
      "Iteration 196, loss = 0.13882267\n",
      "Iteration 197, loss = 0.13966020\n",
      "Iteration 198, loss = 0.13789286\n",
      "Iteration 199, loss = 0.13825152\n",
      "Iteration 200, loss = 0.13723056\n",
      "Iteration 201, loss = 0.13720385\n",
      "Iteration 202, loss = 0.13519550\n",
      "Iteration 203, loss = 0.13503173\n",
      "Iteration 204, loss = 0.13600694\n",
      "Iteration 205, loss = 0.13368184\n",
      "Iteration 206, loss = 0.13489994\n",
      "Iteration 207, loss = 0.13349093\n",
      "Iteration 208, loss = 0.13302092\n",
      "Iteration 209, loss = 0.13040487\n",
      "Iteration 210, loss = 0.13118534\n",
      "Iteration 211, loss = 0.13256186\n",
      "Iteration 212, loss = 0.13069699\n",
      "Iteration 213, loss = 0.12945328\n",
      "Iteration 214, loss = 0.13001067\n",
      "Iteration 215, loss = 0.12930744\n",
      "Iteration 216, loss = 0.12878441\n",
      "Iteration 217, loss = 0.12788925\n",
      "Iteration 218, loss = 0.12802760\n",
      "Iteration 219, loss = 0.12788914\n",
      "Iteration 220, loss = 0.12734742\n",
      "Iteration 221, loss = 0.12669677\n",
      "Iteration 222, loss = 0.12530476\n",
      "Iteration 223, loss = 0.12540393\n",
      "Iteration 224, loss = 0.12525639\n",
      "Iteration 225, loss = 0.12436427\n",
      "Iteration 226, loss = 0.12258106\n",
      "Iteration 227, loss = 0.12404197\n",
      "Iteration 228, loss = 0.12325486\n",
      "Iteration 229, loss = 0.12318569\n",
      "Iteration 230, loss = 0.12174536\n",
      "Iteration 231, loss = 0.12083950\n",
      "Iteration 232, loss = 0.12058735\n",
      "Iteration 233, loss = 0.11956962\n",
      "Iteration 234, loss = 0.11914372\n",
      "Iteration 235, loss = 0.12057967\n",
      "Iteration 236, loss = 0.11762892\n",
      "Iteration 237, loss = 0.11760077\n",
      "Iteration 238, loss = 0.11656277\n",
      "Iteration 239, loss = 0.11759810\n",
      "Iteration 240, loss = 0.11599392\n",
      "Iteration 241, loss = 0.11532042\n",
      "Iteration 242, loss = 0.11467297\n",
      "Iteration 243, loss = 0.11401808\n",
      "Iteration 244, loss = 0.11384157\n",
      "Iteration 245, loss = 0.11395556\n",
      "Iteration 246, loss = 0.11283885\n",
      "Iteration 247, loss = 0.11249710\n",
      "Iteration 248, loss = 0.11335569\n",
      "Iteration 249, loss = 0.11090907\n",
      "Iteration 250, loss = 0.11129785\n",
      "Iteration 251, loss = 0.11061095\n",
      "Iteration 252, loss = 0.11254673\n",
      "Iteration 253, loss = 0.11011909\n",
      "Iteration 254, loss = 0.10960425\n",
      "Iteration 255, loss = 0.10867695\n",
      "Iteration 256, loss = 0.10732857\n",
      "Iteration 257, loss = 0.10812564\n",
      "Iteration 258, loss = 0.10827927\n",
      "Iteration 259, loss = 0.10859186\n",
      "Iteration 260, loss = 0.10693541\n",
      "Iteration 261, loss = 0.10516326\n",
      "Iteration 262, loss = 0.10546011\n",
      "Iteration 263, loss = 0.10511256\n",
      "Iteration 264, loss = 0.10528655\n",
      "Iteration 265, loss = 0.10528382\n",
      "Iteration 266, loss = 0.10706421\n",
      "Iteration 267, loss = 0.10415180\n",
      "Iteration 268, loss = 0.10259732\n",
      "Iteration 269, loss = 0.10228462\n",
      "Iteration 270, loss = 0.10076698\n",
      "Iteration 271, loss = 0.10269727\n",
      "Iteration 272, loss = 0.10032212\n",
      "Iteration 273, loss = 0.10054636\n",
      "Iteration 274, loss = 0.09989720\n",
      "Iteration 275, loss = 0.10189390\n",
      "Iteration 276, loss = 0.09887818\n",
      "Iteration 277, loss = 0.09777944\n",
      "Iteration 278, loss = 0.09818358\n",
      "Iteration 279, loss = 0.09721849\n",
      "Iteration 280, loss = 0.09757924\n",
      "Iteration 281, loss = 0.09729686\n",
      "Iteration 282, loss = 0.09522872\n",
      "Iteration 283, loss = 0.09596218\n",
      "Iteration 284, loss = 0.09500856\n",
      "Iteration 285, loss = 0.09452436\n",
      "Iteration 286, loss = 0.09388316\n",
      "Iteration 287, loss = 0.09345640\n",
      "Iteration 288, loss = 0.09371436\n",
      "Iteration 289, loss = 0.09331413\n",
      "Iteration 290, loss = 0.09255982\n",
      "Iteration 291, loss = 0.09186114\n",
      "Iteration 292, loss = 0.09208333\n",
      "Iteration 293, loss = 0.09134251\n",
      "Iteration 294, loss = 0.09018473\n",
      "Iteration 295, loss = 0.09025129\n",
      "Iteration 296, loss = 0.09084044\n",
      "Iteration 297, loss = 0.08862313\n",
      "Iteration 298, loss = 0.08912032\n",
      "Iteration 299, loss = 0.08909830\n",
      "Iteration 300, loss = 0.08848144\n",
      "Iteration 301, loss = 0.08730864\n",
      "Iteration 302, loss = 0.08727810\n",
      "Iteration 303, loss = 0.08709625\n",
      "Iteration 304, loss = 0.08681009\n",
      "Iteration 305, loss = 0.08539589\n",
      "Iteration 306, loss = 0.08499427\n",
      "Iteration 307, loss = 0.08561083\n",
      "Iteration 308, loss = 0.08522832\n",
      "Iteration 309, loss = 0.08489727\n",
      "Iteration 310, loss = 0.08517318\n",
      "Iteration 311, loss = 0.08343804\n",
      "Iteration 312, loss = 0.08379105\n",
      "Iteration 313, loss = 0.08301269\n",
      "Iteration 314, loss = 0.08192236\n",
      "Iteration 315, loss = 0.08231831\n",
      "Iteration 316, loss = 0.08280019\n",
      "Iteration 317, loss = 0.08086369\n",
      "Iteration 318, loss = 0.08007720\n",
      "Iteration 319, loss = 0.08052340\n",
      "Iteration 320, loss = 0.07969938\n",
      "Iteration 321, loss = 0.08015829\n",
      "Iteration 322, loss = 0.07894744\n",
      "Iteration 323, loss = 0.07865374\n",
      "Iteration 324, loss = 0.07937283\n",
      "Iteration 325, loss = 0.07795644\n",
      "Iteration 326, loss = 0.07746744\n",
      "Iteration 327, loss = 0.07624469\n",
      "Iteration 328, loss = 0.07748965\n",
      "Iteration 329, loss = 0.07649273\n",
      "Iteration 330, loss = 0.07606625\n",
      "Iteration 331, loss = 0.07675415\n",
      "Iteration 332, loss = 0.07496830\n",
      "Iteration 333, loss = 0.07552066\n",
      "Iteration 334, loss = 0.07433562\n",
      "Iteration 335, loss = 0.07416976\n",
      "Iteration 336, loss = 0.07470582\n",
      "Iteration 337, loss = 0.07561163\n",
      "Iteration 338, loss = 0.07376672\n",
      "Iteration 339, loss = 0.07339130\n",
      "Iteration 340, loss = 0.07291485\n",
      "Iteration 341, loss = 0.07292918\n",
      "Iteration 342, loss = 0.07177381\n",
      "Iteration 343, loss = 0.07044806\n",
      "Iteration 344, loss = 0.07144878\n",
      "Iteration 345, loss = 0.07085709\n",
      "Iteration 346, loss = 0.07060325\n",
      "Iteration 347, loss = 0.07028848\n",
      "Iteration 348, loss = 0.07047631\n",
      "Iteration 349, loss = 0.06905470\n",
      "Iteration 350, loss = 0.06819756\n",
      "Iteration 351, loss = 0.06922524\n",
      "Iteration 352, loss = 0.06852788\n",
      "Iteration 353, loss = 0.06926477\n",
      "Iteration 354, loss = 0.06813599\n",
      "Iteration 355, loss = 0.06773612\n",
      "Iteration 356, loss = 0.06690084\n",
      "Iteration 357, loss = 0.06681731\n",
      "Iteration 358, loss = 0.06679021\n",
      "Iteration 359, loss = 0.06625811\n",
      "Iteration 360, loss = 0.06595588\n",
      "Iteration 361, loss = 0.06660633\n",
      "Iteration 362, loss = 0.06581815\n",
      "Iteration 363, loss = 0.06484958\n",
      "Iteration 364, loss = 0.06389112\n",
      "Iteration 365, loss = 0.06496386\n",
      "Iteration 366, loss = 0.06421280\n",
      "Iteration 367, loss = 0.06384881\n",
      "Iteration 368, loss = 0.06435501\n",
      "Iteration 369, loss = 0.06302749\n",
      "Iteration 370, loss = 0.06295559\n",
      "Iteration 371, loss = 0.06152982\n",
      "Iteration 372, loss = 0.06220706\n",
      "Iteration 373, loss = 0.06253744\n",
      "Iteration 374, loss = 0.06196823\n",
      "Iteration 375, loss = 0.06133910\n",
      "Iteration 376, loss = 0.06074361\n",
      "Iteration 377, loss = 0.06086431\n",
      "Iteration 378, loss = 0.06023016\n",
      "Iteration 379, loss = 0.06030953\n",
      "Iteration 380, loss = 0.06160461\n",
      "Iteration 381, loss = 0.05889927\n",
      "Iteration 382, loss = 0.05902576\n",
      "Iteration 383, loss = 0.05856743\n",
      "Iteration 384, loss = 0.05837082\n",
      "Iteration 385, loss = 0.05903072\n",
      "Iteration 386, loss = 0.05886741\n",
      "Iteration 387, loss = 0.05817572\n",
      "Iteration 388, loss = 0.05724158\n",
      "Iteration 389, loss = 0.05780870\n",
      "Iteration 390, loss = 0.05702085\n",
      "Iteration 391, loss = 0.05656404\n",
      "Iteration 392, loss = 0.05708897\n",
      "Iteration 393, loss = 0.05646617\n",
      "Iteration 394, loss = 0.05733025\n",
      "Iteration 395, loss = 0.05530099\n",
      "Iteration 396, loss = 0.05609469\n",
      "Iteration 397, loss = 0.05571908\n",
      "Iteration 398, loss = 0.05568655\n",
      "Iteration 399, loss = 0.05466602\n",
      "Iteration 400, loss = 0.05543165\n",
      "Iteration 401, loss = 0.05480421\n",
      "Iteration 402, loss = 0.05463067\n",
      "Iteration 403, loss = 0.05400602\n",
      "Iteration 404, loss = 0.05372865\n",
      "Iteration 405, loss = 0.05322633\n",
      "Iteration 406, loss = 0.05314607\n",
      "Iteration 407, loss = 0.05313596\n",
      "Iteration 408, loss = 0.05252987\n",
      "Iteration 409, loss = 0.05360369\n",
      "Iteration 410, loss = 0.05306546\n",
      "Iteration 411, loss = 0.05236167\n",
      "Iteration 412, loss = 0.05168022\n",
      "Iteration 413, loss = 0.05234158\n",
      "Iteration 414, loss = 0.05172666\n",
      "Iteration 415, loss = 0.05195786\n",
      "Iteration 416, loss = 0.05107588\n",
      "Iteration 417, loss = 0.05111639\n",
      "Iteration 418, loss = 0.05126200\n",
      "Iteration 419, loss = 0.05029354\n",
      "Iteration 420, loss = 0.04993032\n",
      "Iteration 421, loss = 0.05003915\n",
      "Iteration 422, loss = 0.05036199\n",
      "Iteration 423, loss = 0.04954175\n",
      "Iteration 424, loss = 0.04876128\n",
      "Iteration 425, loss = 0.04967194\n",
      "Iteration 426, loss = 0.04907149\n",
      "Iteration 427, loss = 0.04866834\n",
      "Iteration 428, loss = 0.04929330\n",
      "Iteration 429, loss = 0.04867082\n",
      "Iteration 430, loss = 0.04915075\n",
      "Iteration 431, loss = 0.04786396\n",
      "Iteration 432, loss = 0.04807553\n",
      "Iteration 433, loss = 0.04763655\n",
      "Iteration 434, loss = 0.04773846\n",
      "Iteration 435, loss = 0.04739880\n",
      "Iteration 436, loss = 0.04734262\n",
      "Iteration 437, loss = 0.04877579\n",
      "Iteration 438, loss = 0.04775665\n",
      "Iteration 439, loss = 0.04787372\n",
      "Iteration 440, loss = 0.04616027\n",
      "Iteration 441, loss = 0.04630940\n",
      "Iteration 442, loss = 0.04672385\n",
      "Iteration 443, loss = 0.04620169\n",
      "Iteration 444, loss = 0.04618398\n",
      "Iteration 445, loss = 0.04609921\n",
      "Iteration 446, loss = 0.04531552\n",
      "Iteration 447, loss = 0.04568599\n",
      "Iteration 448, loss = 0.04641761\n",
      "Iteration 449, loss = 0.04529201\n",
      "Iteration 450, loss = 0.04453170\n",
      "Iteration 451, loss = 0.04405374\n",
      "Iteration 452, loss = 0.04395002\n",
      "Iteration 453, loss = 0.04451758\n",
      "Iteration 454, loss = 0.04467991\n",
      "Iteration 455, loss = 0.04393691\n",
      "Iteration 456, loss = 0.04371706\n",
      "Iteration 457, loss = 0.04349590\n",
      "Iteration 458, loss = 0.04305134\n",
      "Iteration 459, loss = 0.04314242\n",
      "Iteration 460, loss = 0.04346665\n",
      "Iteration 461, loss = 0.04316396\n",
      "Iteration 462, loss = 0.04343477\n",
      "Iteration 463, loss = 0.04276444\n",
      "Iteration 464, loss = 0.04269114\n",
      "Iteration 465, loss = 0.04286126\n",
      "Iteration 466, loss = 0.04221561\n",
      "Iteration 467, loss = 0.04240160\n",
      "Iteration 468, loss = 0.04169594\n",
      "Iteration 469, loss = 0.04202666\n",
      "Iteration 470, loss = 0.04204706\n",
      "Iteration 471, loss = 0.04252413\n",
      "Iteration 472, loss = 0.04169094\n",
      "Iteration 473, loss = 0.04094880\n",
      "Iteration 474, loss = 0.04133552\n",
      "Iteration 475, loss = 0.04110297\n",
      "Iteration 476, loss = 0.04155925\n",
      "Iteration 477, loss = 0.04081739\n",
      "Iteration 478, loss = 0.04038600\n",
      "Iteration 479, loss = 0.04100974\n",
      "Iteration 480, loss = 0.04052491\n",
      "Iteration 481, loss = 0.04023570\n",
      "Iteration 482, loss = 0.04046540\n",
      "Iteration 483, loss = 0.04123484\n",
      "Iteration 484, loss = 0.04011131\n",
      "Iteration 485, loss = 0.04015549\n",
      "Iteration 486, loss = 0.04041249\n",
      "Iteration 487, loss = 0.03937155\n",
      "Iteration 488, loss = 0.04033261\n",
      "Iteration 489, loss = 0.03922517\n",
      "Iteration 490, loss = 0.03935327\n",
      "Iteration 491, loss = 0.04025653\n",
      "Iteration 492, loss = 0.04020226\n",
      "Iteration 493, loss = 0.03988386\n",
      "Iteration 494, loss = 0.03913596\n",
      "Iteration 495, loss = 0.03959264\n",
      "Iteration 496, loss = 0.03847811\n",
      "Iteration 497, loss = 0.03880995\n",
      "Iteration 498, loss = 0.03867321\n",
      "Iteration 499, loss = 0.03860600\n",
      "Iteration 500, loss = 0.03810648\n",
      "Iteration 501, loss = 0.03812189\n",
      "Iteration 502, loss = 0.03784253\n",
      "Iteration 503, loss = 0.03810120\n",
      "Iteration 504, loss = 0.03822012\n",
      "Iteration 505, loss = 0.03767622\n",
      "Iteration 506, loss = 0.03786108\n",
      "Iteration 507, loss = 0.03777981\n",
      "Iteration 508, loss = 0.03795041\n",
      "Iteration 509, loss = 0.03791443\n",
      "Iteration 510, loss = 0.03839815\n",
      "Iteration 511, loss = 0.03867425\n",
      "Iteration 512, loss = 0.03740542\n",
      "Iteration 513, loss = 0.03724344\n",
      "Iteration 514, loss = 0.03675008\n",
      "Iteration 515, loss = 0.03703848\n",
      "Iteration 516, loss = 0.03735431\n",
      "Iteration 517, loss = 0.03747020\n",
      "Iteration 518, loss = 0.03764007\n",
      "Iteration 519, loss = 0.03818142\n",
      "Iteration 520, loss = 0.03660380\n",
      "Iteration 521, loss = 0.03652354\n",
      "Iteration 522, loss = 0.03703982\n",
      "Iteration 523, loss = 0.03666952\n",
      "Iteration 524, loss = 0.03617985\n",
      "Iteration 525, loss = 0.03738201\n",
      "Iteration 526, loss = 0.03618891\n",
      "Iteration 527, loss = 0.03620990\n",
      "Iteration 528, loss = 0.03601235\n",
      "Iteration 529, loss = 0.03629356\n",
      "Iteration 530, loss = 0.03605889\n",
      "Iteration 531, loss = 0.03558848\n",
      "Iteration 532, loss = 0.03623066\n",
      "Iteration 533, loss = 0.03665195\n",
      "Iteration 534, loss = 0.03694770\n",
      "Iteration 535, loss = 0.03599608\n",
      "Iteration 536, loss = 0.03547163\n",
      "Iteration 537, loss = 0.03498044\n",
      "Iteration 538, loss = 0.03602295\n",
      "Iteration 539, loss = 0.03544251\n",
      "Iteration 540, loss = 0.03619795\n",
      "Iteration 541, loss = 0.03648323\n",
      "Iteration 542, loss = 0.03484071\n",
      "Iteration 543, loss = 0.03464939\n",
      "Iteration 544, loss = 0.03504381\n",
      "Iteration 545, loss = 0.03525503\n",
      "Iteration 546, loss = 0.03435400\n",
      "Iteration 547, loss = 0.03465028\n",
      "Iteration 548, loss = 0.03473376\n",
      "Iteration 549, loss = 0.03425622\n",
      "Iteration 550, loss = 0.03479474\n",
      "Iteration 551, loss = 0.03487327\n",
      "Iteration 552, loss = 0.03459661\n",
      "Iteration 553, loss = 0.03450417\n",
      "Iteration 554, loss = 0.03389957\n",
      "Iteration 555, loss = 0.03449830\n",
      "Iteration 556, loss = 0.03425780\n",
      "Iteration 557, loss = 0.03376676\n",
      "Iteration 558, loss = 0.03369912\n",
      "Iteration 559, loss = 0.03417413\n",
      "Iteration 560, loss = 0.03405275\n",
      "Iteration 561, loss = 0.03364735\n",
      "Iteration 562, loss = 0.03376160\n",
      "Iteration 563, loss = 0.03367581\n",
      "Iteration 564, loss = 0.03392261\n",
      "Iteration 565, loss = 0.03374676\n",
      "Iteration 566, loss = 0.03370348\n",
      "Iteration 567, loss = 0.03328696\n",
      "Iteration 568, loss = 0.03331887\n",
      "Iteration 569, loss = 0.03315634\n",
      "Iteration 570, loss = 0.03402791\n",
      "Iteration 571, loss = 0.03377971\n",
      "Iteration 572, loss = 0.03285263\n",
      "Iteration 573, loss = 0.03398172\n",
      "Iteration 574, loss = 0.03349218\n",
      "Iteration 575, loss = 0.03359501\n",
      "Iteration 576, loss = 0.03327606\n",
      "Iteration 577, loss = 0.03276517\n",
      "Iteration 578, loss = 0.03327024\n",
      "Iteration 579, loss = 0.03272940\n",
      "Iteration 580, loss = 0.03306680\n",
      "Iteration 581, loss = 0.03339986\n",
      "Iteration 582, loss = 0.03315111\n",
      "Iteration 583, loss = 0.03284537\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV 3/3] END activation=tanh, alpha=0.01, hidden_layer_sizes=(50, 100), learning_rate_init=0.001, max_iter=600, solver=adam, verbose=True;, score=0.874 total time=  22.1s\n",
      "Iteration 1, loss = 0.36678681\n",
      "Iteration 2, loss = 0.28805800\n",
      "Iteration 3, loss = 0.28270413\n",
      "Iteration 4, loss = 0.27260496\n",
      "Iteration 5, loss = 0.26664781\n",
      "Iteration 6, loss = 0.26112709\n",
      "Iteration 7, loss = 0.25647548\n",
      "Iteration 8, loss = 0.25416082\n",
      "Iteration 9, loss = 0.25365278\n",
      "Iteration 10, loss = 0.25262834\n",
      "Iteration 11, loss = 0.24912466\n",
      "Iteration 12, loss = 0.24574152\n",
      "Iteration 13, loss = 0.24564732\n",
      "Iteration 14, loss = 0.24409425\n",
      "Iteration 15, loss = 0.24336954\n",
      "Iteration 16, loss = 0.24125477\n",
      "Iteration 17, loss = 0.23949181\n",
      "Iteration 18, loss = 0.23708410\n",
      "Iteration 19, loss = 0.23854200\n",
      "Iteration 20, loss = 0.23706218\n",
      "Iteration 21, loss = 0.23636115\n",
      "Iteration 22, loss = 0.23489314\n",
      "Iteration 23, loss = 0.23497196\n",
      "Iteration 24, loss = 0.23360567\n",
      "Iteration 25, loss = 0.22915279\n",
      "Iteration 26, loss = 0.23147544\n",
      "Iteration 27, loss = 0.22934493\n",
      "Iteration 28, loss = 0.22474675\n",
      "Iteration 29, loss = 0.22590390\n",
      "Iteration 30, loss = 0.22280424\n",
      "Iteration 31, loss = 0.22233926\n",
      "Iteration 32, loss = 0.22167457\n",
      "Iteration 33, loss = 0.22087309\n",
      "Iteration 34, loss = 0.21962688\n",
      "Iteration 35, loss = 0.22194764\n",
      "Iteration 36, loss = 0.21698680\n",
      "Iteration 37, loss = 0.21481793\n",
      "Iteration 38, loss = 0.21511686\n",
      "Iteration 39, loss = 0.21496064\n",
      "Iteration 40, loss = 0.21214426\n",
      "Iteration 41, loss = 0.20882422\n",
      "Iteration 42, loss = 0.21235810\n",
      "Iteration 43, loss = 0.20943736\n",
      "Iteration 44, loss = 0.20675859\n",
      "Iteration 45, loss = 0.20643558\n",
      "Iteration 46, loss = 0.20430321\n",
      "Iteration 47, loss = 0.20762528\n",
      "Iteration 48, loss = 0.20378956\n",
      "Iteration 49, loss = 0.20408688\n",
      "Iteration 50, loss = 0.19976272\n",
      "Iteration 51, loss = 0.19894435\n",
      "Iteration 52, loss = 0.20299351\n",
      "Iteration 53, loss = 0.19749672\n",
      "Iteration 54, loss = 0.19850991\n",
      "Iteration 55, loss = 0.19852520\n",
      "Iteration 56, loss = 0.19572409\n",
      "Iteration 57, loss = 0.19698768\n",
      "Iteration 58, loss = 0.19070472\n",
      "Iteration 59, loss = 0.19007553\n",
      "Iteration 60, loss = 0.19126705\n",
      "Iteration 61, loss = 0.19142205\n",
      "Iteration 62, loss = 0.18826242\n",
      "Iteration 63, loss = 0.18386788\n",
      "Iteration 64, loss = 0.18578496\n",
      "Iteration 65, loss = 0.18089461\n",
      "Iteration 66, loss = 0.18092227\n",
      "Iteration 67, loss = 0.17930278\n",
      "Iteration 68, loss = 0.18401697\n",
      "Iteration 69, loss = 0.18313379\n",
      "Iteration 70, loss = 0.17753872\n",
      "Iteration 71, loss = 0.18062877\n",
      "Iteration 72, loss = 0.17614546\n",
      "Iteration 73, loss = 0.17248046\n",
      "Iteration 74, loss = 0.17545987\n",
      "Iteration 75, loss = 0.17188046\n",
      "Iteration 76, loss = 0.17181409\n",
      "Iteration 77, loss = 0.17114480\n",
      "Iteration 78, loss = 0.17122870\n",
      "Iteration 79, loss = 0.16707338\n",
      "Iteration 80, loss = 0.17017579\n",
      "Iteration 81, loss = 0.16641791\n",
      "Iteration 82, loss = 0.16803499\n",
      "Iteration 83, loss = 0.16908649\n",
      "Iteration 84, loss = 0.16203103\n",
      "Iteration 85, loss = 0.16589810\n",
      "Iteration 86, loss = 0.16156124\n",
      "Iteration 87, loss = 0.16175257\n",
      "Iteration 88, loss = 0.16090471\n",
      "Iteration 89, loss = 0.15774906\n",
      "Iteration 90, loss = 0.15583962\n",
      "Iteration 91, loss = 0.15698082\n",
      "Iteration 92, loss = 0.15581407\n",
      "Iteration 93, loss = 0.15530146\n",
      "Iteration 94, loss = 0.15500453\n",
      "Iteration 95, loss = 0.16115008\n",
      "Iteration 96, loss = 0.15611527\n",
      "Iteration 97, loss = 0.15104858\n",
      "Iteration 98, loss = 0.15365203\n",
      "Iteration 99, loss = 0.14635102\n",
      "Iteration 100, loss = 0.14242317\n",
      "Iteration 101, loss = 0.14754319\n",
      "Iteration 102, loss = 0.14802095\n",
      "Iteration 103, loss = 0.14345660\n",
      "Iteration 104, loss = 0.14407738\n",
      "Iteration 105, loss = 0.13856741\n",
      "Iteration 106, loss = 0.14079262\n",
      "Iteration 107, loss = 0.14016895\n",
      "Iteration 108, loss = 0.14375116\n",
      "Iteration 109, loss = 0.13919493\n",
      "Iteration 110, loss = 0.13540979\n",
      "Iteration 111, loss = 0.13744635\n",
      "Iteration 112, loss = 0.13418693\n",
      "Iteration 113, loss = 0.13334354\n",
      "Iteration 114, loss = 0.13042602\n",
      "Iteration 115, loss = 0.13290109\n",
      "Iteration 116, loss = 0.13025880\n",
      "Iteration 117, loss = 0.12893404\n",
      "Iteration 118, loss = 0.12712277\n",
      "Iteration 119, loss = 0.12733944\n",
      "Iteration 120, loss = 0.12627092\n",
      "Iteration 121, loss = 0.12413350\n",
      "Iteration 122, loss = 0.12183572\n",
      "Iteration 123, loss = 0.12194901\n",
      "Iteration 124, loss = 0.12187283\n",
      "Iteration 125, loss = 0.12112774\n",
      "Iteration 126, loss = 0.11855877\n",
      "Iteration 127, loss = 0.11768848\n",
      "Iteration 128, loss = 0.12445995\n",
      "Iteration 129, loss = 0.11662235\n",
      "Iteration 130, loss = 0.11639150\n",
      "Iteration 131, loss = 0.11919137\n",
      "Iteration 132, loss = 0.11272048\n",
      "Iteration 133, loss = 0.11459816\n",
      "Iteration 134, loss = 0.11575066\n",
      "Iteration 135, loss = 0.11122127\n",
      "Iteration 136, loss = 0.10696456\n",
      "Iteration 137, loss = 0.10644640\n",
      "Iteration 138, loss = 0.10886860\n",
      "Iteration 139, loss = 0.10742056\n",
      "Iteration 140, loss = 0.10600493\n",
      "Iteration 141, loss = 0.10436279\n",
      "Iteration 142, loss = 0.10392839\n",
      "Iteration 143, loss = 0.10110867\n",
      "Iteration 144, loss = 0.10015216\n",
      "Iteration 145, loss = 0.10206530\n",
      "Iteration 146, loss = 0.09974849\n",
      "Iteration 147, loss = 0.10755297\n",
      "Iteration 148, loss = 0.09679457\n",
      "Iteration 149, loss = 0.09399893\n",
      "Iteration 150, loss = 0.09868687\n",
      "Iteration 151, loss = 0.09413036\n",
      "Iteration 152, loss = 0.09081358\n",
      "Iteration 153, loss = 0.09525012\n",
      "Iteration 154, loss = 0.09488571\n",
      "Iteration 155, loss = 0.09354025\n",
      "Iteration 156, loss = 0.09216032\n",
      "Iteration 157, loss = 0.08859812\n",
      "Iteration 158, loss = 0.08655554\n",
      "Iteration 159, loss = 0.08633295\n",
      "Iteration 160, loss = 0.08748631\n",
      "Iteration 161, loss = 0.09088511\n",
      "Iteration 162, loss = 0.08499662\n",
      "Iteration 163, loss = 0.08534753\n",
      "Iteration 164, loss = 0.08546777\n",
      "Iteration 165, loss = 0.08410926\n",
      "Iteration 166, loss = 0.08149491\n",
      "Iteration 167, loss = 0.07988858\n",
      "Iteration 168, loss = 0.08132539\n",
      "Iteration 169, loss = 0.07975032\n",
      "Iteration 170, loss = 0.08338433\n",
      "Iteration 171, loss = 0.07668614\n",
      "Iteration 172, loss = 0.07799100\n",
      "Iteration 173, loss = 0.07605652\n",
      "Iteration 174, loss = 0.07812255\n",
      "Iteration 175, loss = 0.07932401\n",
      "Iteration 176, loss = 0.07675653\n",
      "Iteration 177, loss = 0.07883539\n",
      "Iteration 178, loss = 0.07425010\n",
      "Iteration 179, loss = 0.07251405\n",
      "Iteration 180, loss = 0.07331529\n",
      "Iteration 181, loss = 0.06900305\n",
      "Iteration 182, loss = 0.06951386\n",
      "Iteration 183, loss = 0.06718648\n",
      "Iteration 184, loss = 0.06684240\n",
      "Iteration 185, loss = 0.06761137\n",
      "Iteration 186, loss = 0.07044173\n",
      "Iteration 187, loss = 0.06400419\n",
      "Iteration 188, loss = 0.06219513\n",
      "Iteration 189, loss = 0.06413219\n",
      "Iteration 190, loss = 0.06585647\n",
      "Iteration 191, loss = 0.06565636\n",
      "Iteration 192, loss = 0.06538196\n",
      "Iteration 193, loss = 0.06152247\n",
      "Iteration 194, loss = 0.06060477\n",
      "Iteration 195, loss = 0.06358957\n",
      "Iteration 196, loss = 0.05625140\n",
      "Iteration 197, loss = 0.06015648\n",
      "Iteration 198, loss = 0.05968595\n",
      "Iteration 199, loss = 0.05710425\n",
      "Iteration 200, loss = 0.05621105\n",
      "Iteration 201, loss = 0.05850097\n",
      "Iteration 202, loss = 0.05588662\n",
      "Iteration 203, loss = 0.05797755\n",
      "Iteration 204, loss = 0.06191843\n",
      "Iteration 205, loss = 0.05510307\n",
      "Iteration 206, loss = 0.05408366\n",
      "Iteration 207, loss = 0.05396926\n",
      "Iteration 208, loss = 0.05574344\n",
      "Iteration 209, loss = 0.05095013\n",
      "Iteration 210, loss = 0.06005469\n",
      "Iteration 211, loss = 0.05750815\n",
      "Iteration 212, loss = 0.05033269\n",
      "Iteration 213, loss = 0.05168490\n",
      "Iteration 214, loss = 0.04991869\n",
      "Iteration 215, loss = 0.05332192\n",
      "Iteration 216, loss = 0.05269883\n",
      "Iteration 217, loss = 0.04808168\n",
      "Iteration 218, loss = 0.05041363\n",
      "Iteration 219, loss = 0.04586377\n",
      "Iteration 220, loss = 0.04562714\n",
      "Iteration 221, loss = 0.04621382\n",
      "Iteration 222, loss = 0.04606738\n",
      "Iteration 223, loss = 0.04534015\n",
      "Iteration 224, loss = 0.05164556\n",
      "Iteration 225, loss = 0.04460874\n",
      "Iteration 226, loss = 0.04551258\n",
      "Iteration 227, loss = 0.04451754\n",
      "Iteration 228, loss = 0.04395663\n",
      "Iteration 229, loss = 0.04634185\n",
      "Iteration 230, loss = 0.04440371\n",
      "Iteration 231, loss = 0.04119330\n",
      "Iteration 232, loss = 0.04227379\n",
      "Iteration 233, loss = 0.04078017\n",
      "Iteration 234, loss = 0.04327766\n",
      "Iteration 235, loss = 0.03924448\n",
      "Iteration 236, loss = 0.04258264\n",
      "Iteration 237, loss = 0.04273721\n",
      "Iteration 238, loss = 0.04446963\n",
      "Iteration 239, loss = 0.04553529\n",
      "Iteration 240, loss = 0.04169756\n",
      "Iteration 241, loss = 0.04109651\n",
      "Iteration 242, loss = 0.03936561\n",
      "Iteration 243, loss = 0.03981288\n",
      "Iteration 244, loss = 0.04075228\n",
      "Iteration 245, loss = 0.03886592\n",
      "Iteration 246, loss = 0.04703466\n",
      "Iteration 247, loss = 0.03846501\n",
      "Iteration 248, loss = 0.03834650\n",
      "Iteration 249, loss = 0.03706666\n",
      "Iteration 250, loss = 0.03572133\n",
      "Iteration 251, loss = 0.03675209\n",
      "Iteration 252, loss = 0.03763449\n",
      "Iteration 253, loss = 0.03409980\n",
      "Iteration 254, loss = 0.03388107\n",
      "Iteration 255, loss = 0.03415314\n",
      "Iteration 256, loss = 0.03347677\n",
      "Iteration 257, loss = 0.03582391\n",
      "Iteration 258, loss = 0.03255508\n",
      "Iteration 259, loss = 0.03486056\n",
      "Iteration 260, loss = 0.03513428\n",
      "Iteration 261, loss = 0.03258484\n",
      "Iteration 262, loss = 0.03161948\n",
      "Iteration 263, loss = 0.03379215\n",
      "Iteration 264, loss = 0.03253335\n",
      "Iteration 265, loss = 0.03297379\n",
      "Iteration 266, loss = 0.03379985\n",
      "Iteration 267, loss = 0.03220160\n",
      "Iteration 268, loss = 0.03223576\n",
      "Iteration 269, loss = 0.03079960\n",
      "Iteration 270, loss = 0.03578825\n",
      "Iteration 271, loss = 0.03147841\n",
      "Iteration 272, loss = 0.03204705\n",
      "Iteration 273, loss = 0.03040976\n",
      "Iteration 274, loss = 0.03394824\n",
      "Iteration 275, loss = 0.03245167\n",
      "Iteration 276, loss = 0.03274984\n",
      "Iteration 277, loss = 0.02997857\n",
      "Iteration 278, loss = 0.02878296\n",
      "Iteration 279, loss = 0.03182536\n",
      "Iteration 280, loss = 0.03099806\n",
      "Iteration 281, loss = 0.03017327\n",
      "Iteration 282, loss = 0.03495289\n",
      "Iteration 283, loss = 0.02995838\n",
      "Iteration 284, loss = 0.02821060\n",
      "Iteration 285, loss = 0.02933847\n",
      "Iteration 286, loss = 0.03086323\n",
      "Iteration 287, loss = 0.03178763\n",
      "Iteration 288, loss = 0.03371460\n",
      "Iteration 289, loss = 0.03219333\n",
      "Iteration 290, loss = 0.03123857\n",
      "Iteration 291, loss = 0.03244601\n",
      "Iteration 292, loss = 0.02712669\n",
      "Iteration 293, loss = 0.02683237\n",
      "Iteration 294, loss = 0.02804108\n",
      "Iteration 295, loss = 0.02726511\n",
      "Iteration 296, loss = 0.02678573\n",
      "Iteration 297, loss = 0.02632980\n",
      "Iteration 298, loss = 0.02924988\n",
      "Iteration 299, loss = 0.02630301\n",
      "Iteration 300, loss = 0.02669938\n",
      "Iteration 301, loss = 0.02718471\n",
      "Iteration 302, loss = 0.02601592\n",
      "Iteration 303, loss = 0.02521270\n",
      "Iteration 304, loss = 0.02542299\n",
      "Iteration 305, loss = 0.02596508\n",
      "Iteration 306, loss = 0.02662733\n",
      "Iteration 307, loss = 0.02418276\n",
      "Iteration 308, loss = 0.02685040\n",
      "Iteration 309, loss = 0.02706823\n",
      "Iteration 310, loss = 0.02511186\n",
      "Iteration 311, loss = 0.02517656\n",
      "Iteration 312, loss = 0.02435023\n",
      "Iteration 313, loss = 0.02862988\n",
      "Iteration 314, loss = 0.03231568\n",
      "Iteration 315, loss = 0.02988892\n",
      "Iteration 316, loss = 0.02768286\n",
      "Iteration 317, loss = 0.02661146\n",
      "Iteration 318, loss = 0.02981761\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV 1/3] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 50, 150), learning_rate_init=0.001, max_iter=600, solver=adam, verbose=True;, score=0.873 total time=  22.3s\n",
      "Iteration 1, loss = 0.35706306\n",
      "Iteration 2, loss = 0.29289374\n",
      "Iteration 3, loss = 0.28496781\n",
      "Iteration 4, loss = 0.27701769\n",
      "Iteration 5, loss = 0.26776456\n",
      "Iteration 6, loss = 0.26149797\n",
      "Iteration 7, loss = 0.25935143\n",
      "Iteration 8, loss = 0.25434035\n",
      "Iteration 9, loss = 0.25279697\n",
      "Iteration 10, loss = 0.24881467\n",
      "Iteration 11, loss = 0.24768725\n",
      "Iteration 12, loss = 0.24724301\n",
      "Iteration 13, loss = 0.24446618\n",
      "Iteration 14, loss = 0.24188415\n",
      "Iteration 15, loss = 0.24199756\n",
      "Iteration 16, loss = 0.23994462\n",
      "Iteration 17, loss = 0.24032879\n",
      "Iteration 18, loss = 0.23665140\n",
      "Iteration 19, loss = 0.23493115\n",
      "Iteration 20, loss = 0.23496191\n",
      "Iteration 21, loss = 0.23224794\n",
      "Iteration 22, loss = 0.23077632\n",
      "Iteration 23, loss = 0.22954872\n",
      "Iteration 24, loss = 0.23019787\n",
      "Iteration 25, loss = 0.22916912\n",
      "Iteration 26, loss = 0.22650737\n",
      "Iteration 27, loss = 0.22679870\n",
      "Iteration 28, loss = 0.22500940\n",
      "Iteration 29, loss = 0.22441528\n",
      "Iteration 30, loss = 0.22513688\n",
      "Iteration 31, loss = 0.21813432\n",
      "Iteration 32, loss = 0.21900749\n",
      "Iteration 33, loss = 0.21740667\n",
      "Iteration 34, loss = 0.21709771\n",
      "Iteration 35, loss = 0.21597638\n",
      "Iteration 36, loss = 0.21403853\n",
      "Iteration 37, loss = 0.21315147\n",
      "Iteration 38, loss = 0.21310124\n",
      "Iteration 39, loss = 0.21031638\n",
      "Iteration 40, loss = 0.21056721\n",
      "Iteration 41, loss = 0.20838561\n",
      "Iteration 42, loss = 0.21001131\n",
      "Iteration 43, loss = 0.20782570\n",
      "Iteration 44, loss = 0.20528086\n",
      "Iteration 45, loss = 0.20555768\n",
      "Iteration 46, loss = 0.20103401\n",
      "Iteration 47, loss = 0.20063581\n",
      "Iteration 48, loss = 0.19794856\n",
      "Iteration 49, loss = 0.20054056\n",
      "Iteration 50, loss = 0.20397383\n",
      "Iteration 51, loss = 0.20028235\n",
      "Iteration 52, loss = 0.19935669\n",
      "Iteration 53, loss = 0.19533131\n",
      "Iteration 54, loss = 0.19658478\n",
      "Iteration 55, loss = 0.19241202\n",
      "Iteration 56, loss = 0.19633875\n",
      "Iteration 57, loss = 0.19037899\n",
      "Iteration 58, loss = 0.19288036\n",
      "Iteration 59, loss = 0.19074413\n",
      "Iteration 60, loss = 0.18671007\n",
      "Iteration 61, loss = 0.18630382\n",
      "Iteration 62, loss = 0.18353054\n",
      "Iteration 63, loss = 0.18232746\n",
      "Iteration 64, loss = 0.18217133\n",
      "Iteration 65, loss = 0.18178066\n",
      "Iteration 66, loss = 0.18045377\n",
      "Iteration 67, loss = 0.18175792\n",
      "Iteration 68, loss = 0.17904435\n",
      "Iteration 69, loss = 0.17552014\n",
      "Iteration 70, loss = 0.17995535\n",
      "Iteration 71, loss = 0.17378292\n",
      "Iteration 72, loss = 0.17081754\n",
      "Iteration 73, loss = 0.17133837\n",
      "Iteration 74, loss = 0.17134321\n",
      "Iteration 75, loss = 0.16882680\n",
      "Iteration 76, loss = 0.16728202\n",
      "Iteration 77, loss = 0.16694213\n",
      "Iteration 78, loss = 0.16839686\n",
      "Iteration 79, loss = 0.16686248\n",
      "Iteration 80, loss = 0.16328092\n",
      "Iteration 81, loss = 0.16260709\n",
      "Iteration 82, loss = 0.15934496\n",
      "Iteration 83, loss = 0.15567885\n",
      "Iteration 84, loss = 0.15927241\n",
      "Iteration 85, loss = 0.15769308\n",
      "Iteration 86, loss = 0.15562577\n",
      "Iteration 87, loss = 0.15435352\n",
      "Iteration 88, loss = 0.15425972\n",
      "Iteration 89, loss = 0.15236509\n",
      "Iteration 90, loss = 0.14866212\n",
      "Iteration 91, loss = 0.14617123\n",
      "Iteration 92, loss = 0.14841062\n",
      "Iteration 93, loss = 0.14255258\n",
      "Iteration 94, loss = 0.14999994\n",
      "Iteration 95, loss = 0.14835137\n",
      "Iteration 96, loss = 0.14216653\n",
      "Iteration 97, loss = 0.14470684\n",
      "Iteration 98, loss = 0.14060754\n",
      "Iteration 99, loss = 0.13714494\n",
      "Iteration 100, loss = 0.13545141\n",
      "Iteration 101, loss = 0.13908412\n",
      "Iteration 102, loss = 0.13483388\n",
      "Iteration 103, loss = 0.13450978\n",
      "Iteration 104, loss = 0.13347206\n",
      "Iteration 105, loss = 0.13321088\n",
      "Iteration 106, loss = 0.13271504\n",
      "Iteration 107, loss = 0.13047210\n",
      "Iteration 108, loss = 0.13204144\n",
      "Iteration 109, loss = 0.13287637\n",
      "Iteration 110, loss = 0.12594538\n",
      "Iteration 111, loss = 0.12654062\n",
      "Iteration 112, loss = 0.12331923\n",
      "Iteration 113, loss = 0.12362823\n",
      "Iteration 114, loss = 0.12049005\n",
      "Iteration 115, loss = 0.12207909\n",
      "Iteration 116, loss = 0.11895009\n",
      "Iteration 117, loss = 0.11779169\n",
      "Iteration 118, loss = 0.11758376\n",
      "Iteration 119, loss = 0.11541690\n",
      "Iteration 120, loss = 0.11480379\n",
      "Iteration 121, loss = 0.11427590\n",
      "Iteration 122, loss = 0.11308065\n",
      "Iteration 123, loss = 0.11111536\n",
      "Iteration 124, loss = 0.11347412\n",
      "Iteration 125, loss = 0.10955606\n",
      "Iteration 126, loss = 0.10813350\n",
      "Iteration 127, loss = 0.10503519\n",
      "Iteration 128, loss = 0.10619106\n",
      "Iteration 129, loss = 0.10759419\n",
      "Iteration 130, loss = 0.10515912\n",
      "Iteration 131, loss = 0.10585550\n",
      "Iteration 132, loss = 0.10138406\n",
      "Iteration 133, loss = 0.09956287\n",
      "Iteration 134, loss = 0.10105635\n",
      "Iteration 135, loss = 0.10254372\n",
      "Iteration 136, loss = 0.10197453\n",
      "Iteration 137, loss = 0.09745456\n",
      "Iteration 138, loss = 0.09740590\n",
      "Iteration 139, loss = 0.09575444\n",
      "Iteration 140, loss = 0.09191156\n",
      "Iteration 141, loss = 0.09590411\n",
      "Iteration 142, loss = 0.09492216\n",
      "Iteration 143, loss = 0.08877321\n",
      "Iteration 144, loss = 0.09753293\n",
      "Iteration 145, loss = 0.09337071\n",
      "Iteration 146, loss = 0.08817196\n",
      "Iteration 147, loss = 0.08585419\n",
      "Iteration 148, loss = 0.08763669\n",
      "Iteration 149, loss = 0.08456261\n",
      "Iteration 150, loss = 0.08289408\n",
      "Iteration 151, loss = 0.08603676\n",
      "Iteration 152, loss = 0.08743815\n",
      "Iteration 153, loss = 0.08206473\n",
      "Iteration 154, loss = 0.08085557\n",
      "Iteration 155, loss = 0.08134961\n",
      "Iteration 156, loss = 0.08203036\n",
      "Iteration 157, loss = 0.07744456\n",
      "Iteration 158, loss = 0.07654854\n",
      "Iteration 159, loss = 0.07910550\n",
      "Iteration 160, loss = 0.07621617\n",
      "Iteration 161, loss = 0.07464066\n",
      "Iteration 162, loss = 0.07338426\n",
      "Iteration 163, loss = 0.07631424\n",
      "Iteration 164, loss = 0.07511624\n",
      "Iteration 165, loss = 0.07073464\n",
      "Iteration 166, loss = 0.07093446\n",
      "Iteration 167, loss = 0.07023961\n",
      "Iteration 168, loss = 0.07580643\n",
      "Iteration 169, loss = 0.07136001\n",
      "Iteration 170, loss = 0.07313592\n",
      "Iteration 171, loss = 0.06980278\n",
      "Iteration 172, loss = 0.06654555\n",
      "Iteration 173, loss = 0.06822245\n",
      "Iteration 174, loss = 0.06276534\n",
      "Iteration 175, loss = 0.06710798\n",
      "Iteration 176, loss = 0.06830567\n",
      "Iteration 177, loss = 0.06523664\n",
      "Iteration 178, loss = 0.06453307\n",
      "Iteration 179, loss = 0.06346145\n",
      "Iteration 180, loss = 0.05978415\n",
      "Iteration 181, loss = 0.06182025\n",
      "Iteration 182, loss = 0.05961801\n",
      "Iteration 183, loss = 0.06083406\n",
      "Iteration 184, loss = 0.06070112\n",
      "Iteration 185, loss = 0.06250929\n",
      "Iteration 186, loss = 0.05939193\n",
      "Iteration 187, loss = 0.05744986\n",
      "Iteration 188, loss = 0.05344202\n",
      "Iteration 189, loss = 0.05145079\n",
      "Iteration 190, loss = 0.05343314\n",
      "Iteration 191, loss = 0.05766347\n",
      "Iteration 192, loss = 0.05232551\n",
      "Iteration 193, loss = 0.05268301\n",
      "Iteration 194, loss = 0.05149542\n",
      "Iteration 195, loss = 0.04891761\n",
      "Iteration 196, loss = 0.04841423\n",
      "Iteration 197, loss = 0.05475936\n",
      "Iteration 198, loss = 0.05283722\n",
      "Iteration 199, loss = 0.05564406\n",
      "Iteration 200, loss = 0.05187078\n",
      "Iteration 201, loss = 0.04657296\n",
      "Iteration 202, loss = 0.04571752\n",
      "Iteration 203, loss = 0.04654258\n",
      "Iteration 204, loss = 0.04424371\n",
      "Iteration 205, loss = 0.04438373\n",
      "Iteration 206, loss = 0.04439579\n",
      "Iteration 207, loss = 0.04235103\n",
      "Iteration 208, loss = 0.04364499\n",
      "Iteration 209, loss = 0.04392638\n",
      "Iteration 210, loss = 0.04186620\n",
      "Iteration 211, loss = 0.04354674\n",
      "Iteration 212, loss = 0.04428782\n",
      "Iteration 213, loss = 0.04098358\n",
      "Iteration 214, loss = 0.04319693\n",
      "Iteration 215, loss = 0.04126998\n",
      "Iteration 216, loss = 0.03906357\n",
      "Iteration 217, loss = 0.04343064\n",
      "Iteration 218, loss = 0.04237897\n",
      "Iteration 219, loss = 0.04555774\n",
      "Iteration 220, loss = 0.04297500\n",
      "Iteration 221, loss = 0.03813388\n",
      "Iteration 222, loss = 0.03933432\n",
      "Iteration 223, loss = 0.03679614\n",
      "Iteration 224, loss = 0.03591810\n",
      "Iteration 225, loss = 0.03591788\n",
      "Iteration 226, loss = 0.03764809\n",
      "Iteration 227, loss = 0.03907623\n",
      "Iteration 228, loss = 0.03668802\n",
      "Iteration 229, loss = 0.03533928\n",
      "Iteration 230, loss = 0.03979721\n",
      "Iteration 231, loss = 0.04117292\n",
      "Iteration 232, loss = 0.03667918\n",
      "Iteration 233, loss = 0.03642104\n",
      "Iteration 234, loss = 0.03506182\n",
      "Iteration 235, loss = 0.03821697\n",
      "Iteration 236, loss = 0.03640032\n",
      "Iteration 237, loss = 0.03073477\n",
      "Iteration 238, loss = 0.03312205\n",
      "Iteration 239, loss = 0.03365347\n",
      "Iteration 240, loss = 0.03527410\n",
      "Iteration 241, loss = 0.03420748\n",
      "Iteration 242, loss = 0.03329565\n",
      "Iteration 243, loss = 0.03161044\n",
      "Iteration 244, loss = 0.03351506\n",
      "Iteration 245, loss = 0.03376062\n",
      "Iteration 246, loss = 0.03513634\n",
      "Iteration 247, loss = 0.03505403\n",
      "Iteration 248, loss = 0.02882000\n",
      "Iteration 249, loss = 0.02855382\n",
      "Iteration 250, loss = 0.02906101\n",
      "Iteration 251, loss = 0.03024959\n",
      "Iteration 252, loss = 0.02788152\n",
      "Iteration 253, loss = 0.02666472\n",
      "Iteration 254, loss = 0.02798567\n",
      "Iteration 255, loss = 0.02598689\n",
      "Iteration 256, loss = 0.02803891\n",
      "Iteration 257, loss = 0.02855600\n",
      "Iteration 258, loss = 0.03073185\n",
      "Iteration 259, loss = 0.03459741\n",
      "Iteration 260, loss = 0.04132061\n",
      "Iteration 261, loss = 0.04477457\n",
      "Iteration 262, loss = 0.03639861\n",
      "Iteration 263, loss = 0.02960140\n",
      "Iteration 264, loss = 0.02819396\n",
      "Iteration 265, loss = 0.02790226\n",
      "Iteration 266, loss = 0.02768761\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV 2/3] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 50, 150), learning_rate_init=0.001, max_iter=600, solver=adam, verbose=True;, score=0.866 total time=  18.7s\n",
      "Iteration 1, loss = 0.34834337\n",
      "Iteration 2, loss = 0.28306660\n",
      "Iteration 3, loss = 0.27757700\n",
      "Iteration 4, loss = 0.27212719\n",
      "Iteration 5, loss = 0.26590752\n",
      "Iteration 6, loss = 0.25903442\n",
      "Iteration 7, loss = 0.25379614\n",
      "Iteration 8, loss = 0.25213856\n",
      "Iteration 9, loss = 0.24894415\n",
      "Iteration 10, loss = 0.24556728\n",
      "Iteration 11, loss = 0.24354440\n",
      "Iteration 12, loss = 0.24253183\n",
      "Iteration 13, loss = 0.24029272\n",
      "Iteration 14, loss = 0.24018958\n",
      "Iteration 15, loss = 0.23886239\n",
      "Iteration 16, loss = 0.23754664\n",
      "Iteration 17, loss = 0.23467253\n",
      "Iteration 18, loss = 0.23401402\n",
      "Iteration 19, loss = 0.23301289\n",
      "Iteration 20, loss = 0.23011103\n",
      "Iteration 21, loss = 0.22991109\n",
      "Iteration 22, loss = 0.22734888\n",
      "Iteration 23, loss = 0.22544436\n",
      "Iteration 24, loss = 0.22599234\n",
      "Iteration 25, loss = 0.22433786\n",
      "Iteration 26, loss = 0.22609102\n",
      "Iteration 27, loss = 0.22148944\n",
      "Iteration 28, loss = 0.21895559\n",
      "Iteration 29, loss = 0.21787670\n",
      "Iteration 30, loss = 0.21704195\n",
      "Iteration 31, loss = 0.21815368\n",
      "Iteration 32, loss = 0.21239325\n",
      "Iteration 33, loss = 0.21337119\n",
      "Iteration 34, loss = 0.21218940\n",
      "Iteration 35, loss = 0.21511503\n",
      "Iteration 36, loss = 0.20895090\n",
      "Iteration 37, loss = 0.20879742\n",
      "Iteration 38, loss = 0.20725580\n",
      "Iteration 39, loss = 0.20702363\n",
      "Iteration 40, loss = 0.20313064\n",
      "Iteration 41, loss = 0.20450238\n",
      "Iteration 42, loss = 0.20325066\n",
      "Iteration 43, loss = 0.20216835\n",
      "Iteration 44, loss = 0.20154496\n",
      "Iteration 45, loss = 0.20204164\n",
      "Iteration 46, loss = 0.20011132\n",
      "Iteration 47, loss = 0.19619956\n",
      "Iteration 48, loss = 0.19770743\n",
      "Iteration 49, loss = 0.19287501\n",
      "Iteration 50, loss = 0.19062504\n",
      "Iteration 51, loss = 0.19011206\n",
      "Iteration 52, loss = 0.19281281\n",
      "Iteration 53, loss = 0.18957923\n",
      "Iteration 54, loss = 0.19074945\n",
      "Iteration 55, loss = 0.18878758\n",
      "Iteration 56, loss = 0.18296043\n",
      "Iteration 57, loss = 0.18376783\n",
      "Iteration 58, loss = 0.18157260\n",
      "Iteration 59, loss = 0.18071711\n",
      "Iteration 60, loss = 0.18200705\n",
      "Iteration 61, loss = 0.17642245\n",
      "Iteration 62, loss = 0.17516666\n",
      "Iteration 63, loss = 0.17799071\n",
      "Iteration 64, loss = 0.17357590\n",
      "Iteration 65, loss = 0.17309872\n",
      "Iteration 66, loss = 0.17475949\n",
      "Iteration 67, loss = 0.17110925\n",
      "Iteration 68, loss = 0.16853802\n",
      "Iteration 69, loss = 0.16783807\n",
      "Iteration 70, loss = 0.17139311\n",
      "Iteration 71, loss = 0.16784661\n",
      "Iteration 72, loss = 0.16557973\n",
      "Iteration 73, loss = 0.16671271\n",
      "Iteration 74, loss = 0.16037046\n",
      "Iteration 75, loss = 0.16248769\n",
      "Iteration 76, loss = 0.16165880\n",
      "Iteration 77, loss = 0.15863826\n",
      "Iteration 78, loss = 0.16044374\n",
      "Iteration 79, loss = 0.15438473\n",
      "Iteration 80, loss = 0.15742187\n",
      "Iteration 81, loss = 0.15483259\n",
      "Iteration 82, loss = 0.15405035\n",
      "Iteration 83, loss = 0.15606753\n",
      "Iteration 84, loss = 0.14930002\n",
      "Iteration 85, loss = 0.14786469\n",
      "Iteration 86, loss = 0.14778703\n",
      "Iteration 87, loss = 0.14511885\n",
      "Iteration 88, loss = 0.15095691\n",
      "Iteration 89, loss = 0.14673274\n",
      "Iteration 90, loss = 0.14342201\n",
      "Iteration 91, loss = 0.14607222\n",
      "Iteration 92, loss = 0.14506849\n",
      "Iteration 93, loss = 0.14280967\n",
      "Iteration 94, loss = 0.14365673\n",
      "Iteration 95, loss = 0.13918987\n",
      "Iteration 96, loss = 0.14030380\n",
      "Iteration 97, loss = 0.13489313\n",
      "Iteration 98, loss = 0.13514864\n",
      "Iteration 99, loss = 0.13227306\n",
      "Iteration 100, loss = 0.13199628\n",
      "Iteration 101, loss = 0.13003803\n",
      "Iteration 102, loss = 0.12920185\n",
      "Iteration 103, loss = 0.12877261\n",
      "Iteration 104, loss = 0.12717390\n",
      "Iteration 105, loss = 0.12628783\n",
      "Iteration 106, loss = 0.12507073\n",
      "Iteration 107, loss = 0.12594870\n",
      "Iteration 108, loss = 0.12265367\n",
      "Iteration 109, loss = 0.12523228\n",
      "Iteration 110, loss = 0.12377585\n",
      "Iteration 111, loss = 0.12010049\n",
      "Iteration 112, loss = 0.12001262\n",
      "Iteration 113, loss = 0.11856328\n",
      "Iteration 114, loss = 0.11814528\n",
      "Iteration 115, loss = 0.11846951\n",
      "Iteration 116, loss = 0.11450455\n",
      "Iteration 117, loss = 0.11847658\n",
      "Iteration 118, loss = 0.11912792\n",
      "Iteration 119, loss = 0.11809386\n",
      "Iteration 120, loss = 0.11479681\n",
      "Iteration 121, loss = 0.10770053\n",
      "Iteration 122, loss = 0.10997474\n",
      "Iteration 123, loss = 0.10455353\n",
      "Iteration 124, loss = 0.10325713\n",
      "Iteration 125, loss = 0.11216477\n",
      "Iteration 126, loss = 0.10352347\n",
      "Iteration 127, loss = 0.10382246\n",
      "Iteration 128, loss = 0.10442277\n",
      "Iteration 129, loss = 0.10210068\n",
      "Iteration 130, loss = 0.10301931\n",
      "Iteration 131, loss = 0.10051693\n",
      "Iteration 132, loss = 0.09999127\n",
      "Iteration 133, loss = 0.09680846\n",
      "Iteration 134, loss = 0.09488653\n",
      "Iteration 135, loss = 0.10088245\n",
      "Iteration 136, loss = 0.09518512\n",
      "Iteration 137, loss = 0.09369789\n",
      "Iteration 138, loss = 0.09540101\n",
      "Iteration 139, loss = 0.09466746\n",
      "Iteration 140, loss = 0.09397382\n",
      "Iteration 141, loss = 0.09527939\n",
      "Iteration 142, loss = 0.09367339\n",
      "Iteration 143, loss = 0.09114443\n",
      "Iteration 144, loss = 0.08832357\n",
      "Iteration 145, loss = 0.09090921\n",
      "Iteration 146, loss = 0.08863105\n",
      "Iteration 147, loss = 0.08548240\n",
      "Iteration 148, loss = 0.08420132\n",
      "Iteration 149, loss = 0.08377709\n",
      "Iteration 150, loss = 0.08395185\n",
      "Iteration 151, loss = 0.08685670\n",
      "Iteration 152, loss = 0.08648542\n",
      "Iteration 153, loss = 0.08165553\n",
      "Iteration 154, loss = 0.08702486\n",
      "Iteration 155, loss = 0.08197152\n",
      "Iteration 156, loss = 0.07782872\n",
      "Iteration 157, loss = 0.07787590\n",
      "Iteration 158, loss = 0.07859790\n",
      "Iteration 159, loss = 0.07625488\n",
      "Iteration 160, loss = 0.07436773\n",
      "Iteration 161, loss = 0.08054088\n",
      "Iteration 162, loss = 0.07519393\n",
      "Iteration 163, loss = 0.07586142\n",
      "Iteration 164, loss = 0.07129836\n",
      "Iteration 165, loss = 0.07022136\n",
      "Iteration 166, loss = 0.07264449\n",
      "Iteration 167, loss = 0.07197864\n",
      "Iteration 168, loss = 0.07287361\n",
      "Iteration 169, loss = 0.07132481\n",
      "Iteration 170, loss = 0.06596979\n",
      "Iteration 171, loss = 0.06674054\n",
      "Iteration 172, loss = 0.06789852\n",
      "Iteration 173, loss = 0.06945308\n",
      "Iteration 174, loss = 0.06676778\n",
      "Iteration 175, loss = 0.07007911\n",
      "Iteration 176, loss = 0.06513830\n",
      "Iteration 177, loss = 0.06201809\n",
      "Iteration 178, loss = 0.06213255\n",
      "Iteration 179, loss = 0.06590982\n",
      "Iteration 180, loss = 0.06246083\n",
      "Iteration 181, loss = 0.06063713\n",
      "Iteration 182, loss = 0.05861919\n",
      "Iteration 183, loss = 0.06057255\n",
      "Iteration 184, loss = 0.06033305\n",
      "Iteration 185, loss = 0.06277384\n",
      "Iteration 186, loss = 0.06303674\n",
      "Iteration 187, loss = 0.05875431\n",
      "Iteration 188, loss = 0.05757241\n",
      "Iteration 189, loss = 0.05996400\n",
      "Iteration 190, loss = 0.05752669\n",
      "Iteration 191, loss = 0.05598915\n",
      "Iteration 192, loss = 0.05360760\n",
      "Iteration 193, loss = 0.05324568\n",
      "Iteration 194, loss = 0.05599352\n",
      "Iteration 195, loss = 0.05597005\n",
      "Iteration 196, loss = 0.05145260\n",
      "Iteration 197, loss = 0.05361761\n",
      "Iteration 198, loss = 0.06056826\n",
      "Iteration 199, loss = 0.05043519\n",
      "Iteration 200, loss = 0.05054255\n",
      "Iteration 201, loss = 0.05204441\n",
      "Iteration 202, loss = 0.05037983\n",
      "Iteration 203, loss = 0.05047832\n",
      "Iteration 204, loss = 0.05169752\n",
      "Iteration 205, loss = 0.04927405\n",
      "Iteration 206, loss = 0.04886772\n",
      "Iteration 207, loss = 0.04910395\n",
      "Iteration 208, loss = 0.04707336\n",
      "Iteration 209, loss = 0.04897653\n",
      "Iteration 210, loss = 0.04523979\n",
      "Iteration 211, loss = 0.04321319\n",
      "Iteration 212, loss = 0.04466264\n",
      "Iteration 213, loss = 0.04531699\n",
      "Iteration 214, loss = 0.04420523\n",
      "Iteration 215, loss = 0.04549770\n",
      "Iteration 216, loss = 0.04107054\n",
      "Iteration 217, loss = 0.04222280\n",
      "Iteration 218, loss = 0.04282573\n",
      "Iteration 219, loss = 0.04312132\n",
      "Iteration 220, loss = 0.04440215\n",
      "Iteration 221, loss = 0.04482424\n",
      "Iteration 222, loss = 0.04736287\n",
      "Iteration 223, loss = 0.04504037\n",
      "Iteration 224, loss = 0.04117539\n",
      "Iteration 225, loss = 0.04324085\n",
      "Iteration 226, loss = 0.04429826\n",
      "Iteration 227, loss = 0.04495499\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "[CV 3/3] END activation=tanh, alpha=0.01, hidden_layer_sizes=(100, 50, 150), learning_rate_init=0.001, max_iter=600, solver=adam, verbose=True;, score=0.874 total time=  15.9s\n",
      "Iteration 1, loss = 0.33649252\n",
      "Iteration 2, loss = 0.28805284\n",
      "Iteration 3, loss = 0.28308738\n",
      "Iteration 4, loss = 0.27671010\n",
      "Iteration 5, loss = 0.26837119\n",
      "Iteration 6, loss = 0.26056882\n",
      "Iteration 7, loss = 0.25620274\n",
      "Iteration 8, loss = 0.25269513\n",
      "Iteration 9, loss = 0.25213145\n",
      "Iteration 10, loss = 0.25006271\n",
      "Iteration 11, loss = 0.24691798\n",
      "Iteration 12, loss = 0.24621811\n",
      "Iteration 13, loss = 0.24575099\n",
      "Iteration 14, loss = 0.24421835\n",
      "Iteration 15, loss = 0.24270491\n",
      "Iteration 16, loss = 0.24238359\n",
      "Iteration 17, loss = 0.24084389\n",
      "Iteration 18, loss = 0.24023099\n",
      "Iteration 19, loss = 0.23854803\n",
      "Iteration 20, loss = 0.23819563\n",
      "Iteration 21, loss = 0.23618880\n",
      "Iteration 22, loss = 0.23630334\n",
      "Iteration 23, loss = 0.23538683\n",
      "Iteration 24, loss = 0.23356912\n",
      "Iteration 25, loss = 0.23286939\n",
      "Iteration 26, loss = 0.23289088\n",
      "Iteration 27, loss = 0.23153734\n",
      "Iteration 28, loss = 0.23040555\n",
      "Iteration 29, loss = 0.22935356\n",
      "Iteration 30, loss = 0.22869763\n",
      "Iteration 31, loss = 0.22905736\n",
      "Iteration 32, loss = 0.22814842\n",
      "Iteration 33, loss = 0.22560424\n",
      "Iteration 34, loss = 0.22657604\n",
      "Iteration 35, loss = 0.22487530\n",
      "Iteration 36, loss = 0.22392110\n",
      "Iteration 37, loss = 0.22299370\n",
      "Iteration 38, loss = 0.22470620\n",
      "Iteration 39, loss = 0.22266535\n",
      "Iteration 40, loss = 0.22021522\n",
      "Iteration 41, loss = 0.22196932\n",
      "Iteration 42, loss = 0.21865013\n",
      "Iteration 43, loss = 0.21952330\n",
      "Iteration 44, loss = 0.21935018\n",
      "Iteration 45, loss = 0.21924963\n",
      "Iteration 46, loss = 0.21867531\n",
      "Iteration 47, loss = 0.21648146\n",
      "Iteration 48, loss = 0.21539196\n",
      "Iteration 49, loss = 0.21534983\n",
      "Iteration 50, loss = 0.21384604\n",
      "Iteration 51, loss = 0.21372396\n",
      "Iteration 52, loss = 0.21312730\n",
      "Iteration 53, loss = 0.21228670\n",
      "Iteration 54, loss = 0.21195220\n",
      "Iteration 55, loss = 0.21126398\n",
      "Iteration 56, loss = 0.20995570\n",
      "Iteration 57, loss = 0.21059910\n",
      "Iteration 58, loss = 0.21129201\n",
      "Iteration 59, loss = 0.20873431\n",
      "Iteration 60, loss = 0.20750281\n",
      "Iteration 61, loss = 0.20738032\n",
      "Iteration 62, loss = 0.20795780\n",
      "Iteration 63, loss = 0.20671420\n",
      "Iteration 64, loss = 0.20769121\n",
      "Iteration 65, loss = 0.20581682\n",
      "Iteration 66, loss = 0.20367600\n",
      "Iteration 67, loss = 0.20230839\n",
      "Iteration 68, loss = 0.20509979\n",
      "Iteration 69, loss = 0.20143791\n",
      "Iteration 70, loss = 0.20144789\n",
      "Iteration 71, loss = 0.19973474\n",
      "Iteration 72, loss = 0.20059374\n",
      "Iteration 73, loss = 0.19867397\n",
      "Iteration 74, loss = 0.20022467\n",
      "Iteration 75, loss = 0.19752451\n",
      "Iteration 76, loss = 0.19592342\n",
      "Iteration 77, loss = 0.19507375\n",
      "Iteration 78, loss = 0.19644682\n",
      "Iteration 79, loss = 0.19603464\n",
      "Iteration 80, loss = 0.19347054\n",
      "Iteration 81, loss = 0.19228346\n",
      "Iteration 82, loss = 0.19320900\n",
      "Iteration 83, loss = 0.19093627\n",
      "Iteration 84, loss = 0.19010139\n",
      "Iteration 85, loss = 0.18933273\n",
      "Iteration 86, loss = 0.18959121\n",
      "Iteration 87, loss = 0.18811555\n",
      "Iteration 88, loss = 0.18676045\n",
      "Iteration 89, loss = 0.18877032\n",
      "Iteration 90, loss = 0.18875754\n",
      "Iteration 91, loss = 0.18424799\n",
      "Iteration 92, loss = 0.18506024\n",
      "Iteration 93, loss = 0.18580356\n",
      "Iteration 94, loss = 0.18283478\n",
      "Iteration 95, loss = 0.18165098\n",
      "Iteration 96, loss = 0.18458063\n",
      "Iteration 97, loss = 0.18135097\n",
      "Iteration 98, loss = 0.18008271\n",
      "Iteration 99, loss = 0.17960049\n",
      "Iteration 100, loss = 0.17713392\n",
      "Iteration 101, loss = 0.17856204\n",
      "Iteration 102, loss = 0.17669559\n",
      "Iteration 103, loss = 0.17603051\n",
      "Iteration 104, loss = 0.17581301\n",
      "Iteration 105, loss = 0.17382920\n",
      "Iteration 106, loss = 0.17422035\n",
      "Iteration 107, loss = 0.17428150\n",
      "Iteration 108, loss = 0.17176416\n",
      "Iteration 109, loss = 0.17064214\n",
      "Iteration 110, loss = 0.17027826\n",
      "Iteration 111, loss = 0.17009792\n",
      "Iteration 112, loss = 0.16891438\n",
      "Iteration 113, loss = 0.16666433\n",
      "Iteration 114, loss = 0.16640647\n",
      "Iteration 115, loss = 0.16638331\n",
      "Iteration 116, loss = 0.16635570\n",
      "Iteration 117, loss = 0.16432025\n",
      "Iteration 118, loss = 0.16478264\n",
      "Iteration 119, loss = 0.16294934\n",
      "Iteration 120, loss = 0.16048400\n",
      "Iteration 121, loss = 0.16180588\n",
      "Iteration 122, loss = 0.15991896\n",
      "Iteration 123, loss = 0.15946367\n",
      "Iteration 124, loss = 0.15902425\n",
      "Iteration 125, loss = 0.15764721\n",
      "Iteration 126, loss = 0.15670907\n",
      "Iteration 127, loss = 0.15525909\n",
      "Iteration 128, loss = 0.15427732\n",
      "Iteration 129, loss = 0.15431249\n",
      "Iteration 130, loss = 0.15389958\n",
      "Iteration 131, loss = 0.15156041\n",
      "Iteration 132, loss = 0.15233173\n",
      "Iteration 133, loss = 0.15244205\n",
      "Iteration 134, loss = 0.14860301\n",
      "Iteration 135, loss = 0.14852677\n",
      "Iteration 136, loss = 0.14770702\n",
      "Iteration 137, loss = 0.14687429\n",
      "Iteration 138, loss = 0.14654189\n",
      "Iteration 139, loss = 0.14521081\n",
      "Iteration 140, loss = 0.14530934\n",
      "Iteration 141, loss = 0.14252024\n",
      "Iteration 142, loss = 0.14462929\n",
      "Iteration 143, loss = 0.14108933\n",
      "Iteration 144, loss = 0.14330130\n",
      "Iteration 145, loss = 0.14183720\n",
      "Iteration 146, loss = 0.13911567\n",
      "Iteration 147, loss = 0.13822665\n",
      "Iteration 148, loss = 0.13745074\n",
      "Iteration 149, loss = 0.13808675\n",
      "Iteration 150, loss = 0.13537897\n",
      "Iteration 151, loss = 0.13444179\n",
      "Iteration 152, loss = 0.13389577\n",
      "Iteration 153, loss = 0.13294265\n",
      "Iteration 154, loss = 0.13314265\n",
      "Iteration 155, loss = 0.13220336\n",
      "Iteration 156, loss = 0.12876328\n",
      "Iteration 157, loss = 0.13285811\n",
      "Iteration 158, loss = 0.13023275\n",
      "Iteration 159, loss = 0.12836517\n",
      "Iteration 160, loss = 0.12628833\n",
      "Iteration 161, loss = 0.12718695\n",
      "Iteration 162, loss = 0.12485028\n",
      "Iteration 163, loss = 0.12574636\n",
      "Iteration 164, loss = 0.12269011\n",
      "Iteration 165, loss = 0.12369774\n",
      "Iteration 166, loss = 0.12257788\n",
      "Iteration 167, loss = 0.12331715\n",
      "Iteration 168, loss = 0.12044365\n",
      "Iteration 169, loss = 0.11969081\n",
      "Iteration 170, loss = 0.11847272\n",
      "Iteration 171, loss = 0.11731431\n",
      "Iteration 172, loss = 0.11861418\n",
      "Iteration 173, loss = 0.11602138\n",
      "Iteration 174, loss = 0.11514075\n",
      "Iteration 175, loss = 0.11433227\n",
      "Iteration 176, loss = 0.11398304\n",
      "Iteration 177, loss = 0.11240602\n",
      "Iteration 178, loss = 0.11243086\n",
      "Iteration 179, loss = 0.11289813\n",
      "Iteration 180, loss = 0.11029728\n",
      "Iteration 181, loss = 0.11001418\n",
      "Iteration 182, loss = 0.11089573\n",
      "Iteration 183, loss = 0.10898774\n",
      "Iteration 184, loss = 0.10819453\n",
      "Iteration 185, loss = 0.10676677\n",
      "Iteration 186, loss = 0.10559611\n",
      "Iteration 187, loss = 0.10428880\n",
      "Iteration 188, loss = 0.10634112\n",
      "Iteration 189, loss = 0.10386994\n",
      "Iteration 190, loss = 0.10393979\n",
      "Iteration 191, loss = 0.10198373\n",
      "Iteration 192, loss = 0.10148635\n",
      "Iteration 193, loss = 0.10140078\n",
      "Iteration 194, loss = 0.09910975\n",
      "Iteration 195, loss = 0.09871786\n",
      "Iteration 196, loss = 0.09831753\n",
      "Iteration 197, loss = 0.09807627\n",
      "Iteration 198, loss = 0.09621482\n",
      "Iteration 199, loss = 0.09517152\n",
      "Iteration 200, loss = 0.09706232\n",
      "Iteration 201, loss = 0.09539933\n",
      "Iteration 202, loss = 0.09573721\n",
      "Iteration 203, loss = 0.09371995\n",
      "Iteration 204, loss = 0.09459495\n",
      "Iteration 205, loss = 0.09228293\n",
      "Iteration 206, loss = 0.09195209\n",
      "Iteration 207, loss = 0.09169570\n",
      "Iteration 208, loss = 0.09117897\n",
      "Iteration 209, loss = 0.08916050\n",
      "Iteration 210, loss = 0.09044500\n",
      "Iteration 211, loss = 0.08799032\n",
      "Iteration 212, loss = 0.08739588\n",
      "Iteration 213, loss = 0.08864841\n",
      "Iteration 214, loss = 0.08593844\n",
      "Iteration 215, loss = 0.08658275\n",
      "Iteration 216, loss = 0.08500217\n",
      "Iteration 217, loss = 0.08524945\n",
      "Iteration 218, loss = 0.08442963\n",
      "Iteration 219, loss = 0.08352141\n",
      "Iteration 220, loss = 0.08361063\n",
      "Iteration 221, loss = 0.08335541\n",
      "Iteration 222, loss = 0.08230468\n",
      "Iteration 223, loss = 0.08168528\n",
      "Iteration 224, loss = 0.08050885\n",
      "Iteration 225, loss = 0.08036353\n",
      "Iteration 226, loss = 0.08036848\n",
      "Iteration 227, loss = 0.07803374\n",
      "Iteration 228, loss = 0.07847046\n",
      "Iteration 229, loss = 0.07750243\n",
      "Iteration 230, loss = 0.07790939\n",
      "Iteration 231, loss = 0.07697539\n",
      "Iteration 232, loss = 0.07551727\n",
      "Iteration 233, loss = 0.07657758\n",
      "Iteration 234, loss = 0.07596484\n",
      "Iteration 235, loss = 0.07564708\n",
      "Iteration 236, loss = 0.07335555\n",
      "Iteration 237, loss = 0.07344761\n",
      "Iteration 238, loss = 0.07359594\n",
      "Iteration 239, loss = 0.07304899\n",
      "Iteration 240, loss = 0.07211207\n",
      "Iteration 241, loss = 0.07121523\n",
      "Iteration 242, loss = 0.07146770\n",
      "Iteration 243, loss = 0.07044210\n",
      "Iteration 244, loss = 0.07050155\n",
      "Iteration 245, loss = 0.07183786\n",
      "Iteration 246, loss = 0.06959871\n",
      "Iteration 247, loss = 0.06849924\n",
      "Iteration 248, loss = 0.06848778\n",
      "Iteration 249, loss = 0.06796578\n",
      "Iteration 250, loss = 0.06750791\n",
      "Iteration 251, loss = 0.06694764\n",
      "Iteration 252, loss = 0.06627247\n",
      "Iteration 253, loss = 0.06693080\n",
      "Iteration 254, loss = 0.06635493\n",
      "Iteration 255, loss = 0.06631673\n",
      "Iteration 256, loss = 0.06653278\n",
      "Iteration 257, loss = 0.06462966\n",
      "Iteration 258, loss = 0.06604790\n",
      "Iteration 259, loss = 0.06388438\n",
      "Iteration 260, loss = 0.06453560\n",
      "Iteration 261, loss = 0.06369888\n",
      "Iteration 262, loss = 0.06325442\n",
      "Iteration 263, loss = 0.06317995\n",
      "Iteration 264, loss = 0.06208588\n",
      "Iteration 265, loss = 0.06225470\n",
      "Iteration 266, loss = 0.06173277\n",
      "Iteration 267, loss = 0.06146409\n",
      "Iteration 268, loss = 0.06088036\n",
      "Iteration 269, loss = 0.06070205\n",
      "Iteration 270, loss = 0.06022017\n",
      "Iteration 271, loss = 0.05998447\n",
      "Iteration 272, loss = 0.05952511\n",
      "Iteration 273, loss = 0.05884764\n",
      "Iteration 274, loss = 0.05917387\n",
      "Iteration 275, loss = 0.05960115\n",
      "Iteration 276, loss = 0.05848108\n",
      "Iteration 277, loss = 0.05778665\n",
      "Iteration 278, loss = 0.05814618\n",
      "Iteration 279, loss = 0.05741764\n",
      "Iteration 280, loss = 0.05750811\n",
      "Iteration 281, loss = 0.05638360\n",
      "Iteration 282, loss = 0.05666717\n",
      "Iteration 283, loss = 0.05730882\n",
      "Iteration 284, loss = 0.05660464\n",
      "Iteration 285, loss = 0.05599764\n",
      "Iteration 286, loss = 0.05536521\n",
      "Iteration 287, loss = 0.05589148\n",
      "Iteration 288, loss = 0.05534676\n",
      "Iteration 289, loss = 0.05490650\n",
      "Iteration 290, loss = 0.05681760\n",
      "Iteration 291, loss = 0.05448891\n",
      "Iteration 292, loss = 0.05415172\n",
      "Iteration 293, loss = 0.05313749\n",
      "Iteration 294, loss = 0.05306746\n",
      "Iteration 295, loss = 0.05303950\n",
      "Iteration 296, loss = 0.05260033\n",
      "Iteration 297, loss = 0.05266865\n",
      "Iteration 298, loss = 0.05323585\n",
      "Iteration 299, loss = 0.05197810\n",
      "Iteration 300, loss = 0.05263398\n",
      "Iteration 301, loss = 0.05178519\n",
      "Iteration 302, loss = 0.05129862\n",
      "Iteration 303, loss = 0.05213986\n",
      "Iteration 304, loss = 0.05169058\n",
      "Iteration 305, loss = 0.05137618\n",
      "Iteration 306, loss = 0.05095846\n",
      "Iteration 307, loss = 0.05107522\n",
      "Iteration 308, loss = 0.05078588\n",
      "Iteration 309, loss = 0.04990218\n",
      "Iteration 310, loss = 0.04963768\n",
      "Iteration 311, loss = 0.04909355\n",
      "Iteration 312, loss = 0.04984447\n",
      "Iteration 313, loss = 0.05066553\n",
      "Iteration 314, loss = 0.04966106\n",
      "Iteration 315, loss = 0.04943689\n",
      "Iteration 316, loss = 0.04965370\n",
      "Iteration 317, loss = 0.04806943\n",
      "Iteration 318, loss = 0.04935830\n",
      "Iteration 319, loss = 0.04826946\n",
      "Iteration 320, loss = 0.04774551\n",
      "Iteration 321, loss = 0.04938599\n",
      "Iteration 322, loss = 0.04893316\n",
      "Iteration 323, loss = 0.04772606\n",
      "Iteration 324, loss = 0.04795677\n",
      "Iteration 325, loss = 0.04888491\n",
      "Iteration 326, loss = 0.04801171\n",
      "Iteration 327, loss = 0.04771328\n",
      "Iteration 328, loss = 0.04679840\n",
      "Iteration 329, loss = 0.04656513\n",
      "Iteration 330, loss = 0.04605678\n",
      "Iteration 331, loss = 0.04689972\n",
      "Iteration 332, loss = 0.04571334\n",
      "Iteration 333, loss = 0.04731136\n",
      "Iteration 334, loss = 0.04593711\n",
      "Iteration 335, loss = 0.04566179\n",
      "Iteration 336, loss = 0.04613628\n",
      "Iteration 337, loss = 0.04581733\n",
      "Iteration 338, loss = 0.04566845\n",
      "Iteration 339, loss = 0.04573765\n",
      "Iteration 340, loss = 0.04551909\n",
      "Iteration 341, loss = 0.04496293\n",
      "Iteration 342, loss = 0.04416816\n",
      "Iteration 343, loss = 0.04431449\n",
      "Iteration 344, loss = 0.04535244\n",
      "Iteration 345, loss = 0.04497583\n",
      "Iteration 346, loss = 0.04468608\n",
      "Iteration 347, loss = 0.04421298\n",
      "Iteration 348, loss = 0.04345146\n",
      "Iteration 349, loss = 0.04483681\n",
      "Iteration 350, loss = 0.04393700\n",
      "Iteration 351, loss = 0.04375306\n",
      "Iteration 352, loss = 0.04274554\n",
      "Iteration 353, loss = 0.04278303\n",
      "Iteration 354, loss = 0.04423197\n",
      "Iteration 355, loss = 0.04460774\n",
      "Iteration 356, loss = 0.04445761\n",
      "Iteration 357, loss = 0.04349978\n",
      "Iteration 358, loss = 0.04408846\n",
      "Iteration 359, loss = 0.04367156\n",
      "Iteration 360, loss = 0.04300037\n",
      "Iteration 361, loss = 0.04208181\n",
      "Iteration 362, loss = 0.04330914\n",
      "Iteration 363, loss = 0.04329268\n",
      "Iteration 364, loss = 0.04441425\n",
      "Iteration 365, loss = 0.04248559\n",
      "Iteration 366, loss = 0.04206927\n",
      "Iteration 367, loss = 0.04132200\n",
      "Iteration 368, loss = 0.04160005\n",
      "Iteration 369, loss = 0.04155362\n",
      "Iteration 370, loss = 0.04137401\n",
      "Iteration 371, loss = 0.04149835\n",
      "Iteration 372, loss = 0.04142838\n",
      "Iteration 373, loss = 0.04257507\n",
      "Iteration 374, loss = 0.04143333\n",
      "Iteration 375, loss = 0.04152072\n",
      "Iteration 376, loss = 0.04054238\n",
      "Iteration 377, loss = 0.04091350\n",
      "Iteration 378, loss = 0.04080685\n",
      "Iteration 379, loss = 0.04194316\n",
      "Iteration 380, loss = 0.04067055\n",
      "Iteration 381, loss = 0.04130092\n",
      "Iteration 382, loss = 0.04060244\n",
      "Iteration 383, loss = 0.04016894\n",
      "Iteration 384, loss = 0.04126316\n",
      "Iteration 385, loss = 0.04342428\n",
      "Iteration 386, loss = 0.04109380\n",
      "Iteration 387, loss = 0.04017409\n",
      "Iteration 388, loss = 0.04072658\n",
      "Iteration 389, loss = 0.03943730\n",
      "Iteration 390, loss = 0.04066674\n",
      "Iteration 391, loss = 0.03995449\n",
      "Iteration 392, loss = 0.03950274\n",
      "Iteration 393, loss = 0.04010588\n",
      "Iteration 394, loss = 0.04069664\n",
      "Iteration 395, loss = 0.04012157\n",
      "Iteration 396, loss = 0.04030249\n",
      "Iteration 397, loss = 0.03942928\n",
      "Iteration 398, loss = 0.04065573\n",
      "Iteration 399, loss = 0.03922656\n",
      "Iteration 400, loss = 0.03942679\n",
      "Iteration 401, loss = 0.03866805\n",
      "Iteration 402, loss = 0.03931500\n",
      "Iteration 403, loss = 0.03951051\n",
      "Iteration 404, loss = 0.04139625\n",
      "Iteration 405, loss = 0.04058867\n",
      "Iteration 406, loss = 0.03877483\n",
      "Iteration 407, loss = 0.03884915\n",
      "Iteration 408, loss = 0.03850630\n",
      "Iteration 409, loss = 0.03900379\n",
      "Iteration 410, loss = 0.03832281\n",
      "Iteration 411, loss = 0.03783736\n",
      "Iteration 412, loss = 0.03820316\n",
      "Iteration 413, loss = 0.03797904\n",
      "Iteration 414, loss = 0.03865256\n",
      "Iteration 415, loss = 0.03936487\n",
      "Iteration 416, loss = 0.03839161\n",
      "Iteration 417, loss = 0.03814324\n",
      "Iteration 418, loss = 0.03853073\n",
      "Iteration 419, loss = 0.03793432\n",
      "Iteration 420, loss = 0.03804483\n",
      "Iteration 421, loss = 0.03931021\n",
      "Iteration 422, loss = 0.04021427\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Best parameters:\n",
      " {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (100, 150), 'learning_rate_init': 0.001, 'max_iter': 600, 'solver': 'adam', 'verbose': True}\n"
     ]
    }
   ],
   "source": [
    "# Otimização com GridSearchCV, mas mantendo os nomes do modelo original\n",
    "\n",
    "mlp = MLPClassifier()\n",
    "\n",
    "params = {\n",
    "    'hidden_layer_sizes': [(100, 150), (50, 100), (100, 50, 150)],\n",
    "    'activation': ['tanh'],\n",
    "    'learning_rate_init': [0.001],  # igual ao do modelo original\n",
    "    'alpha': [0.01],\n",
    "    'solver': ['adam'],\n",
    "    'max_iter': [600],              # igual ao do modelo original\n",
    "    'verbose': [True]               # para mostrar o loss\n",
    "}\n",
    "\n",
    "gs_mlp = GridSearchCV(mlp, params, cv=3, verbose=3, scoring='accuracy')\n",
    "gs_mlp.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\\n\", gs_mlp.best_params_)\n",
    "\n",
    "# Fazendo a mágica: reaproveita os nomes do código original\n",
    "optimized_mlp = gs_mlp.best_estimator_         # substitui o modelo fixo pelo otimizado\n",
    "optimized_mlp_pred = optimized_mlp.predict(X_test)  # predições com o modelo otimizado\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70cef47d-5ef1-4084-9d00-de4cc9694852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Matriz de Confusão:\n",
      "[[1935  149]\n",
      " [ 173  209]]\n",
      "\n",
      "📊 Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.92      0.93      0.92      2084\n",
      "        True       0.58      0.55      0.56       382\n",
      "\n",
      "    accuracy                           0.87      2466\n",
      "   macro avg       0.75      0.74      0.74      2466\n",
      "weighted avg       0.87      0.87      0.87      2466\n",
      "\n",
      "\n",
      "✅ Acurácia: 0.8694\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\n🔍 Matriz de Confusão:\")\n",
    "print(confusion_matrix(y_test, optimized_mlp_pred))\n",
    "\n",
    "print(\"\\n📊 Relatório de Classificação:\")\n",
    "print(classification_report(y_test, optimized_mlp_pred))\n",
    "\n",
    "print(f\"\\n✅ Acurácia: {accuracy_score(y_test, optimized_mlp_pred):.4f}\")\n",
    "\n",
    "#Curva de perda\n",
    "plt.plot(optimized_mlp.loss_curve_)\n",
    "plt.title(\"Curva de Perda Durante o Treinamento\")\n",
    "plt.xlabel(\"Épocas\")\n",
    "plt.ylabel(\"Perda\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32544e45",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f44d0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=1 → Acurácia: 0.8423\n",
      "K=3 → Acurácia: 0.8642\n",
      "K=5 → Acurácia: 0.8678\n",
      "K=7 → Acurácia: 0.8783\n",
      "K=9 → Acurácia: 0.8792\n",
      "K=11 → Acurácia: 0.8771\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>K</th>\n",
       "      <th>Acurácia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>0.879157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0.878345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>0.877129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.867802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0.864152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0.842255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    K  Acurácia\n",
       "0   9  0.879157\n",
       "1   7  0.878345\n",
       "2  11  0.877129\n",
       "3   5  0.867802\n",
       "4   3  0.864152\n",
       "5   1  0.842255"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13484\\1275377054.py:36: UserWarning: Glyph 129504 (\\N{BRAIN}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 129504 (\\N{BRAIN}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Matriz de Confusão (K=9):\n",
      "[[2032   52]\n",
      " [ 246  136]]\n",
      "\n",
      "📋 Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.89      0.98      0.93      2084\n",
      "        True       0.72      0.36      0.48       382\n",
      "\n",
      "    accuracy                           0.88      2466\n",
      "   macro avg       0.81      0.67      0.70      2466\n",
      "weighted avg       0.87      0.88      0.86      2466\n",
      "\n",
      "\n",
      "✅ Acurácia Final: 0.8792\n"
     ]
    }
   ],
   "source": [
    "# Separação dos dados para o KNN\n",
    "X_train_knn, X_test_knn, y_train_knn, y_test_knn = train_test_split(\n",
    "    X_encoded, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# Normalização exclusiva para o KNN\n",
    "scaler_knn = StandardScaler()\n",
    "X_train_knn = scaler_knn.fit_transform(X_train_knn)\n",
    "X_test_knn = scaler_knn.transform(X_test_knn)\n",
    "\n",
    "# Teste com diferentes valores de k\n",
    "k_values = [1, 3, 5, 7, 9, 11]\n",
    "accuracies_knn = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_knn, y_train_knn)\n",
    "    y_pred = knn.predict(X_test_knn)\n",
    "    acc = accuracy_score(y_test_knn, y_pred)\n",
    "    accuracies_knn.append(acc)\n",
    "    print(f\"K={k} → Acurácia: {acc:.4f}\")\n",
    "\n",
    "# Tabela da acurácia por valor de K\n",
    "df_knn = pd.DataFrame({'K': k_values, 'Acurácia': accuracies_knn})\n",
    "display(df_knn.sort_values(by='Acurácia', ascending=False).reset_index(drop=True))\n",
    "\n",
    "\n",
    "# Gráfico mais bonito e informativo\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_knn['K'], df_knn['Acurácia'], marker='o', linestyle='-', color='blue')\n",
    "plt.xticks(k_values)\n",
    "plt.title(\"Acurácia do KNN por Número de Vizinhos (K)\")\n",
    "plt.xlabel(\"Número de Vizinhos (K)\")\n",
    "plt.ylabel(\"Acurácia\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Avaliação final com o melhor k\n",
    "melhor_k = k_values[accuracies_knn.index(max(accuracies_knn))]\n",
    "knn_final = KNeighborsClassifier(n_neighbors=melhor_k)\n",
    "knn_final.fit(X_train_knn, y_train_knn)\n",
    "y_pred_final = knn_final.predict(X_test_knn)\n",
    "\n",
    "print(f\"\\nMatriz de Confusão (K={melhor_k}):\")\n",
    "print(confusion_matrix(y_test_knn, y_pred_final))\n",
    "\n",
    "print(\"\\nRelatório de Classificação:\")\n",
    "print(classification_report(y_test_knn, y_pred_final))\n",
    "\n",
    "print(f\"\\nAcurácia Final: {accuracy_score(y_test_knn, y_pred_final):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437fa64d",
   "metadata": {},
   "source": [
    "COMPARAÇÂO MELHOR KNN E MELHOR MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1515eb53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modelo</th>\n",
       "      <th>Acurácia</th>\n",
       "      <th>Precisão</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.879157</td>\n",
       "      <td>0.723404</td>\n",
       "      <td>0.356021</td>\n",
       "      <td>0.477193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MLP</td>\n",
       "      <td>0.869424</td>\n",
       "      <td>0.583799</td>\n",
       "      <td>0.547120</td>\n",
       "      <td>0.564865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Modelo  Acurácia  Precisão    Recall  F1-Score\n",
       "0    KNN  0.879157  0.723404  0.356021  0.477193\n",
       "1    MLP  0.869424  0.583799  0.547120  0.564865"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tiago\\AppData\\Local\\Temp\\ipykernel_13484\\2686164800.py:36: UserWarning: Glyph 128269 (\\N{LEFT-POINTING MAGNIFYING GLASS}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "c:\\Users\\Tiago\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 128269 (\\N{LEFT-POINTING MAGNIFYING GLASS}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔷 Melhor valor de K no KNN: 9\n",
      "\n",
      "🔶 Melhores parâmetros do MLP otimizados pelo GridSearch:\n",
      "{'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (100, 150), 'learning_rate_init': 0.001, 'max_iter': 600, 'solver': 'adam', 'verbose': True}\n"
     ]
    }
   ],
   "source": [
    "# Métricas do KNN\n",
    "knn_precision = precision_score(y_test_knn, y_pred_final, zero_division=0)\n",
    "knn_recall = recall_score(y_test_knn, y_pred_final, zero_division=0)\n",
    "knn_f1 = f1_score(y_test_knn, y_pred_final, zero_division=0)\n",
    "knn_accuracy = accuracy_score(y_test_knn, y_pred_final)\n",
    "\n",
    "# Métricas do MLP\n",
    "mlp_precision = precision_score(y_test, optimized_mlp_pred, zero_division=0)\n",
    "mlp_recall = recall_score(y_test, optimized_mlp_pred, zero_division=0)\n",
    "mlp_f1 = f1_score(y_test, optimized_mlp_pred, zero_division=0)\n",
    "mlp_accuracy = accuracy_score(y_test, optimized_mlp_pred)\n",
    "\n",
    "# Tabela comparativa\n",
    "df_comp = pd.DataFrame({\n",
    "    'Modelo': ['KNN', 'MLP'],\n",
    "    'Acurácia': [knn_accuracy, mlp_accuracy],\n",
    "    'Precisão': [knn_precision, mlp_precision],\n",
    "    'Recall': [knn_recall, mlp_recall],\n",
    "    'F1-Score': [knn_f1, mlp_f1]\n",
    "})\n",
    "\n",
    "# Exibindo a tabela\n",
    "display(df_comp)\n",
    "\n",
    "# Gráfico comparativo\n",
    "df_comp_plot = df_comp.set_index('Modelo')\n",
    "df_comp_plot.plot(kind='bar', figsize=(10,6), colormap='coolwarm')\n",
    "plt.title(\"🔍 Comparação de Desempenho: KNN vs MLP\")\n",
    "plt.ylabel(\"Pontuação\")\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True, axis='y')\n",
    "plt.xticks(rotation=0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Parâmetros usados\n",
    "print(\"\\nMelhor valor de K no KNN:\", melhor_k)\n",
    "print(\"\\nMelhores parâmetros do MLP otimizados pelo GridSearch:\")\n",
    "print(gs_mlp.best_params_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
